{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r5OMfQfNaAm"
   },
   "source": [
    "#**Licença de Uso**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d22_lVKTNenJ"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "→ You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution–NonCommercial 4.0 International License.\n",
    "\n",
    "→ You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "<br>GitHub: [AE Tabular](github.com/LeoBR84p/ae-tabular)\n",
    "\n",
    "\n",
    "**© 2025 Leandro Bernardo Rodrigues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRNqsqxZWPzs"
   },
   "source": [
    "# **Gestão do Ambiente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clC8wQo4V2_j"
   },
   "source": [
    "## **Criar repositório .git no Colab**\n",
    "---\n",
    "Google Drive é considerado o ponto de verdade.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQi7bUIQUmAr"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# parágrafo git: inicialização do repositório no drive e push inicial para o github\n",
    "\n",
    "# imports\n",
    "from pathlib import Path\n",
    "import subprocess, os, sys, getpass, textwrap\n",
    "\n",
    "# util de shell\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout, r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "# garantir que o diretório do projeto exista\n",
    "repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# montar drive no colab se necessário\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive  # type: ignore\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        print(\"montando google drive…\")\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# configurar safe.directory para evitar avisos do git com caminhos de rede\n",
    "try:\n",
    "    sh([\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", str(repo_dir)])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# inicializar repositório se ainda não existir\n",
    "if not (repo_dir / \".git\").exists():\n",
    "    print(\"inicializando repositório git…\")\n",
    "    sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "    # garantir branch principal como main (compatível com versões antigas)\n",
    "    try:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"branch\", \"-M\", default_branch], cwd=repo_dir)\n",
    "else:\n",
    "    print(\".git já existe; seguindo\")\n",
    "\n",
    "# configurar identidade local\n",
    "sh([\"git\", \"config\", \"user.name\", author_name], cwd=repo_dir)\n",
    "sh([\"git\", \"config\", \"user.email\", author_email], cwd=repo_dir)\n",
    "\n",
    "# criar .gitignore básico e readme se estiverem ausentes\n",
    "gitignore_path = repo_dir / \".gitignore\"\n",
    "if not gitignore_path.exists():\n",
    "    gitignore_path.write_text(textwrap.dedent(\"\"\"\n",
    "      # python\n",
    "      __pycache__/\n",
    "      *.py[cod]\n",
    "      *.egg-info/\n",
    "      .venv*/\n",
    "      venv/\n",
    "\n",
    "      # segredos\n",
    "      .env\n",
    "      *.key\n",
    "      *.pem\n",
    "      *.tok\n",
    "\n",
    "      # jupyter/colab\n",
    "      .ipynb_checkpoints/\n",
    "\n",
    "      # artefatos e dados locais (não versionar)\n",
    "      data/\n",
    "      input/                 # inclui input.csv sensível\n",
    "      output/\n",
    "      runs/\n",
    "      logs/\n",
    "      figures/\n",
    "      *.log\n",
    "      *.tmp\n",
    "      *.bak\n",
    "      *.png\n",
    "      *.jpg\n",
    "      *.pdf\n",
    "      *.html\n",
    "\n",
    "      # allowlist para a pasta de referências\n",
    "      !references/\n",
    "      !references/**\n",
    "    \"\"\").strip() + \"\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado .gitignore\")\n",
    "\n",
    "readme_path = repo_dir / \"README.md\"\n",
    "if not readme_path.exists():\n",
    "    readme_path.write_text(f\"# {repo_name}\\n\\nprojeto de autoencoder tabular para journal entries.\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado README.md\")\n",
    "\n",
    "# configurar remoto origin\n",
    "remote_base = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "existing_remotes = sh([\"git\", \"remote\"], cwd=repo_dir)\n",
    "if \"origin\" not in existing_remotes.split():\n",
    "    sh([\"git\", \"remote\", \"add\", \"origin\", remote_base], cwd=repo_dir)\n",
    "    print(f\"remoto origin adicionado: {remote_base}\")\n",
    "else:\n",
    "    # se já existe, garantir que aponta para o repo correto\n",
    "    current_url = sh([\"git\", \"remote\", \"get-url\", \"origin\"], cwd=repo_dir)\n",
    "    if current_url != remote_base:\n",
    "        sh([\"git\", \"remote\", \"set-url\", \"origin\", remote_base], cwd=repo_dir)\n",
    "        print(f\"remoto origin atualizado para: {remote_base}\")\n",
    "    else:\n",
    "        print(\"remoto origin já configurado corretamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-aIMDUNNhfi"
   },
   "source": [
    "## **Utilitário:** verificação da formatação de código\n",
    "---\n",
    "\n",
    "**Cuidado: Pode ocasionar alterações no código.**\n",
    "\n",
    "Black [88] + Isort, desconsiderando células mágicas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jfa9X-d-Kfdn"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0001\n",
    "#pré-visualizar/aplicar (pula magics) — isort(profile=black)+black(88) { display-mode: \"form\" }\n",
    "import sys, subprocess, os, re, difflib, textwrap, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===== CONFIG =====\n",
    "NOTEBOOK = \"/content/drive/MyDrive/Notebooks/data-analysis/notebooks/AETabular_main.ipynb\"  # <- ajuste\n",
    "LINE_LENGTH = 88\n",
    "# ==================\n",
    "\n",
    "# 1) Instalar libs no MESMO Python do kernel\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"black\", \"isort\", \"nbformat\"])\n",
    "\n",
    "import nbformat\n",
    "import black\n",
    "import isort\n",
    "\n",
    "BLACK_MODE = black.Mode(line_length=LINE_LENGTH)\n",
    "ISORT_CFG  = isort.Config(profile=\"black\", line_length=LINE_LENGTH)\n",
    "\n",
    "# 2) Regras para pular células com magics/shell\n",
    "#   - linhas começando com %, %%, !\n",
    "#   - chamadas a get_ipython(\n",
    "MAGIC_LINE = re.compile(r\"^\\s*(%{1,2}|!)\", re.M)\n",
    "GET_IPY    = re.compile(r\"get_ipython\\s*\\(\")\n",
    "\n",
    "def has_magics(code: str) -> bool:\n",
    "    return bool(MAGIC_LINE.search(code) or GET_IPY.search(code))\n",
    "\n",
    "def format_code(code: str) -> str:\n",
    "    # isort primeiro, depois black\n",
    "    sorted_code = isort.api.sort_code_string(code, config=ISORT_CFG)\n",
    "    return black.format_str(sorted_code, mode=BLACK_MODE)\n",
    "\n",
    "def summarize_diff(diff_lines: List[str]) -> Tuple[int, int]:\n",
    "    added = removed = 0\n",
    "    for ln in diff_lines:\n",
    "        # ignorar cabeçalhos do diff\n",
    "        if ln.startswith((\"---\", \"+++\", \"@@\")):\n",
    "            continue\n",
    "        if ln.startswith(\"+\"):\n",
    "            added += 1\n",
    "        elif ln.startswith(\"-\"):\n",
    "            removed += 1\n",
    "    return added, removed\n",
    "\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(title)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "if not os.path.exists(NOTEBOOK):\n",
    "    raise FileNotFoundError(f\"Notebook não encontrado:\\n{NOTEBOOK}\")\n",
    "\n",
    "# 3) Leitura do .ipynb\n",
    "with open(NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "changed_cells = []  # (idx, added, removed, diff_text, preview_snippet, new_code)\n",
    "\n",
    "# 4) Pré-visualização célula a célula\n",
    "header(\"Pré-visualização (NÃO grava) — somente células com mudanças\")\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "\n",
    "    original = cell.get(\"source\", \"\")\n",
    "    if not original.strip():\n",
    "        continue\n",
    "\n",
    "    # Pular células com magics/shell\n",
    "    if has_magics(original):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formatted = format_code(original)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] célula {i}: erro no formatador — pulando ({e})\")\n",
    "        continue\n",
    "\n",
    "    if original.strip() != formatted.strip():\n",
    "        # Gerar diff unificado legível\n",
    "        diff = list(difflib.unified_diff(\n",
    "            original.splitlines(), formatted.splitlines(),\n",
    "            fromfile=f\"cell_{i}:before\", tofile=f\"cell_{i}:after\", lineterm=\"\"\n",
    "        ))\n",
    "        add, rem = summarize_diff(diff)\n",
    "        snippet = original.strip().splitlines()[0][:120] if original.strip().splitlines() else \"<célula vazia>\"\n",
    "        changed_cells.append((i, add, rem, \"\\n\".join(diff), snippet, formatted))\n",
    "\n",
    "# 5) Exibição dos diffs por célula (se houver)\n",
    "if not changed_cells:\n",
    "    print(\"✔ Nada a alterar: todas as células (não mágicas) já estão conforme isort/black.\")\n",
    "else:\n",
    "    total_add = total_rem = 0\n",
    "    for (idx, add, rem, diff_text, snippet, _new) in changed_cells:\n",
    "        total_add += add\n",
    "        total_rem += rem\n",
    "        header(f\"Diff — Célula #{idx}  (+{add}/-{rem})\")\n",
    "        print(f\"Primeira linha da célula: {snippet!r}\\n\")\n",
    "        print(diff_text)\n",
    "\n",
    "    header(\"Resumo\")\n",
    "    print(f\"Células com mudanças: {len(changed_cells)}\")\n",
    "    print(f\"Linhas adicionadas:   {total_add}\")\n",
    "    print(f\"Linhas removidas:     {total_rem}\")\n",
    "\n",
    "# 6) Perguntar se aplica\n",
    "if changed_cells:\n",
    "    print(\"\\nDigite 'p' para **Proceder** e gravar as mudanças nessas células, ou 'c' para **Cancelar**.\")\n",
    "    try:\n",
    "        choice = input(\"Proceder (p) / Cancelar (c): \").strip().lower()\n",
    "    except Exception:\n",
    "        choice = \"c\"\n",
    "\n",
    "    if choice == \"p\":\n",
    "        # Backup antes de escrever\n",
    "        backup = NOTEBOOK + \".bak\"\n",
    "        if not os.path.exists(backup):\n",
    "            with open(backup, \"w\", encoding=\"utf-8\") as bf:\n",
    "                nbformat.write(nb, bf)\n",
    "\n",
    "        # Aplicar somente nas células com mudanças\n",
    "        idx_to_new = {idx: new for (idx, _a, _r, _d, _s, new) in changed_cells}\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if i in idx_to_new and cell.get(\"cell_type\") == \"code\":\n",
    "                cell[\"source\"] = idx_to_new[i]\n",
    "\n",
    "        # Escrever no .ipynb\n",
    "        with open(NOTEBOOK, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "\n",
    "        # Sync delay (Drive)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        header(\"Concluído\")\n",
    "        print(f\"✔ Mudanças aplicadas em {len(changed_cells)} célula(s).\")\n",
    "        print(f\"Backup criado em: {backup}\")\n",
    "        print(\"Dica: recarregue o notebook no Colab para ver a formatação atualizada.\")\n",
    "    else:\n",
    "        print(\"\\nOperação cancelada. Nada foi gravado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwzVcwk9Ol0K"
   },
   "source": [
    "##**Sincronizar alterações no código do projeto**\n",
    "\n",
    "---\n",
    "Comandos para sincronizar código (Google Drive, Git, GitHub) e realizar versionamento.\n",
    "\n",
    "Google Drive é considerado o ponto de verdade.\n",
    "\n",
    "Exige que a ETAPA 1 do código tenha sido executada uma vez na sessão.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2hJZaAa2OqEp"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive é a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de versão (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; só 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_AETABULAR (fallback: env; último caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#utilitários silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heurística curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permissões/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local inválido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#configurações do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do repositório no GitHub\n",
    "repo_name      = \"ae-tabular\"    # nome do repositório\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive não montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se é um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta operações pendentes (não apaga histórico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #força branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diretório como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necessário)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normalização\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos já rastreados que hoje são ignorados pelo .gitignore,\n",
    "    limpa o índice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contrário.\n",
    "    \"\"\"\n",
    "    #remove lock de índice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o índice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que estão rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detecção? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de versão\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato legível + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autenticação (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_AETABULAR')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - variável de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_AETABULAR\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#listas de força\n",
    "FORCE_UNTRACK = [\"input/\", \"output/\", \"data/\", \"runs/\", \"logs/\", \"figures/\"]\n",
    "FORCE_TRACK   = [\"references/\"]  #versionar tudo dentro (PDFs inclusive)\n",
    "\n",
    "def force_index_rules():\n",
    "    #garante que pastas sensíveis NUNCA fiquem rastreadas\n",
    "    for p in FORCE_UNTRACK:\n",
    "        try:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "    #garante que references/ SEMPRE entre (útil se ainda há *.pdf globais)\n",
    "    for p in FORCE_TRACK:\n",
    "        try:\n",
    "            git(\"add\", \"-f\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de versão (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudança: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #versão\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma única vez, se necessário)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #aplica regras de força\n",
    "        force_index_rules()\n",
    "\n",
    "        #stage de tudo (Drive é a verdade; remoções entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se só a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se já existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive é a verdade): validação + push forçado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # você é o owner; não perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se não existir (repo vazio), faz um probe genérico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #repositório pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push forçado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. Versão atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem única, curta, sem detalhes sensíveis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KG4WKvaiBxh"
   },
   "source": [
    "# **Introdução**: Autoencoder Tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb24GhmzVAQO"
   },
   "source": [
    "## **O que é um Autoencoder Tabular**\n",
    "\n",
    "---\n",
    "Um autoencoder tabular é um modelo de aprendizado não supervisionado baseado em redes neurais, desenvolvido para processar dados estruturados (tabelas com colunas numéricas e categóricas).\n",
    "Ele aprende a reconstruir as próprias entradas e, em seguida, tenta reconstruí-las com o mínimo erro possível.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmlE3qE6ijLN"
   },
   "source": [
    "## **Como pode ajudar a detectar anomalias**\n",
    "\n",
    "---\n",
    "\n",
    "O modelo é treinado apenas com registros considerados regulares, dentro dos padrões normais, aprendendo a distribuição típica dos lançamentos e pagamentos.\n",
    "\n",
    "Quando um novo registro foge desses padrões — por exemplo, um valor fora de faixa, um centro de custo inusual ou uma combinação de contas atípica — o autoencoder não consegue reconstruí-lo com precisão.\n",
    "\n",
    "A diferença entre o valor original e o reconstruído (erro de reconstrução) é usada como indicador de anomalia, permitindo priorizar revisões contábeis, auditorias e análises de compliance.\n",
    "\n",
    "Essa técnica é útil para detectar erros, duplicidades ou lançamentos indevidos e até a ocorrência de fraudes, sem precisar de exemplos prévios rotulados de irregularidades.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1BPMOQeik9-"
   },
   "source": [
    "## **Como funciona tecnicamente**\n",
    "\n",
    "1. Camada de entrada: recebe as variáveis tabulares (ex.: contas contábeis, valores, datas, centros de custo, usuários).\n",
    "\n",
    "2. Codificador (encoder): reduz a dimensionalidade, ou seja, foca nas variáveis mais relevantes e extrai representações comprimida/reduzida, mas ainda significativa dos dados.\n",
    "\n",
    "3. Camada latente (bottleneck): identifica as características mais importantes do padrão contábil normal.\n",
    "\n",
    "4. Decodificador (decoder): reconstrói os dados originais a partir da representação comprimida.\n",
    "\n",
    "5. Treinamento: o modelo é ajustado para minimizar o erro de reconstrução (ex.: Erro Quadrático Médio - MSE ou Erro Absoluto Médio - MAE).\n",
    "\n",
    "6. Pontuação: durante a operação, cada novo registro recebe uma pontuação *(score)* de anomalia proporcional ao seu erro.\n",
    "\n",
    "7. Revisão manual: as anomalias identificadas são então comunicadas aos gestores do processo para revisão manual e confirmação da natureza: erro ou não.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayolv9Dbi67q"
   },
   "source": [
    "\n",
    "## **Referências de estudos utilizados**\n",
    "---\n",
    "\n",
    "1 JONNALAGADDA, Omkeerthan; RAJU, M.; REDDY, G. S. Deep Risk Profiling: An Autoencoder-Based Framework for Detecting Suspicious Financial Transactions. International Journal of Communication Networks and Information Security, v.17, n.3, 2025.\n",
    "<br><br>\n",
    "2. SU, ICCK. Anomaly Detection in Temporal Financial Series Using Hybrid Autoencoder Architectures. Proceedings of the International Conference on Computing and Knowledge (ICCK), 2025.\n",
    "<br><br>\n",
    "3. YADAV, A. K.; SINGH, G. Anomaly Detection in Financial Transactions Using Advanced Data Mining Algorithms. International Journal of Sciences and Innovation Engineering, v.1, n.3, p.28–34, 2024.\n",
    "<br><br>\n",
    "4. HERNÁNDEZ AROS, Ludivia; BUSTAMANTE MOLANO, Luisa Ximena; GUTIÉRREZ-PORTELA, Fernando; MORENO HERNÁNDEZ, John J.; RODRÍGUEZ BARRERO, Mario S. Financial fraud detection through the application of machine learning techniques: a literature review. Humanities and Social Sciences Communications, v.11, n.1130, 2024. DOI:10.1057/s41599-024-03606-0.\n",
    "<br><br>\n",
    "5. BELLO, O. A.; FOLORUNSO, A.; EJIOFOR, O. E. Enhancing Cyber Financial Fraud Detection Using Deep Learning Techniques: A Study on Neural Networks and Anomaly Detection. International Journal of Network and Communication Research, v.7, n.1, p.90–113, 2022. DOI:10.37745/ijncr.16/vol7n190113.\n",
    "<br><br>\n",
    "6. PINTO, Sarah Oliveira. Abordagens de Detecção de Anomalias em Dados Financeiros. Trabalho de Conclusão de Curso — Universidade de Brasília, 2023. Disponível em: https://bdm.unb.br/bitstream/10483/35738/1/2023_SarahOliveiraPinto_tcc.pdf. Acesso em: 17 out. 2025.\n",
    "<br><br>\n",
    "7. STEFÁNSSON, H. A. Unsupervised Anomaly Detection in Financial Transactions. University of Iceland, 2021. Disponível em: https://skemman.is/bitstream/1946/44727/1/Unsupervised_Anomaly_Detection_in_Financial_Transactions.pdf. Acesso em: 17 out. 2025.\n",
    "<br><br>\n",
    "8. SCHREYER, Marco; SATTAROV, Timur; BORTH, Damian; et al. Detection of Anomalies in Large Scale Accounting Data using Deep Autoencoder Networks. arXiv preprint arXiv:1709.05254, 2017. Disponível em: https://arxiv.org/abs/1709.05254. Acesso em: 17 out. 2025.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HQjxadcsDf-"
   },
   "source": [
    "## **Detalhamento técnico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33t9D-iGV_uQ"
   },
   "source": [
    "### **Visão geral**\n",
    "\n",
    "1. Ambiente + metadados da execução → prepara diretórios (input/, prerun/, output/, artifacts/, runs/, reports/), fixa timezone SP, abre um RUN_DIR com carimbo temporal e salva run.json com contexto de execução.   \n",
    "2. Pré-processamento do CSV (prerun) → valida BOM UTF-8 e separador “,”, padroniza colunas, normaliza tipos (numérico, data, dc, COSIF), gera CSV limpo em prerun/, um snapshot Parquet e um relatório JSON com estatísticas de limpeza.   \n",
    "3. Ingestão de treino/validação → lê o CSV pré-processado de prerun/ com encoding e separador fixos, checa colunas requeridas (username, lotacao, data_lcto, valormi, dc, contacontabil) e domínio de dc (d/c).   \n",
    "4. Vocabulário categórico → constrói mapas inteiros (*_int) para username, lotacao, dc, contacontabil, salva maps, cardinalidades e manifesto; loga as cardinalidades.  \n",
    "5. Engenharia de features → aplica codificação categórica, cria derivadas numéricas (ex.: log1p(valormi)), features de data (mês/semana), e combina tudo; salva features_preview.csv.  \n",
    "6. Treino AE, pontuação e governança:\n",
    "    - a) Limpeza/normalização/split → prepara dataset, imputa e escala (artefatos salvos), gera dataset_npz.npz.\n",
    "    - b) Treino do Autoencoder → salva ae.pt, model_config.json, training_history.csv, erros de reconstrução da validação.  \n",
    "    - c) Pontuação de um lote → reaplica features, imputa/escala com artefatos, calcula erro linha-a-linha; salva scores.csv, score_stats.json, snapshot do CSV pontuado e vetor de erros.  \n",
    "    - d) Calibração do corte (threshold) → três modos: budget (taxa alvo), meta (N alertas), costmin (minimiza custo FP/FN com prevalência). Gera threshold.json e figuras (ex.: hist/KS/PSI).  \n",
    "<br>\n",
    "\n",
    "O pipeline inclui um Relatório HTML **(Etapa 12)** consolidando artefatos, métricas e rastros.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBY47ZLqW5cZ"
   },
   "source": [
    "### **Etapa a etapa**\n",
    "\n",
    "**ETAPA 1** — Ambiente, diretórios, metadados (auditoria)\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Monta Drive (se Colab), cria a estrutura de pastas do projeto e um RUN_DIR carimbado em America/Sao_Paulo; isto dá reprodutibilidade e trilha de auditoria por execução.  \n",
    "\n",
    "Salva runs/[RUN_ID]/run.json com ambiente (host, user, python, paths, seed, deps).  \n",
    "\n",
    "**Decisões fixas:** estrutura de pastas; TIMEZONE = “America/Sao_Paulo”; salvar run.json.\n",
    "\n",
    "**Calibráveis:** PROJ_ROOT, SEED, lista NEED_PIP.\n",
    "\n",
    "**Arquivos:** run.json (conteúdo: metadados de execução).\n",
    "\n",
    "**Salvo em memória:** variáveis de caminho (e.g., RUN_DIR).\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 2** — Pré-processamento (prerun/)\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Lista CSVs em input/, valida formato (BOM + separador “,”), padroniza nomes, normaliza tipos, checa colunas obrigatórias e persiste o CSV “clean” em prerun/ + Parquet + relatório JSON de limpeza. Isso garante padronização e auditabilidade ex-ante.    \n",
    "\n",
    "**Decisões fixas:** exigência UTF-8 BOM e separador “,”; domínios válidos de dc.\n",
    "\n",
    "**Calibráveis:** mapping de renome de colunas, REQUIRED_COLS.\n",
    "\n",
    "**Arquivos:** prerun/[base]-clean-[time].csv (+ .parquet) e runs/[RUN_ID]/preprocess_report_[base]-[time].json (estatísticas: NA, vazios, dc inválido, COSIF não-numérico, etc.).\n",
    "\n",
    "**Salvo em memória:** apenas df temporários.\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 3** — Ingestão de treino/val\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Carrega de prerun/ com leitura estrita (encoding/sep fixos), valida colunas necessárias e domínio de dc.  \n",
    "\n",
    "\n",
    "**Decisões fixas**: CSV_ENCODING=\"utf-8-sig\", CSV_SEP=\",\", DC_VALIDOS={\"d\",\"c\"}.\n",
    "\n",
    "**Arquivos:** apenas logs; etapa serve para preparar DF_RAW.\n",
    "\n",
    "**Salvo em memória:** DF_RAW.\n",
    "Rastreabilidade: logs e pré-checagens ajudam a reconstruir o insumo usado.\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 4** — Vocabulário categórico (dimensionalidade)\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Constrói mapas inteiros para username, lotacao, dc, contacontabil, salva maps, frequências, cardinalidades e manifesto (facilita reuso na inferência e transparência).\n",
    "\n",
    "**Decisões fixas**: colunas categóricas (username, lotacao, dc, contacontabil) e sufixo _int.\n",
    "\n",
    "**Arquivos:** categorical_maps.json, categorical_cardinality.json, categorical_frequencies_*.csv, vocab_manifest_*.json.\n",
    "\n",
    "**Salvo em memória:** categorical_maps.\n",
    "\n",
    "**Rastreabilidade:** cardinalidades e manifesto salvos registram o “estado” do vocabulário por execução.\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 5** — Engenharia de features (tabular → numérico)\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Codifica categóricas com os maps da ETAPA 4, cria derivadas (ex.: feat_log_valormi), adiciona features de data (mês/trimestre) e produz prévia para inspeção.\n",
    "\n",
    "**Decisões fixas:** NUMERIC_BASE_COLS=[\"valormi\"], CAT_SUFFIX=\"_int\".\n",
    "\n",
    "**Itens calibráveis:** ligar/desligar derivadas (FEATURE_DERIVATIONS).\n",
    "\n",
    "**Arquivos:** features_preview.csv (no RUN_DIR).\n",
    "\n",
    "**Salvo em memória:** DF_FEATURES/FEATURE_COLS (descritas como mantidas em memória).\n",
    "\n",
    "**Rastreabilidade:** prefixos feat_* nos dados deixam claro o que foi derivado.\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 6** — Limpeza → normalização → split (artefatos)\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Imputa/escala os dados de treino, guarda artefatos de transformação e persiste dataset(s) para o treino do AE; gera dataset_npz.npz.\n",
    "\n",
    "**Arquivos:** imputer.joblib, scaler.joblib, dataset_npz.npz. Os artefatos são versionados por RUN_DIR.\n",
    "\n",
    "**Salvo em memória:** os arrays de treino/validação.\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 7** — Treinamento do Autoencoder\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Treina um MLP AE simétrico com gargalo (bottleneck), registra histórico de loss e salva configuração de modelo; exporta pesos (ae.pt) e erros de reconstrução da validação (suportam calibração, KS:Kolmogorov-Smirnov /PSI:Population Stability Index e explicabilidade).  \n",
    "\n",
    "**Arquivos:** ae.pt, model_config.json, training_history.csv, reconstruction_errors_val.npy.\n",
    "\n",
    "**Salvo em memória:** objeto do modelo durante o treino.\n",
    "\n",
    "**Rastreabilidade/Explicabilidade:** histórico de perda + config documentada apoiam reprodutibilidade e justificativa de hiperparâmetros.\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 8** — Pontuação (inferência) de um CSV\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Carrega artefatos das ETAPAS 5 até ETAPA 7, replica a engenharia de features, imputa/escala, infere AE e calcula erro de reconstrução por linha; salva pontuação (score) em arquivo .csv.\n",
    "\n",
    "**Arquivos:** scores.csv, score_stats.json, selected_source.csv e reconstruction_errors_score.npy; scores.csv inclui colunas de contexto + score.\n",
    "\n",
    "**Salvo em memória:** vetor recon_error para gráficos/relatórios.\n",
    "\n",
    "---\n",
    "\n",
    "** ETAPA 9** — Calibração do threshold (budget | meta | costmin)\n",
    "\n",
    "<u>O que faz e por quê</u>\n",
    "\n",
    "Define corte do score via:\n",
    "budget (quantil pela taxa-alvo), meta (número de alertas), costmin (minimização de custo Falso Positivo/Falso Negativo com prevalência).\n",
    "\n",
    "**Calibráveis:** modos de corte; c_fp (custo do falso positivo), c_fn (custo do falso negativo), p no costmin; metas de taxa/N nos outros.\n",
    "\n",
    "**Arquivos:** threshold.json, drift_hist.png, scores_summary.json (sumário).\n",
    "\n",
    "**Rastreabilidade:** JSON de threshold + estatísticas val vs. atual (KS/PSI) sustentam a governança de corte.\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 10** — Marcação de anomalias (resultado operacional)\n",
    "\n",
    "<b>TO DO - CORREÇÕES\n",
    "\n",
    "Observação: não localizei um bloco explícito de “marcação final” (ex.: alerts.csv com is_alert = score >= threshold). Recomendo uma célula dedicada que:\n",
    "\n",
    "Leia scores.csv + threshold.json, gere alerts.csv (com identificadores chaves e is_alert), e amostras estratificadas (ex.: alerts_top100.csv).\n",
    "\n",
    "Motivos: separar “pontuação” de “decisão”, e deixar a política de corte auditável.</b>\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 11** — Monitoramento de drift\n",
    "\n",
    "Identificação no fluxo de KS/PSI e figuras derivadas do histograma de erros para validar estabilidade entre a distribuição dos dados de validação e a distribuição do lote atual.\n",
    "\n",
    "<b>TO DO - CORREÇÕES\n",
    "\n",
    "Garantir que arquivos/informações sejam salvas.</b>\n",
    "\n",
    "---\n",
    "\n",
    "**ETAPA 12** — Relatório (HTML/PDF)\n",
    "\n",
    "Consolida artefatos (paths), métricas, figuras e logs em relatório HTML dentro do RUN_DIR e/ou em reports/.\n",
    "\n",
    "**Arquivos**: relatório HTML com imagens embedded na pasta reports/.\n",
    "\n",
    "---\n",
    "\n",
    "Recomendações específicas (alinhadas ao objetivo “mensal/trimestral”)\n",
    "\n",
    "1. Marcação operacional\n",
    "\n",
    "Padronizar alerts.csv com colunas-chave (ex.: data, lotação, conta, dc, valormi, score, threshold, is_alert).\n",
    "\n",
    "Persistir amostras (ex.: top-N por unidade ou conta) para revisão humana.\n",
    "\n",
    "---\n",
    "\n",
    "**RESUMO TÉCNICO**\n",
    "\n",
    "<u>O que fica só em memória vs. o que é persistido (síntese)</u>\n",
    "\n",
    "- Apenas em memória: DF_RAW (ingestão), categorical_maps (também salvo em disco), FEATURE_COLS/DF_FEATURES (processo), arrays de treino/val (antes de persistir), recon_error (além de salvo em .npy).  \n",
    "\n",
    "- Persistido (principais):\n",
    "  - Ambiente: run.json.\n",
    "  - Pré-processo: *-clean-<ts>.csv, *.parquet, preprocess_report_*.json.\n",
    "  - Vocabulário: categorical_maps.json, categorical_cardinality.json, categorical_frequencies_*.csv, vocab_manifest_*.json.\n",
    "  - Treino: dataset_npz.npz, imputer.joblib, scaler.joblib, ae.pt, model_config.json, training_history.csv, reconstruction_errors_val.npy.  \n",
    "  - Pontuação: scores.csv, score_stats.json, selected_source.csv, reconstruction_errors_score.npy.  \n",
    "  - Calibração: threshold.json (+ figuras KS/PSI/hist).\n",
    "  - Relatório: HTML consolidado.\n",
    "\n",
    "---\n",
    "\n",
    "**Explicabilidade:** o que o pipeline preserva.\n",
    "\n",
    "- Reprodutibilidade por RUN_DIR e run.json (contexto completo da execução).\n",
    "\n",
    "- Transparência de insumo pela dupla prerun/*.csv + preprocess_report_*.json.\n",
    "\n",
    "- Rastreio de transformações por vocabulário e artefatos de imputação/escala (nomeados por execução).  \n",
    "\n",
    "- Justificativa de modelo com model_config.json + training_history.csv.\n",
    "\n",
    "- Tomada de decisão explícita via threshold.json.\n",
    "\n",
    "- Análise de estabilidade com KS/PSI e histograma de erros (validação vs. atual).\n",
    "\n",
    "- Relato final com relatório HTML.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJJxFx9IbAVa"
   },
   "source": [
    "# **Exemplo:** Caso hipotético simplificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYJZ1cACbF47"
   },
   "source": [
    "## Exemplo de registros contábeis para treino:\n",
    "\n",
    "| username | lotacao | dc | contacontabil | nome_conta | valormi | data_lcto  |\n",
    "|-----------|----------|----|---------------|-------------|----------|-------------|\n",
    "| leobr     | local1   | d  | 111000000     | exemplo     | 100.00   | 15/10/2025  |\n",
    "| leobr     | local1   | c  | 311000000     | exemplo     | 100.00   | 15/10/2025  |\n",
    "| leobr     | local1   | c  | 211000000     | exemplo     | 1000.00  | 20/10/2025  |\n",
    "| leobr     | local1   | d  | 111000000     | exemplo     | 1000.00  | 20/10/2025  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcHQ68wYbZLj"
   },
   "source": [
    "- **Etapa 1** não altera registros;\n",
    "- **Etapa 2** padroniza formato de registros, conforme a seguir:\n",
    "\n",
    "   • normaliza strings (trim/lower), garante dc € fd, c>, zera/padroniza contacontabil como string numérica;\n",
    "\n",
    "   • converte valormi para float (valormi_float);\n",
    "\n",
    "   • cria sinal_dc (+1 para \"d\", -1 para \"c\") e valor_signed = sinal dc x valormi_float;\n",
    "\n",
    "   • parseia a data e deriva ano, mes_num, tri_num (Q1..04), dia, ym\n",
    "   (YYYY-MM);\n",
    "   \n",
    "   • extrai recortes da conta como códigos puramente numéricos\n",
    "   (sem rotular o significado): conta_grupo=conta [0],\n",
    "   conta_subgrupo2=conta[:2],conta_classe3=conta[:3]; e\n",
    "\n",
    "   • constrói chaves úteis para agregações futuras (frequências e\n",
    "   valores): chave_user_conta_dc, chave_user_lotacao.\n",
    "\n",
    "| username | lotacao | dc | contacontabil | nome_conta | valormi | valormi_float | sinal_dc | valor_signed | data_lcto  | data_dt    | ano  | mes_num | tri_num | dia | ym      | conta_grupo | conta_subgrupo2 | conta_classe3 | chave_user_conta_dc          | chave_user_lotacao |\n",
    "|----------|---------|----|---------------|------------|---------|---------------|----------|--------------|------------|------------|------|---------|---------|-----|---------|-------------|-----------------|---------------|-------------------------------|--------------------|\n",
    "| leobr    | local1  | d  | 111000000     | exemplo    | 100.00  | 100.00        | 1        | 100.00       | 15/10/2025 | 2025-10-15 | 2025 | 10      | 4       | 15  | 2025-10 | 1           | 11              | 111           | leobr|111000000|d               | leobr|local1       |\n",
    "| leobr    | local1  | c  | 311000000     | exemplo    | 100.00  | 100.00        | -1       | -100.00      | 15/10/2025 | 2025-10-15 | 2025 | 10      | 4       | 15  | 2025-10 | 3           | 31              | 311           | leobr|311000000|c               | leobr|local1       |\n",
    "| leobr    | local1  | c  | 211000000     | exemplo    | 1000.00 | 1000.00       | -1       | -1000.00     | 20/10/2025 | 2025-10-20 | 2025 | 10      | 4       | 20  | 2025-10 | 2           | 21              | 211           | leobr|211000000|c               | leobr|local1       |\n",
    "| leobr    | local1  | d  | 111000000     | exemplo    | 1000.00 | 1000.00       | 1        | 1000.00      | 20/10/2025 | 2025-10-20 | 2025 | 10      | 4       | 20  | 2025-10 | 1           | 11              | 111           | leobr|111000000|d               | leobr|local1       |\n",
    "\n",
    "<br>\n",
    "- **Etapa 3** expande o dataset com agregações básicas (por usuário, conta, DC, lotação e períodos)\n",
    "\n",
    "| username | lotacao | contacontabil | dc | mes_num | tri_num | freq_mes_user_total | freq_tri_user_total | freq_mes_lotacao_total | freq_tri_lotacao_total | freq_mes_user_conta_dc | val_mes_user_conta_dc | val_med_mes_user_conta_dc |\n",
    "|-----------|----------|---------------|----|----------|----------|--------------------|--------------------|------------------------|------------------------|------------------------|-----------------------|---------------------------|\n",
    "| leobr     | local1   | 111000000     | d  | 10       | 4        | 4                  | 4                  | 4                      | 4                      | 2                      | 1100.00               | 550.00                    |\n",
    "| leobr     | local1   | 311000000     | c  | 10       | 4        | 4                  | 4                  | 4                      | 4                      | 1                      | 100.00                | 100.00                    |\n",
    "| leobr     | local1   | 211000000     | c  | 10       | 4        | 4                  | 4                  | 4                      | 4                      | 1                      | 1000.00               | 1000.00                   |\n",
    "\n",
    "*note que há consolidação de contas para contabilizações do mesmo usuario/lotacao.\n",
    "<br>\n",
    "\n",
    "- **Etapa 4** normaliza e realiza derivações de variáveis (z-scores, proporções e índices)\n",
    "\n",
    "| Tipo de operação | Descrição | Exemplo |\n",
    "|------------------|------------|----------|\n",
    "| Padronização z-score | Para cada variável contínua, calcula-se 𝑧 = (𝑥 − μ)/σ sobre todo o conjunto (ou subset) | val_mes_user_conta_dc_z |\n",
    "| Proporções relativas | Divide valores de um grupo por totais do mesmo período | prop_user_conta_dc_mes = val_mes_user_conta_dc / sum(val_mes_user_conta_dc do usuário no mês) |\n",
    "| Normalização robusta | Opcionalmente usa mediana e IQR para robustez a outliers | valormi_float_robust |\n",
    "| Reordenação e consistência | Garante ordem de colunas conforme FEATURES_COLS | — |\n",
    "\n",
    "• val mes user conta_ dc_z captura desvios de valor dentro do comportamento típico do usuário;\n",
    "\n",
    "• prop_user_conta_dc_mes captura relevância proporcional da conta no total movimentado; e\n",
    "\n",
    "◦ freq_* preservam padrőes de recorrência temporal.\n",
    "\n",
    "O resultado dessa etapa conforme o exemplo é:\n",
    "\n",
    "| username | lotacao | contacontabil | dc | ano  | mes_num | tri_num | val_mes_user_conta_dc | val_mes_user_conta_dc_z | prop_user_conta_dc_mes | freq_mes_user_conta_dc | freq_mes_user_total |\n",
    "|----------|---------|---------------|----|------|---------|---------|------------------------|-------------------------|------------------------|------------------------|---------------------|\n",
    "| leobr    | local1  | 111000000     | d  | 2025 | 10      | 4       | 1100.00                | 0.78                    | 0.5000                 | 2                      | 4                   |\n",
    "| leobr    | local1  | 311000000     | c  | 2025 | 10      | 4       | 100.00                 | -1.35                   | 0.0455                 | 1                      | 4                   |\n",
    "| leobr    | local1  | 211000000     | c  | 2025 | 10      | 4       | 1000.00                | 0.57                    | 0.4545                 | 1                      | 4                   |\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Etapa 5** transforma os dados preparados em tensores e os normaliza para treino. Converte os features da etapa anterior em uma matriz numérica.\n",
    "\n",
    "\n",
    "| Procedimento | Descrição | Resultado esperado |\n",
    "|---------------|------------|--------------------|\n",
    "| Seleção de FEATURES_COLS | Mantém apenas as variáveis numéricas informativas para o AE | 4 a 50 colunas, conforme engenharia aplicada |\n",
    "| Substituição de NaN / inf | Preenche nulos com 0 ou média (conforme config) | Nenhum valor ausente |\n",
    "| Normalização Min-Max (0–1) | Escala cada variável para o intervalo [0,1] | Facilita o treino estável da rede |\n",
    "| Montagem de matriz X | Transforma o dataframe em matriz numpy | X.shape = (n_registros, n_features) |\n",
    "| Split temporal (opcional) | Divide em treino e validação por data ou amostra | X_train, X_val |\n",
    "\n",
    "<br>\n",
    "No exemplo:\n",
    "\n",
    "| username | contacontabil | val_mes_user_conta_dc_z | prop_user_conta_dc_mes | freq_mes_user_conta_dc | freq_mes_user_total |\n",
    "|-----------|---------------|-------------------------|------------------------|------------------------|---------------------|\n",
    "| leobr     | 111000000     | 1.000                   | 1.0000                 | 1.000                  | 1.000               |\n",
    "| leobr     | 311000000     | 0.000                   | 0.0000                 | 0.000                  | 1.000               |\n",
    "| leobr     | 211000000     | 0.845                   | 0.9091                 | 0.000                  | 1.000               |\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Etapa 6** divide os dados em treino e validação.\n",
    "\n",
    "**X_train** → shape = (2, 4)\n",
    "| val_mes_user_conta_dc_z | prop_user_conta_dc_mes | freq_mes_user_conta_dc | freq_mes_user_total |\n",
    "|--------------------------|------------------------|------------------------|---------------------|\n",
    "| 1.000                   | 1.0000                 | 1.000                  | 1.000               |\n",
    "| 0.000                   | 0.0000                 | 0.000                  | 1.000               |\n",
    "\n",
    "**X_val** → shape = (1, 4)\n",
    "| val_mes_user_conta_dc_z | prop_user_conta_dc_mes | freq_mes_user_conta_dc | freq_mes_user_total |\n",
    "|--------------------------|------------------------|------------------------|---------------------|\n",
    "| 0.845                   | 0.9091                 | 0.000                  | 1.000               |\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Etapa 7** treina o autoencoder tabular\n",
    "\n",
    "### Tabela — Arquitetura do modelo\n",
    "\n",
    "| Componente | Descrição |\n",
    "|-------------|------------|\n",
    "| Input Layer | Dimensão = número de features (ex.: 4) |\n",
    "| Encoder | 2–4 camadas totalmente conectadas (Dense), cada uma reduzindo dimensionalidade |\n",
    "| Bottleneck (Latent Space) | Dimensão reduzida (ex.: 2) — representação comprimida dos padrões |\n",
    "| Decoder | Espelho do encoder, reconstruindo o input |\n",
    "| Funções de ativação | ReLU nas camadas intermediárias, Linear na saída |\n",
    "| Loss Function | MSE (Mean Squared Error) entre input e output |\n",
    "| Otimizador | Adam(lr=1e-3) |\n",
    "| Critério de parada | Early Stopping com paciência (ex.: 5 épocas sem melhora no val_loss) |\n",
    "\n",
    "<br>\n",
    "Tabela — Execução passo a passo\n",
    "\n",
    "| Etapa | Descrição | Saída |\n",
    "|--------|------------|--------|\n",
    "| 1 | Inicializa pesos da rede (seed fixa p/ reprodutibilidade) | model.state_dict() |\n",
    "| 2 | Loop de treinamento: forward → loss → backward → step | curvas de loss |\n",
    "| 3 | Avaliação a cada época: train_loss e val_loss | monitoramento de estabilidade |\n",
    "| 4 | Salva melhor modelo (menor val_loss) | RUN_DIR/model.pt |\n",
    "| 5 | Gera gráficos de convergência | loss_curve.png |\n",
    "\n",
    "<br>\n",
    "Tabela — Arquivos gerados\n",
    "\n",
    "| Arquivo | Conteúdo | Função |\n",
    "|----------|-----------|--------|\n",
    "| model.pt | Pesos treinados do AE | Reutilizado na Etapa 8 |\n",
    "| model_config.json | Arquitetura e parâmetros | Documentação do modelo |\n",
    "| loss_curve.png | Curva de treinamento | Avaliação visual |\n",
    "| train_stats.json | Histórico de losses | Auditoria e rastreabilidade |\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Etapa 8** faz a inferência se novos registros indicam erro de reconstrução e são candidatos à anomalia.\n",
    "\n",
    "Em termos do nosso exemplo, considere que os novos registros abaixo foram recebidos.\n",
    "\n",
    "| username | lotacao | dc | contacontabil | nome_conta | valormi  | data_lcto  |\n",
    "|-----------|----------|----|---------------|-------------|-----------|-------------|\n",
    "| leobr     | local1   | c  | 211000000     | exemplo     | 1000.00   | 05/11/2025  |\n",
    "| leobr     | local1   | d  | 111000000     | exemplo     | 1000.00   | 05/11/2025  |\n",
    "| leobr     | local1   | c  | 211000000     | exemplo     | 100000.00 | 10/11/2025  |\n",
    "| leobr     | local1   | d  | 111000000     | exemplo     | 100000.00 | 10/11/2025  |\n",
    "\n",
    "<br>\n",
    "Principais atividades:\n",
    "\n",
    "| Etapa | Descrição | Saída |\n",
    "|--------|------------|--------|\n",
    "| 1 | Reaplicar os mesmos tratamentos das Etapas 2–5 (normalização, proporções, z-scores) | Dados compatíveis com o modelo |\n",
    "| 2 | Carregar model.pt e FEATURES_COLS | Modelo e ordem das colunas |\n",
    "| 3 | Calcular reconstruções X_recon = AE(X_input) | Saída reconstruída |\n",
    "| 4 | Calcular erro de reconstrução (MSE, MAE, etc.) | Score de erro por linha |\n",
    "| 5 | Comparar erro com limiar de anomalia (threshold.json) | Classificação: normal / anômalo |\n",
    "| 6 | Salvar resultados (scores, flags, figuras) | scores_summary.json, drift_hist.png, drift_cdf.png |\n",
    "\n",
    "<br>\n",
    "Resultado da aplicação no exemplo:\n",
    "\n",
    "| username | lotacao | dc | contacontabil | valormi | data_lcto | reconstruction_error | is_anomaly |\n",
    "|-----------|----------|----|---------------|----------|------------|----------------------|-------------|\n",
    "| leobr     | local1   | c  | 211000000     | 1000.00  | 05/11/2025 | 0.06                 | 0 |\n",
    "| leobr     | local1   | d  | 111000000     | 1000.00  | 05/11/2025 | 0.05                 | 0 |\n",
    "| leobr     | local1   | c  | 211000000     | 100000.00| 10/11/2025 | 0.85                 | 1 |\n",
    "| leobr     | local1   | d  | 111000000     | 100000.00| 10/11/2025 | 0.82                 | 1 |\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Etapa 9** calibra o número de alerta de acordo com a opção desejada (budget)\n",
    "\n",
    "- **Etapa 10** marca as anomalias no arquivo CSV original, informando o score de erro e sua classificação (rank)\n",
    "\n",
    "- **Etapa 11** compara drift do modelo (diferença entre a distribuição dos novos dados e dos dados de treinamento, o que indica a necessidade de retreinar o modelo).\n",
    "\n",
    "| Métrica | Interpretação |\n",
    "|----------|----------------|\n",
    "| mean_train_loss | erro médio do treino |\n",
    "| mean_val_loss | erro médio da validação |\n",
    "| mean_current_loss | erro médio dos dados novos |\n",
    "| KS(val vs atual) | medida de diferença entre distribuições; indica estabilidade ou drift |\n",
    "| PSI | índice de estabilidade populacional; detecta mudanças de comportamento |\n",
    "| threshold | limite adotado para marcação de anomalias |\n",
    "| taxa_val_anomalias | proporção de anomalias na base de validação |\n",
    "| taxa_atual_anomalias | proporção de anomalias na base atual avaliada |\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Etapa 12** apenas gera relatório com principais informações.\n",
    "\n",
    "- **Etapa 13** envia o relatório sem dados brutos para que uma LLM possa avaliar os resultados e emitir uma opinião com sugestões de aprimoramento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BL2I4I9uZKo"
   },
   "source": [
    "# **Checklist operacional**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt9CwUvIWJ4-"
   },
   "source": [
    "## **Gestão do Ambiente**\n",
    "---\n",
    "\n",
    "Utilizado apenas na criação do projeto e atualização de alterações de código (versionamento).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s473SDXSuquz"
   },
   "source": [
    "\n",
    "## **Utilização em treino:**\n",
    "---\n",
    "\n",
    "- Etapa 1 - Setup do ambiente virtual e atualização das instalações necessárias;\n",
    "\n",
    "- Etapa 2 - Preparo de arquivos para treino;\n",
    "\n",
    "- Etapa 3 - Ingestão de dados para treino;\n",
    "\n",
    "- Etapa 4 - Vocabulário de treino (dimensionalidade);\n",
    "\n",
    "- Etapa 5 - Engenharia de Features;\n",
    "\n",
    "- Etapa 6 - Limpeza, normalização e split (treino/val);\n",
    "\n",
    "- Etapa 7 - Encoder (MLP simétrico), Bottleneck (LATENT_DIM) e Decoder (construção de layers_dec e self.decoder);\n",
    "\n",
    "- Etapa 8 - (A e B) Geram arquivo simulado - (C) Pontuação de anomalias com base no modelo treinado; e\n",
    "\n",
    "- Etapas 9 **ATÉ** 12 - **não aplicável na etapa de treino**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qfg9eF4urjq"
   },
   "source": [
    "## **Utilização em produção:**</u>\n",
    "\n",
    "---\n",
    "\n",
    "- Etapa 1 - Setup do ambiente virtual e atualização das instalações necessárias;\n",
    "\n",
    "- Etapa 2 - Preparo de arquivos para execução;\n",
    "\n",
    "- Etapas 3 **ATÉ** 7 - **não executar em produção**;\n",
    "\n",
    "- Etapa 8 - *(Subitem C)* Pontuação de anomalias com base no modelo treinado;\n",
    "\n",
    "- Etapa 9 - Calibração do corte de anomalias;\n",
    "\n",
    "- Etapa 10 - Marcação das anomalias e geração do arquivo resultado;\n",
    "\n",
    "- Etapa 11 - Monitoramento do drift do algoritmo; e\n",
    "\n",
    "- Etapa 12 - Informações de destaque e relatório.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVu46r3fvtxW"
   },
   "source": [
    "# **Etapa 0:** Caso necessário, gerar dados sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GutvsfeF0bku"
   },
   "source": [
    "## **Gerador de dados sintéticos**\n",
    "\n",
    "---\n",
    "\n",
    "Geração de input sintético (COSIF, estável, CSV UTF-8 BOM VÍRGULA) + Diagnósticos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gENo-Wcg2NXC"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from __future__ import annotations\n",
    "import csv, random, math, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from zoneinfo import ZoneInfo  # timezone São Paulo (America/Sao_Paulo)\n",
    "\n",
    "print(\"Skynet Informa: Gerando dados sintéticos com distribuição uniforme e estável.\")\n",
    "\n",
    "# ---------------- util & config ----------------\n",
    "def _sk(msg:str):\n",
    "    print(f\"{msg}\")\n",
    "\n",
    "def parse_ddmmyyyy(s: str) -> date:\n",
    "    return datetime.strptime(s.strip(), \"%d/%m/%Y\").date()\n",
    "\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# --- Pastas do projeto (fixas) ---\n",
    "PROJ_ROOT = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "INPUT_DIR = PROJ_ROOT / \"input\"\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Para compatibilidade com o restante do código:\n",
    "BASE_DIR = PROJ_ROOT\n",
    "OUTPUT_DIR = INPUT_DIR\n",
    "\n",
    "# Grupos (rótulos informativos)\n",
    "GRUPO_DESC = {\n",
    "    \"1\": \"Ativo\",\n",
    "    \"2\": \"Ativo\",\n",
    "    \"3\": \"Compensação\",\n",
    "    \"4\": \"Passivo\",\n",
    "    \"6\": \"Patrimônio Líquido\",\n",
    "    \"7\": \"Resultado\",\n",
    "    \"9\": \"Compensação\"\n",
    "}\n",
    "GRUPOS_VALIDOS = list(GRUPO_DESC.keys())\n",
    "\n",
    "# Catálogo controlado (populares) por grupo -> lista de (subgrupo, detalhe, nome_base)\n",
    "# Observação: são exemplos sintéticos coerentes; ajuste se quiser espelhar seu plano de contas real.\n",
    "CATALOGO = {\n",
    "    \"1\": [(\"1\",\"0\",\"Ativo Circulante\"), (\"2\",\"1\",\"Realizável Curto Prazo\"), (\"3\",\"0\",\"Caixa e Equivalentes\")],\n",
    "    \"2\": [(\"1\",\"0\",\"Ativo Não Circulante\"), (\"2\",\"0\",\"Investimentos\"), (\"3\",\"1\",\"Imobilizado\")],\n",
    "    \"3\": [(\"0\",\"0\",\"Contas de Compensação\"), (\"1\",\"0\",\"Riscos em Garantias\")],\n",
    "    \"4\": [(\"1\",\"0\",\"Passivo Circulante\"), (\"2\",\"1\",\"Obrigações Curto Prazo\"), (\"3\",\"0\",\"Fornecedores\")],\n",
    "    \"6\": [(\"0\",\"1\",\"Capital Social\"), (\"1\",\"0\",\"Reservas\"), (\"2\",\"0\",\"Ajustes Patrimoniais\")],\n",
    "    \"7\": [(\"1\",\"0\",\"Receitas Operacionais\"), (\"2\",\"0\",\"Despesas Operacionais\"), (\"3\",\"0\",\"Outras Receitas/Despesas\")],\n",
    "    \"9\": [(\"0\",\"0\",\"Compensação Diversa\")]\n",
    "}\n",
    "\n",
    "# Sufixos controlados (do 4º dígito em diante) para reforçar repetição\n",
    "SUFIXOS_COMUNS = [\"000001\", \"000010\", \"001000\", \"010000\", \"123456\", \"654321\"]\n",
    "\n",
    "# Faixas de valores por grupo (todas estreitas e comportadas)\n",
    "VALOR_POR_GRUPO = {\n",
    "    \"1\": (120.00, 220.00),\n",
    "    \"2\": (150.00, 250.00),\n",
    "    \"3\": (80.00, 160.00),\n",
    "    \"4\": (120.00, 220.00),\n",
    "    \"6\": (180.00, 260.00),\n",
    "    \"7\": (100.00, 200.00),\n",
    "    \"9\": (80.00, 160.00),\n",
    "}\n",
    "# Probabilidade de aplicar regra D em {1,2,7} e C em {4,6,7}\n",
    "PAREAMENTO_CONTABIL_P = 0.8\n",
    "\n",
    "# ---------------- entrada do usuário ----------------\n",
    "try:\n",
    "    n_lanc_str = input(\"Quantos lançamentos (pares d/c) deseja gerar? [ex.: 1000]: \").strip()\n",
    "    n_lanc = int(n_lanc_str) if n_lanc_str else 1000\n",
    "    if n_lanc <= 0: raise ValueError\n",
    "except Exception:\n",
    "    _sk(\"Entrada inválida. Usando padrão de 1000 lançamentos.\")\n",
    "    n_lanc = 1000\n",
    "\n",
    "def _safe_input_date(prompt, default_str):\n",
    "    s = input(f\"{prompt} [default={default_str}]: \").strip()\n",
    "    if not s: s = default_str\n",
    "    try:\n",
    "        return parse_ddmmyyyy(s)\n",
    "    except Exception:\n",
    "        _sk(\"Data inválida. Usando default.\")\n",
    "        return parse_ddmmyyyy(default_str)\n",
    "\n",
    "data_ini = _safe_input_date(\"Data inicial (DD/MM/AAAA)\", \"01/01/2025\")\n",
    "data_fim = _safe_input_date(\"Data final   (DD/MM/AAAA)\", \"30/09/2025\")\n",
    "if data_fim < data_ini:\n",
    "    _sk(\"Intervalo invertido; trocando datas.\")\n",
    "    data_ini, data_fim = data_fim, data_ini\n",
    "\n",
    "# Conjunto de (ano, mês) no intervalo\n",
    "def meses_no_intervalo(d0: date, d1: date):\n",
    "    ms = []\n",
    "    y, m = d0.year, d0.month\n",
    "    while (y < d1.year) or (y == d1.year and m <= d1.month):\n",
    "        ms.append((y, m))\n",
    "        if m == 12:\n",
    "            y += 1; m = 1\n",
    "        else:\n",
    "            m += 1\n",
    "    return ms\n",
    "\n",
    "MESES = meses_no_intervalo(data_ini, data_fim)\n",
    "if not MESES:\n",
    "    _sk(\"Nenhum mês no intervalo. Abortando.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Para datas: escolheremos dias no \"miolo\" (5..25) e forçaremos pertencer ao mês/ano válidos\n",
    "DIAS_VALIDOS = list(range(5, 26))\n",
    "\n",
    "# ---------------- desenho de população ----------------\n",
    "# Número de usuários cresce sublinearmente ao total (estabilidade por usuário)\n",
    "n_users = max(8, round(math.sqrt(n_lanc)))                # cardinalidade de username\n",
    "n_lot   = max(3, min( max(3, n_users // 3), n_users-1))   # cardinalidade de lotacao < username\n",
    "\n",
    "lotacoes = [f\"LOT{idx:03d}\" for idx in range(1, n_lot+1)]\n",
    "usernames = [f\"user{idx:03d}\" for idx in range(1, n_users+1)]\n",
    "user2lot = {u: lotacoes[i % n_lot] for i, u in enumerate(usernames)}\n",
    "\n",
    "# ---------------- plano estável por mês/usuário ----------------\n",
    "total_meses = len(MESES)\n",
    "base_por_user_mes = n_lanc // (n_users * total_meses)\n",
    "resto = n_lanc % (n_users * total_meses)\n",
    "plano = {u: {m: base_por_user_mes for m in MESES} for u in usernames}\n",
    "slots = [(u, m) for u in usernames for m in MESES]\n",
    "random.shuffle(slots)\n",
    "for i in range(resto):\n",
    "    u, m = slots[i]\n",
    "    plano[u][m] += 1\n",
    "\n",
    "# ---------------- utilitários de geração ----------------\n",
    "def rand_conta_do_catalogo(grupo: str):\n",
    "    # usa catálogo; se grupo não no catálogo (não deve ocorrer), fallback aleatório\n",
    "    if grupo in CATALOGO and CATALOGO[grupo]:\n",
    "        s, d, nome_base = random.choice(CATALOGO[grupo])\n",
    "        sufixo = random.choice(SUFIXOS_COMUNS)\n",
    "        conta = f\"{grupo}{s}{d}{sufixo}\"\n",
    "        nome = f\"{nome_base} {grupo}{s}{d}\"\n",
    "        return conta, nome, grupo, s, d\n",
    "    # fallback\n",
    "    s = str(random.randint(0, 9))\n",
    "    d = str(random.randint(0, 9))\n",
    "    sufixo = random.choice(SUFIXOS_COMUNS)\n",
    "    conta = f\"{grupo}{s}{d}{sufixo}\"\n",
    "    nome = f\"{GRUPO_DESC.get(grupo,'Grupo') } {grupo}{s}{d}\"\n",
    "    return conta, nome, grupo, s, d\n",
    "\n",
    "def rand_grupo_para_dc(dc: str) -> str:\n",
    "    # Probabilisticamente impõe coerência contábil\n",
    "    coerente = random.random() < PAREAMENTO_CONTABIL_P\n",
    "    if dc == \"d\":\n",
    "        return random.choice([\"1\",\"2\",\"7\"]) if coerente else random.choice(GRUPOS_VALIDOS)\n",
    "    else:\n",
    "        return random.choice([\"4\",\"6\",\"7\"]) if coerente else random.choice(GRUPOS_VALIDOS)\n",
    "\n",
    "def rand_valor_por_grupo(grupo: str) -> float:\n",
    "    lo, hi = VALOR_POR_GRUPO.get(grupo, (100.0, 200.0))\n",
    "    return round(random.uniform(lo, hi), 2)\n",
    "\n",
    "def rand_data_no_mes(ano:int, mes:int) -> str:\n",
    "    # escolhe um dia válido entre 5..25, ajustando se exceder fim de mês curto\n",
    "    dia = random.choice(DIAS_VALIDOS)\n",
    "    # Ajuste simplificado: limita dia a 28 para segurança geral\n",
    "    if mes == 2 and dia > 28: dia = 28\n",
    "    if mes in (4,6,9,11) and dia > 30: dia = 30\n",
    "    # Garante pertencer ao intervalo global data_ini..data_fim\n",
    "    dt = date(ano, mes, dia)\n",
    "    if dt < data_ini: dt = data_ini\n",
    "    if dt > data_fim: dt = data_fim\n",
    "    return dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# ---------------- geração dos lançamentos ----------------\n",
    "rows = []\n",
    "doc_counter = 0\n",
    "\n",
    "for u in usernames:\n",
    "    lot = user2lot[u]\n",
    "    for (y, m), qnt in plano[u].items():\n",
    "        for _ in range(qnt):\n",
    "            doc_counter += 1\n",
    "            docnum = f\"{lot}-DOC{y}{m:02d}-{doc_counter:06d}\"\n",
    "            dt = rand_data_no_mes(y, m)\n",
    "\n",
    "            # Seleção de grupos por regra contábil suave\n",
    "            g_d = rand_grupo_para_dc(\"d\")\n",
    "            g_c = rand_grupo_para_dc(\"c\")\n",
    "\n",
    "            # Contas do catálogo por grupo\n",
    "            conta_d, nome_d, g1, s1, d1 = rand_conta_do_catalogo(g_d)\n",
    "            conta_c, nome_c, g2, s2, d2 = rand_conta_do_catalogo(g_c)\n",
    "\n",
    "            # Valor \"comportado\" por grupo do débito (base) — mantém simetria no crédito\n",
    "            val = rand_valor_por_grupo(g1)\n",
    "\n",
    "            rows.append({\n",
    "                \"username\": u,\n",
    "                \"lotacao\": lot,\n",
    "                \"dc\": \"d\",\n",
    "                \"contacontabil\": conta_d,\n",
    "                \"nome_conta\": nome_d,\n",
    "                \"documento_num\": docnum,\n",
    "                \"valormi\": f\"{val:.2f}\",\n",
    "                \"data_lcto\": dt\n",
    "            })\n",
    "            rows.append({\n",
    "                \"username\": u,\n",
    "                \"lotacao\": lot,\n",
    "                \"dc\": \"c\",\n",
    "                \"contacontabil\": conta_c,\n",
    "                \"nome_conta\": nome_c,\n",
    "                \"documento_num\": docnum,\n",
    "                \"valormi\": f\"{-val:.2f}\",\n",
    "                \"data_lcto\": dt\n",
    "            })\n",
    "\n",
    "# ---------------- escrita do CSV (UTF-8 BOM) ----------------\n",
    "now_sp = datetime.now(ZoneInfo(\"America/Sao_Paulo\"))\n",
    "ts_sp = now_sp.strftime(\"%Y%m%d-%H%M%S-%Z\")  # ex.: 20251019-214512-BRT\n",
    "\n",
    "outfile = OUTPUT_DIR / f\"sintetico_AE_{ts_sp}.csv\"\n",
    "fieldnames = [\"username\",\"lotacao\",\"dc\",\"contacontabil\",\"nome_conta\",\"documento_num\",\"valormi\",\"data_lcto\"]\n",
    "\n",
    "with open(outfile, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, dialect=\"excel\")\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "_sk(f\"Arquivo gerado em: {outfile}\")\n",
    "_sk(f\"Caminho da pasta: {outfile.parent}\")\n",
    "_sk(f\"Nome do arquivo : {outfile.name}\")\n",
    "_sk(f\"Linhas: {len(rows)}  | Lançamentos (pares d/c): {doc_counter}\")\n",
    "_sk(f\"Usuários: {n_users} | Lotações: {n_lot} (lotacao < username: {'ok' if n_lot < n_users else 'NOK'})\")\n",
    "\n",
    "# ---------------- Diagnósticos (tabelas) ----------------\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 1) Número de registros em branco por campo\n",
    "def is_blank(x):\n",
    "    return (x is None) or (str(x).strip() == \"\")\n",
    "blank_counts = {col: int(df[col].map(is_blank).sum()) for col in df.columns}\n",
    "diag_blanks = pd.DataFrame([\n",
    "    {\"campo\": col, \"qtd_brancos\": blank_counts[col]}\n",
    "    for col in df.columns\n",
    "]).sort_values(\"campo\")\n",
    "\n",
    "print(\"\\n=== (1) Registros em branco por campo ===\")\n",
    "print(diag_blanks.to_string(index=False))\n",
    "\n",
    "# 2) Categóricos: valor menos e mais frequente\n",
    "categoricos = [\"username\",\"lotacao\",\"dc\",\"contacontabil\",\"nome_conta\",\"documento_num\"]\n",
    "linhas = []\n",
    "for col in categoricos:\n",
    "    vc = df[col].value_counts(dropna=False)\n",
    "    if vc.empty:\n",
    "        linhas.append({\"campo\": col, \"mais_freq_valor\": None, \"mais_freq_qtd\": 0,\n",
    "                       \"menos_freq_valor\": None, \"menos_freq_qtd\": 0})\n",
    "        continue\n",
    "    mais_val, mais_qtd = vc.index[0], int(vc.iloc[0])\n",
    "    menos_val, menos_qtd = vc.index[-1], int(vc.iloc[-1])\n",
    "    linhas.append({\n",
    "        \"campo\": col,\n",
    "        \"menos_freq_valor\": menos_val, \"menos_freq_qtd\": menos_qtd,\n",
    "        \"mais_freq_valor\": mais_val,   \"mais_freq_qtd\":  mais_qtd\n",
    "    })\n",
    "diag_categ = pd.DataFrame(linhas)\n",
    "\n",
    "print(\"\\n=== (2) Categóricos: valor menos e mais frequente ===\")\n",
    "print(diag_categ.to_string(index=False))\n",
    "\n",
    "# 3) Data: menor e maior data\n",
    "# (CRIAR coluna _date antes de usar)\n",
    "df[\"_date\"] = pd.to_datetime(df[\"data_lcto\"], format=\"%d/%m/%Y\").dt.date\n",
    "min_data = df[\"_date\"].min()\n",
    "max_data = df[\"_date\"].max()\n",
    "print(\"\\n=== (3) Datas ===\")\n",
    "print(f\"Menor data: {min_data.strftime('%d/%m/%Y')}  |  Maior data: {max_data.strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# 4) Para cada grupo (1º dígito), subgrupo (2 primeiros), detalhamento (3 primeiros):\n",
    "#    confirma se ∑ valormi = 0 (OK/NOK). Observação: como o pareamento D/C pode ocorrer em grupos diferentes,\n",
    "#    não há garantia de zerar por prefixo; o relatório mostrará OK/NOK explicitamente.\n",
    "def prefixo(s, n):\n",
    "    return s[:n] if isinstance(s, str) and len(s) >= n else None\n",
    "\n",
    "# Criar coluna numérica para somatórios\n",
    "df[\"valormi_float\"] = df[\"valormi\"].astype(float)\n",
    "\n",
    "df[\"_g1\"] = df[\"contacontabil\"].map(lambda x: prefixo(x,1))\n",
    "df[\"_g2\"] = df[\"contacontabil\"].map(lambda x: prefixo(x,2))\n",
    "df[\"_g3\"] = df[\"contacontabil\"].map(lambda x: prefixo(x,3))\n",
    "\n",
    "rep_g1 = (df.groupby(\"_g1\")[\"valormi_float\"].sum().reset_index()\n",
    "            .assign(status=lambda d: d[\"valormi_float\"].apply(lambda v: \"OK\" if abs(v)<1e-6 else \"NOK\"))\n",
    "            .rename(columns={\"_g1\":\"grupo\",\"valormi_float\":\"soma_valormi\"}))\n",
    "rep_g2 = (df.groupby(\"_g2\")[\"valormi_float\"].sum().reset_index()\n",
    "            .assign(status=lambda d: d[\"valormi_float\"].apply(lambda v: \"OK\" if abs(v)<1e-6 else \"NOK\"))\n",
    "            .rename(columns={\"_g2\":\"subgrupo\",\"valormi_float\":\"soma_valormi\"}))\n",
    "rep_g3 = (df.groupby(\"_g3\")[\"valormi_float\"].sum().reset_index()\n",
    "            .assign(status=lambda d: d[\"valormi_float\"].apply(lambda v: \"OK\" if abs(v)<1e-6 else \"NOK\"))\n",
    "            .rename(columns={\"_g3\":\"detalhe\",\"valormi_float\":\"soma_valormi\"}))\n",
    "\n",
    "print(\"\\n=== (4a) Soma por GRUPO (1 dígito) ===\")\n",
    "print(rep_g1.to_string(index=False))\n",
    "print(\"\\n=== (4b) Soma por SUBGRUPO (2 dígitos) ===\")\n",
    "print(rep_g2.to_string(index=False))\n",
    "print(\"\\n=== (4c) Soma por DETALHE (3 dígitos) ===\")\n",
    "print(rep_g3.to_string(index=False))\n",
    "\n",
    "# Também confirma soma por documento (deve ser sempre 0)\n",
    "doc_sums = df.groupby(\"documento_num\")[\"valormi_float\"].sum().abs().max()\n",
    "print(\"\\n=== (4d) Soma por DOCUMENTO (deve ser 0 sempre) ===\")\n",
    "print(f\"Max |∑ valormi| por documento: {doc_sums:.6f}  -> {'OK' if doc_sums < 1e-6 else 'NOK'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Poo6XnSe24zr"
   },
   "source": [
    "### **Select 10 registros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "__bTnmtB2_3B"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Seleciona 10 registros aleatórios do DataFrame\n",
    "amostra = df.sample(n=10, random_state=42)\n",
    "\n",
    "# Exibe no console\n",
    "print(\"\\n=== (5) Amostra de 10 registros aleatórios ===\")\n",
    "print(amostra.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PQAz7fGxy4P"
   },
   "source": [
    "## **Detalhes do Simulador de Dados Sintéticos — AE-Tabular**\n",
    "\n",
    "### 1. Objetivo\n",
    "Gerar um **dataset sintético controlado**, representando **lançamentos contábeis** para fins de **teste e validação** do modelo *Autoencoder Tabular* utilizado na detecção de anomalias em registros contábeis e financeiros.  \n",
    "O simulador visa reproduzir **padrões operacionais típicos** de contabilização, mantendo coerência entre **usuário, lotação, contas, valores, natureza (D/C)** e **datas**, sem introduzir outliers ou ruídos artificiais.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Estrutura dos Dados\n",
    "\n",
    "| Campo | Tipo | Descrição |\n",
    "|-------|------|-----------|\n",
    "| `username` | categórico | Identificador do usuário responsável pelo lançamento. Cada usuário pertence a uma única lotação. |\n",
    "| `lotacao` | categórico | Unidade organizacional associada ao usuário. Cardinalidade menor que a de `username`. |\n",
    "| `dc` | categórico | Natureza contábil do movimento (`d` = débito, `c` = crédito). |\n",
    "| `contacontabil` | categórico | Código da conta contábil no padrão COSIF (1º dígito = grupo, 2º = subgrupo, 3º = detalhamento). |\n",
    "| `nomeconta` | texto | Nome sintético da conta contábil associada. |\n",
    "| `documento_num` | categórico | Identificador único de lançamento, formado por prefixo da lotação e número sequencial (`LOT###-DOCYYYYMM-XXXXXX`). |\n",
    "| `valormi` | numérico | Valor monetário do lançamento. Débitos e créditos de um mesmo documento se anulam (soma = 0). |\n",
    "| `data_lcto` | data | Data de lançamento no formato `DD/MM/AAAA`. Distribuída de forma uniforme entre as datas de início e fim informadas pelo usuário. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Critérios de Geração\n",
    "\n",
    "#### 3.1. Controle de Volume e Distribuição\n",
    "- O usuário informa o **número total de lançamentos (pares D/C)** desejado.  \n",
    "- Cada lançamento gera **duas linhas** — um **débito** e um **crédito** — garantindo soma zero por documento.  \n",
    "- A distribuição de lançamentos é **uniforme e estável** por:\n",
    "  - usuário,  \n",
    "  - lotação,  \n",
    "  - mês,  \n",
    "  - trimestre.\n",
    "\n",
    "> 🔹 *Objetivo:* preservar consistência operacional e evitar variações abruptas que o modelo poderia interpretar como anomalias.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2. Cardinalidade Controlada\n",
    "- A cardinalidade de **lotação** é propositalmente **menor que a de usuários**, refletindo estrutura hierárquica organizacional.  \n",
    "- Cada `username` está associado **exclusivamente a uma lotação fixa** (1:1).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3. Contas Contábeis — Padrão COSIF\n",
    "- O campo `contacontabil` segue o **padrão COSIF**:\n",
    "  - **1º dígito:** grupo contábil (1–Ativo, 2–Ativo, 3–Compensação, 4–Passivo, 6–Patrimônio Líquido, 7–Resultado, 9–Compensação);  \n",
    "  - **2º dígito:** subgrupo;  \n",
    "  - **3º dígito:** detalhamento;  \n",
    "  - **4º em diante:** dígitos livres (preenchidos com sufixos padronizados para reforçar repetição natural).  \n",
    "\n",
    "- Cada grupo possui um **catálogo controlado de subgrupos e detalhes** (`CATALOGO`), com **nomes sintéticos coerentes** (ex.: “Caixa e Equivalentes”, “Fornecedores”, “Receitas Operacionais”).  \n",
    "- O objetivo é gerar **regularidade sem uniformidade total**, permitindo ao AE-Tabular aprender padrões contábeis sem ruído excessivo.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.4. Coerência Contábil (Pareamento D/C)\n",
    "- Implementado um **mecanismo probabilístico** de coerência contábil:\n",
    "  - Débitos tendem a ocorrer em grupos {1, 2, 7}.  \n",
    "  - Créditos tendem a ocorrer em grupos {4, 6, 7}.  \n",
    "  - Em **80% dos casos**, essa coerência é respeitada; nos 20% restantes, ocorre pareamento cruzado para representar exceções normais.  \n",
    "- Cada documento contém **duas linhas complementares (d/c)** com **mesmo valor e data**, garantindo que a soma seja **sempre zero**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.5. Faixas de Valores e “Comportamento Operacional”\n",
    "- Valores são sorteados em **intervalos estreitos e controlados** por grupo contábil (`VALOR_POR_GRUPO`), evitando outliers.  \n",
    "- As faixas variam entre **R$ 80,00 e R$ 260,00**, refletindo movimentações típicas e “comportadas”.  \n",
    "- A distribuição é **quase uniforme** dentro de cada faixa.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.6. Datas de Lançamento\n",
    "- O usuário informa **data inicial e final** do período de simulação (formato `DD/MM/AAAA`).  \n",
    "- As datas são sorteadas **entre o 5º e o 25º dia** de cada mês, garantindo realismo e evitando concentrações em extremos.  \n",
    "- Não há lacunas: todos os registros possuem data válida dentro do intervalo.  \n",
    "- As contagens por mês e trimestre são estáveis.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.7. Identificação e Estrutura de Documento\n",
    "- Cada par D/C compartilha um mesmo número de documento (`documento_num`).  \n",
    "- A estrutura inclui prefixo da **lotação**, reforçando origem hierárquica do lançamento.  \n",
    "- Exemplo: `LOT005-DOC202506-000124`.\n",
    "\n",
    "### 4. Critérios de Qualidade e Diagnóstico\n",
    "\n",
    "Após a geração, o simulador executa **rotinas automáticas de verificação e auditoria** para garantir integridade, completude e coerência dos dados.\n",
    "\n",
    "| Verificação | Descrição | Resultado Esperado |\n",
    "|--------------|------------|--------------------|\n",
    "| **Campos em branco** | Conta o número de registros vazios por coluna. | Zero para todos os campos. |\n",
    "| **Distribuição categórica** | Identifica o valor mais e menos frequente para `username`, `lotacao`, `dc`, `contacontabil`, `nomeconta`, `documento_num`. | Distribuição aproximadamente uniforme, sem concentração excessiva. |\n",
    "| **Valores médios** | Calcula médias e médias absolutas de `valormi` por `lotacao` e `username` nos períodos **mês** e **trimestre**. | Estabilidade entre períodos. |\n",
    "| **Intervalo de datas** | Retorna menor e maior data de lançamento. | Dentro do intervalo informado pelo usuário. |\n",
    "| **Soma por documento** | Verifica se ∑ `valormi` = 0 em cada `documento_num`. | Sempre **OK**. |\n",
    "| **Soma por grupo/subgrupo/detalhe** | Verifica ∑ `valormi` por prefixos COSIF (1, 2, 3 dígitos). | Pode conter **NOK**, pois há lançamentos intergrupos realistas. |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Saída e Codificação\n",
    "- O arquivo é salvo em `./input/` com nome padrão: sintetico_AE_.csv\n",
    "\n",
    "- Codificação: **UTF-8 com BOM**, separador **vírgula (`,`)**.\n",
    "- O cabeçalho contém exatamente os campos definidos na estrutura de dados (seção 2).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Racional de Projeto\n",
    "- **Uniformidade sem artificialidade:** distribuição estável, mas não perfeitamente regular.  \n",
    "- **Realismo contábil controlado:** coerência de D/C e uso do padrão COSIF.  \n",
    "- **Ausência de outliers:** valores e frequências dentro de limites plausíveis.  \n",
    "- **Rastreabilidade total:** logs via função `_sk()` com timestamps e checagens automáticas.  \n",
    "- **Auditabilidade:** diagnósticos tabulares permitem validar integridade e completude do dataset antes do treinamento do AE-Tabular.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vda5PsLZrSp"
   },
   "source": [
    "# **Etapa 1:** Setup do ambiente e criação de RUN_DIR\n",
    "t= 1min (max 2min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91314,
     "status": "ok",
     "timestamp": 1761050457196,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "b9bdUiJqfUIt",
    "outputId": "c066562e-33a6-484a-cf13-adac0af00343"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Preparar o ambiente da execução:\n",
    "- (sentinela) Pré-flight 1x por sessão: desabilitar Dynamo e fixar SymPy compatível.\n",
    "- Montar Google Drive (se em Colab) e garantir a estrutura de diretórios.\n",
    "- Instalar dependências mínimas (sem pin de torch).\n",
    "- Fixar SEED e timezone; abrir/associar um RUN_DIR e salvar/atualizar metadados.\n",
    "\n",
    "Pontos FIXOS:\n",
    "- TIMEZONE = \"America/Sao_Paulo\"\n",
    "- Estrutura: input/, prerun/, output/, artifacts/, runs/, reports/\n",
    "- Metadados: runs/<RUN_ID>/run.json\n",
    "\n",
    "Pontos CALIBRÁVEIS:\n",
    "- PROJ_ROOT\n",
    "- NEED_PIP\n",
    "- SEED\n",
    "\"\"\"\n",
    "\n",
    "print(\"Skynet Informa: Preparo do ambiente.\")\n",
    "\n",
    "# =========================\n",
    "# §1.0 — Pré-flight com sentinela (roda apenas 1x por sessão)\n",
    "# =========================\n",
    "import os, sys, subprocess, pathlib\n",
    "\n",
    "SENTINEL = pathlib.Path(\"/content/.ae_preflight_done\")  # arquivo marca 1x por sessão (Colab)\n",
    "\n",
    "# Variáveis de ambiente — sempre reativadas (idempotentes)\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"   # desliga TorchDynamo (estabilidade)\n",
    "os.environ[\"PYTHONNOUSERSITE\"] = \"1\"      # ignora user site-packages do Colab (PEP-668)\n",
    "\n",
    "if not SENTINEL.exists():\n",
    "    # Garantir SymPy compatível com torch._dynamo\n",
    "    # (PEP-668: é necessário --break-system-packages no Python do sistema)\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"--upgrade\", \"--break-system-packages\", \"sympy==1.12\"\n",
    "    ])\n",
    "    SENTINEL.write_text(\"ok\")\n",
    "    print(\"Pré-flight aplicado (Dynamo off, SymPy=1.12).\")\n",
    "else:\n",
    "    print(\"Pré-flight já aplicado nesta sessão.\")\n",
    "\n",
    "# =========================\n",
    "# §1.1 — Imports base e utilitários\n",
    "# =========================\n",
    "import json, platform, textwrap, random, hashlib, socket, getpass, re, io, base64, time, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "\n",
    "def _sk(msg: str) -> None:\n",
    "    \"\"\"Logger simples (sem prefixos).\"\"\"\n",
    "    print(msg)\n",
    "\n",
    "def _warn(msg: str) -> None:\n",
    "    warnings.warn(msg, RuntimeWarning, stacklevel=2)\n",
    "\n",
    "# =========================\n",
    "# §1.2 — Detecção de “venv” (apenas para registro; sem uso no Colab)\n",
    "# =========================\n",
    "VENV_INFO = {\n",
    "    \"python_exe\": sys.executable,\n",
    "    \"base_prefix\": sys.base_prefix,\n",
    "    \"prefix\": sys.prefix,\n",
    "    \"venv_active\": (sys.prefix != sys.base_prefix)\n",
    "}\n",
    "_sk(f\"python: {VENV_INFO['python_exe']}\")\n",
    "_sk(f\"venv ativa? {VENV_INFO['venv_active']}\")\n",
    "\n",
    "# =========================\n",
    "# §1.3 — Montagem do Google Drive (se em Colab)\n",
    "# =========================\n",
    "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
    "DRIVE_MOUNT = Path(\"/content/drive\")\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive as _colab_drive  # type: ignore\n",
    "        if not DRIVE_MOUNT.exists() or not os.path.ismount(str(DRIVE_MOUNT)):\n",
    "            _sk(\"Montando Google Drive…\")\n",
    "            _colab_drive.mount(str(DRIVE_MOUNT))\n",
    "        else:\n",
    "            _sk(\"Google Drive já montado.\")\n",
    "    except Exception as e:\n",
    "        _warn(f\"Não foi possível montar o Google Drive automaticamente: {e}\")\n",
    "\n",
    "# =========================\n",
    "# §1.4 — Raiz do projeto e estrutura de diretórios\n",
    "# =========================\n",
    "DEFAULT_DRIVE_ROOT = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "DEFAULT_LOCAL_ROOT = Path.cwd() / \"ae-tabular\"\n",
    "\n",
    "PROJ_ROOT = (\n",
    "    DEFAULT_DRIVE_ROOT if (IN_COLAB and DEFAULT_DRIVE_ROOT.parent.exists())\n",
    "    else DEFAULT_LOCAL_ROOT\n",
    ")\n",
    "os.makedirs(PROJ_ROOT, exist_ok=True)\n",
    "\n",
    "INPUT_DIR    = PROJ_ROOT / \"input\"\n",
    "PRERUN_DIR   = PROJ_ROOT / \"prerun\"\n",
    "OUTPUT_DIR   = PROJ_ROOT / \"output\"\n",
    "ARTIF_DIR    = PROJ_ROOT / \"artifacts\"\n",
    "RUNS_DIR     = PROJ_ROOT / \"runs\"\n",
    "REPORTS_DIR  = PROJ_ROOT / \"reports\"\n",
    "\n",
    "for d in (INPUT_DIR, PRERUN_DIR, OUTPUT_DIR, ARTIF_DIR, RUNS_DIR, REPORTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_sk(f\"PROJ_ROOT = {PROJ_ROOT}\")\n",
    "_sk(\"Estrutura ok (input/, prerun/, output/, artifacts/, runs/, reports/)\")\n",
    "\n",
    "# =========================\n",
    "# §1.5 — Carimbo temporal, timezone e escolha do RUN_DIR (novo ou existente)\n",
    "# =========================\n",
    "TIMEZONE = \"America/Sao_Paulo\"\n",
    "_now = datetime.now(TZ)\n",
    "\n",
    "def _list_runs(runs_dir: Path):\n",
    "    subs = [d for d in runs_dir.iterdir() if d.is_dir()]\n",
    "    # ordena por mtime (mais recente primeiro)\n",
    "    subs.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    items = []\n",
    "    for d in subs:\n",
    "        rid = d.name\n",
    "        rj  = d / \"run.json\"\n",
    "        created = None\n",
    "        try:\n",
    "            if rj.exists():\n",
    "                with rj.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                    meta = json.load(f)\n",
    "                created = meta.get(\"created_at\")\n",
    "        except Exception:\n",
    "            created = None\n",
    "        nfiles = sum(1 for _ in d.rglob(\"*\") if _.is_file())\n",
    "        items.append((d, rid, created, nfiles))\n",
    "    return items\n",
    "\n",
    "def _choose_run_dir():\n",
    "    # Tenta ler preferência por variável de ambiente (automatização opcional)\n",
    "    auto = os.getenv(\"AE_RUN_CHOICE\", \"\").strip().lower()  # \"new\", \"exist\"\n",
    "    if auto not in (\"new\",\"exist\",\"\"):\n",
    "        auto = \"\"\n",
    "    try:\n",
    "        if not auto:\n",
    "            ans = input(\"Criar novo RUN_DIR? [N=novo / e=existente] (default=N): \").strip().lower()\n",
    "        else:\n",
    "            ans = \"n\" if auto==\"new\" else \"e\"\n",
    "    except (EOFError, KeyboardInterrupt):\n",
    "        ans = \"n\"\n",
    "\n",
    "    if ans in (\"\", \"n\", \"nao\", \"não\"):\n",
    "        # novo\n",
    "        run_id  = _now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        run_dir = RUNS_DIR / run_id\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        _sk(f\"RUN_ID (novo) = {run_id}\")\n",
    "        _sk(f\"RUN_DIR (novo) = {run_dir}\")\n",
    "        return run_id, run_dir, True\n",
    "\n",
    "    # existente\n",
    "    items = _list_runs(RUNS_DIR)\n",
    "    if not items:\n",
    "        _sk(\"Nenhum RUN_DIR existente encontrado; criando um novo.\")\n",
    "        run_id  = _now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        run_dir = RUNS_DIR / run_id\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return run_id, run_dir, True\n",
    "\n",
    "    print(\"\\nSelecione um RUN_DIR existente:\")\n",
    "    for i, (d, rid, created, nfiles) in enumerate(items):\n",
    "        created_s = f\" | criado: {created}\" if created else \"\"\n",
    "        print(f\"  [{i}] {rid}  ({nfiles} arquivos){created_s}\")\n",
    "\n",
    "    choice = None\n",
    "    for _try in range(3):\n",
    "        try:\n",
    "            raw = input(\"Digite o índice do RUN_DIR (ou Enter para cancelar e criar novo): \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            raw = \"\"\n",
    "        if raw == \"\":\n",
    "            # fallback: novo\n",
    "            run_id  = _now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            run_dir = RUNS_DIR / run_id\n",
    "            run_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Nenhuma seleção feita. Criando RUN_DIR novo.\")\n",
    "            return run_id, run_dir, True\n",
    "        if raw.isdigit():\n",
    "            idx = int(raw)\n",
    "            if 0 <= idx < len(items):\n",
    "                choice = items[idx][0]\n",
    "                break\n",
    "        print(\"Índice inválido. Tente novamente.\")\n",
    "\n",
    "    if choice is None:\n",
    "        print(\"Seleção não concluída. Criando RUN_DIR novo.\")\n",
    "        run_id  = _now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        run_dir = RUNS_DIR / run_id\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return run_id, run_dir, True\n",
    "\n",
    "    run_dir = choice\n",
    "    run_id  = run_dir.name\n",
    "    _sk(f\"RUN_ID (existente) = {run_id}\")\n",
    "    _sk(f\"RUN_DIR (existente) = {run_dir}\")\n",
    "    return run_id, run_dir, False\n",
    "\n",
    "RUN_ID, RUN_DIR, NEW_RUN = _choose_run_dir()\n",
    "\n",
    "# =========================\n",
    "# §1.6 — Dependências (pip) + imports centrais\n",
    "# =========================\n",
    "NEED_PIP = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"pyarrow\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    # NÃO fixe torch aqui no Colab; use o que já vem no ambiente\n",
    "    \"reportlab\",\n",
    "    \"sympy==1.12\",   # redundante ao pré-flight, mas inócuo\n",
    "]\n",
    "\n",
    "def _pip_install_missing(pkgs):\n",
    "    \"\"\"Tenta importar; se falhar, instala via pip no MESMO Python do kernel (com PEP-668 no Colab).\"\"\"\n",
    "    to_install = []\n",
    "    for spec in pkgs:\n",
    "        name = spec.split(\"==\")[0].split(\">=\")[0].split(\"<=\")[0].split(\"[\")[0]\n",
    "        try:\n",
    "            __import__(name)\n",
    "        except Exception:\n",
    "            to_install.append(spec)\n",
    "    if to_install:\n",
    "        _sk(f\"Instalando pacotes: {to_install}\")\n",
    "        extra = [\"--break-system-packages\"] if sys.prefix == sys.base_prefix else []\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *to_install, *extra])\n",
    "    else:\n",
    "        _sk(\"Todas dependências já presentes.\")\n",
    "\n",
    "_pip_install_missing(NEED_PIP)\n",
    "\n",
    "# Imports centrais (torch apenas após o pré-flight)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except Exception:\n",
    "    TORCH_OK = False\n",
    "    _warn(\"PyTorch não disponível — o treino do autoencoder (Etapa 7) exigirá Torch.\")\n",
    "\n",
    "# =========================\n",
    "# §1.7 — SEED / determinismo (quando viável)\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "if TORCH_OK:\n",
    "    try:\n",
    "        torch.manual_seed(SEED)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(SEED)\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "        _sk(f\"torch determinístico habilitado (cuda={torch.cuda.is_available()})\")\n",
    "    except Exception as e:\n",
    "        _warn(f\"Determinismo Torch parcial: {e}\")\n",
    "\n",
    "# =========================\n",
    "# §1.8 — Metadados (criar se novo; atualizar se existente)\n",
    "# =========================\n",
    "RUN_META_PATH = RUN_DIR / \"run.json\"\n",
    "if NEW_RUN or not RUN_META_PATH.exists():\n",
    "    RUN_META = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"created_at\": _now.isoformat(),\n",
    "        \"last_attached_at\": _now.isoformat(),\n",
    "        \"timezone\": TIMEZONE,\n",
    "        \"host\": socket.gethostname(),\n",
    "        \"user\": getpass.getuser() if hasattr(getpass, \"getuser\") else None,\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"paths\": {\n",
    "            \"proj_root\": str(PROJ_ROOT),\n",
    "            \"input_dir\": str(INPUT_DIR),\n",
    "            \"prerun_dir\": str(PRERUN_DIR),\n",
    "            \"output_dir\": str(OUTPUT_DIR),\n",
    "            \"artifacts_dir\": str(ARTIF_DIR),\n",
    "            \"runs_dir\": str(RUNS_DIR),\n",
    "            \"reports_dir\": str(REPORTS_DIR),\n",
    "            \"run_dir\": str(RUN_DIR),\n",
    "        },\n",
    "        \"venv\": VENV_INFO,\n",
    "        \"seed\": SEED,\n",
    "        \"deps\": NEED_PIP,\n",
    "    }\n",
    "    RUN_META_PATH.write_text(json.dumps(RUN_META, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    _sk((\"Metadados criados\" if NEW_RUN else \"Metadados inicializados\") + \" em runs/<RUN_ID>/run.json\")\n",
    "else:\n",
    "    try:\n",
    "        meta = json.loads(RUN_META_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        meta = {}\n",
    "    # Atualiza apenas um campo leve de anexo (não sobrescreve histórico)\n",
    "    meta[\"last_attached_at\"] = _now.isoformat()\n",
    "    # Sincroniza paths (caso projeto tenha sido movido)\n",
    "    meta.setdefault(\"paths\", {})\n",
    "    meta[\"paths\"].update({\n",
    "        \"proj_root\": str(PROJ_ROOT),\n",
    "        \"input_dir\": str(INPUT_DIR),\n",
    "        \"prerun_dir\": str(PRERUN_DIR),\n",
    "        \"output_dir\": str(OUTPUT_DIR),\n",
    "        \"artifacts_dir\": str(ARTIF_DIR),\n",
    "        \"runs_dir\": str(RUNS_DIR),\n",
    "        \"reports_dir\": str(REPORTS_DIR),\n",
    "        \"run_dir\": str(RUN_DIR),\n",
    "    })\n",
    "    RUN_META_PATH.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    _sk(\"Metadados atualizados (last_attached_at) em runs/<RUN_ID>/run.json\")\n",
    "\n",
    "# =========================\n",
    "# §1.9 — Resumo\n",
    "# =========================\n",
    "print(\"\\n==== Etapa 1 — AMBIENTE PRONTO ====\")\n",
    "print(f\"PROJ_ROOT  :  {PROJ_ROOT}\")\n",
    "print(f\"INPUT_DIR  :  {INPUT_DIR}\")\n",
    "print(f\"PRERUN_DIR :  {PRERUN_DIR}\")\n",
    "print(f\"OUTPUT_DIR :  {OUTPUT_DIR}\")\n",
    "print(f\"ARTIF_DIR  :  {ARTIF_DIR}\")\n",
    "print(f\"RUNS_DIR   :  {RUNS_DIR}\")\n",
    "print(f\"REPORTS_DIR:  {REPORTS_DIR}\")\n",
    "print(f\"RUN_DIR    :  {RUN_DIR}   (novo={NEW_RUN})\")\n",
    "print(f\"SEED       :  {SEED}\")\n",
    "print(f\"TIMEZONE   :  {TIMEZONE}\")\n",
    "print(f\"TORCH_OK   :  {TORCH_OK}\")\n",
    "print(\"================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV2gzWLQaIF5"
   },
   "source": [
    "# **Etapa 2:** Utilitário de pré-processamento de arquivos\n",
    "\n",
    "<b>Melhoria: informar o número de registros processados</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "K0rR07EdnRPe"
   },
   "outputs": [],
   "source": [
    "# @title §A — APENAS CASO NECESSÁRIO - Conversão de ponto e vírgula em vírgula\n",
    "# @title Converter CSV de ';' para ',' com seleção por índice e opções de sobrescrever/backup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Skynet Informa: Convertendo arquivo para separação por vírgula.\")\n",
    "\n",
    "# -------- Configuráveis --------\n",
    "INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular/input\")\n",
    "OUTPUT_SUBDIR = \"converted_commas\"   # Usado quando NÃO sobrescrever\n",
    "ENCODING_READ = \"utf-8-sig\"          # lê UTF-8 com BOM\n",
    "ENCODING_WRITE = \"utf-8-sig\"         # grava UTF-8 com BOM\n",
    "KEEP_INDEX = False                   # não salvar índice no CSV\n",
    "\n",
    "# -------- Funções utilitárias --------\n",
    "def _ask_yes_no(msg: str) -> bool:\n",
    "    while True:\n",
    "        ans = input(msg).strip().lower()\n",
    "        if ans in (\"s\", \"n\"):\n",
    "            return ans == \"s\"\n",
    "        print(\"Por favor, responda com 's' ou 'n'.\")\n",
    "\n",
    "def _human_size(nbytes: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if nbytes < 1024:\n",
    "            return f\"{nbytes:.0f} {unit}\"\n",
    "        nbytes /= 1024\n",
    "    return f\"{nbytes:.1f} PB\"\n",
    "\n",
    "def convert_semicolon_to_comma(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Leitura preservando texto (acentos) e sem converter \"NA\"/\"NaN\" automaticamente\n",
    "    df = pd.read_csv(\n",
    "        src,\n",
    "        sep=';',\n",
    "        dtype=str,\n",
    "        encoding=ENCODING_READ,\n",
    "        quoting=csv.QUOTE_MINIMAL,\n",
    "        keep_default_na=False\n",
    "    )\n",
    "    # Escrita com vírgula e BOM\n",
    "    df.to_csv(\n",
    "        dst,\n",
    "        sep=',',\n",
    "        index=KEEP_INDEX,\n",
    "        encoding=ENCODING_WRITE,\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "\n",
    "# -------- Execução --------\n",
    "assert INPUT_DIR.exists(), f\"Diretório não encontrado: {INPUT_DIR}\"\n",
    "\n",
    "csv_files = sorted(INPUT_DIR.glob(\"*.csv\"))\n",
    "if not csv_files:\n",
    "    print(f\"[aviso] Nenhum .csv encontrado em {INPUT_DIR}\")\n",
    "    sys.exit(0)\n",
    "\n",
    "print(f\"Arquivos .csv em {INPUT_DIR}:\")\n",
    "for i, f in enumerate(csv_files):\n",
    "    try:\n",
    "        stat = f.stat()\n",
    "        mtime = datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{i:02d}] {f.name}  —  { _human_size(stat.st_size) }  —  mod: {mtime}\")\n",
    "    except Exception:\n",
    "        print(f\"[{i:02d}] {f.name}\")\n",
    "\n",
    "# Seleção por índice\n",
    "while True:\n",
    "    sel = input(f\"Digite o índice do arquivo para converter [0..{len(csv_files)-1}]: \").strip()\n",
    "    if sel.isdigit():\n",
    "        idx = int(sel)\n",
    "        if 0 <= idx < len(csv_files):\n",
    "            break\n",
    "    print(\"Índice inválido. Tente novamente.\")\n",
    "\n",
    "src = csv_files[idx]\n",
    "print(f\"Selecionado: {src.name}\")\n",
    "\n",
    "# Decisão de sobrescrever e backup\n",
    "overwrite = _ask_yes_no(\"Deseja sobrescrever o arquivo original? (s/n): \")\n",
    "make_backup = False\n",
    "if overwrite:\n",
    "    make_backup = _ask_yes_no(\"Gerar backup (.bak) antes de sobrescrever? (s/n): \")\n",
    "\n",
    "if overwrite:\n",
    "    # Caminhos temporário e backup\n",
    "    tmp_out = src.with_suffix(\".tmp.csv\")\n",
    "    backup = src.with_suffix(src.suffix + \".bak\")\n",
    "    if make_backup:\n",
    "        backup.write_bytes(src.read_bytes())\n",
    "        print(f\"[ok] Backup criado: {backup.name}\")\n",
    "    # Converte para temporário e substitui\n",
    "    convert_semicolon_to_comma(src, tmp_out)\n",
    "    src.unlink()          # remove original\n",
    "    tmp_out.rename(src)   # coloca convertido no lugar\n",
    "    print(f\"[ok] Convertido INPLACE: {src.name} (sep ';' -> ',')\")\n",
    "else:\n",
    "    out_dir = src.parent / OUTPUT_SUBDIR\n",
    "    dst = out_dir / src.name\n",
    "    convert_semicolon_to_comma(src, dst)\n",
    "    rel = dst.relative_to(INPUT_DIR)\n",
    "    print(f\"[ok] Convertido: {src.name} -> {rel} (sem sobrescrever)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "executionInfo": {
     "elapsed": 3330,
     "status": "ok",
     "timestamp": 1761003201738,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "WVv5gvC2nV8D",
    "outputId": "cff2b843-6b0a-47cf-969d-253e6e145d65"
   },
   "outputs": [],
   "source": [
    "# @title §B — Pré-processamento de CSVs (path → prerun/)\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Escolher um CSV (UTF-8 com BOM, separador ',') em PROJ_ROOT/input, aplicar\n",
    "tratamentos padronizados e, se **todas as checagens** passarem, salvar em\n",
    "PROJ_ROOT/prerun (CSV/Parquet) e um relatório JSON em RUN_DIR.\n",
    "Caso qualquer checagem falhe, interrompe e informa o motivo.\n",
    "\n",
    "Regras:\n",
    "- Separador deve ser **vírgula** (',').\n",
    "- Arquivo precisa ter **> 1 coluna**.\n",
    "- **Todas** colunas obrigatórias devem existir após normalização.\n",
    "\"\"\"\n",
    "\n",
    "import json, re, sys, csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "from typing import Optional, List\n",
    "\n",
    "print(\"Skynet Informa: Pré-processamento de arquivo para posterior ingestão.\")\n",
    "\n",
    "# --------- Pré-checagens ----------\n",
    "assert 'INPUT_DIR' in globals() and 'PRERUN_DIR' in globals() and 'RUN_DIR' in globals(), \\\n",
    "    \"Execute a Etapa 1 antes (dirs e run.json).\"\n",
    "assert isinstance(INPUT_DIR, Path) and isinstance(PRERUN_DIR, Path), \"INPUT_DIR/PRERUN_DIR inválidos.\"\n",
    "\n",
    "CSV_ENCODING_REQ = \"utf-8-sig\"   # alvo com BOM\n",
    "CSV_SEP_REQ      = \",\"           # separador OBRIGATÓRIO (entrada e saída)\n",
    "\n",
    "# --------- Domínio contábil (CALIBRÁVEL) ----------\n",
    "REQUIRED_COLS = [\n",
    "    \"username\", \"lotacao\", \"data_lcto\", \"valormi\", \"dc\",\n",
    "    \"contacontabil\", \"nome_conta\", \"documento_num\"\n",
    "]\n",
    "\n",
    "RENAME_MAP = {\n",
    "    \"usuario\": \"username\", \"user\": \"username\",\n",
    "    \"lotação\": \"lotacao\",\n",
    "    \"data\": \"data_lcto\", \"data_lancamento\": \"data_lcto\",\n",
    "    \"valor\": \"valormi\",\n",
    "    \"debito_credito\": \"dc\", \"d_c\": \"dc\",\n",
    "    \"conta_contabil\": \"contacontabil\", \"conta\": \"contacontabil\",\n",
    "    \"nome_da_conta\": \"nome_conta\",\n",
    "    \"documento\": \"documento_num\", \"num_documento\": \"documento_num\",\n",
    "}\n",
    "\n",
    "NUMERIC_COLS = [\"valormi\"]\n",
    "DATE_COLS    = [\"data_lcto\"]\n",
    "\n",
    "DC_MAP = {\n",
    "    \"d\": \"d\", \"deb\": \"d\", \"debito\": \"d\", \"débito\": \"d\",\n",
    "    \"c\": \"c\", \"cred\": \"c\", \"credito\": \"c\", \"crédito\": \"c\",\n",
    "}\n",
    "\n",
    "# ----------------- Utilitários -----------------\n",
    "def _peek_first_line(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return f.readline().decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "\n",
    "def _strict_read_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Leitura **estrita** com separador vírgula e header=0.\n",
    "    - Verifica se a 1ª linha contém vírgula; se não, ERRO imediato.\n",
    "    - Tenta engine='c' (estável) com utf-8-sig e utf-8.\n",
    "    - Fallback: engine='python' configurado para aspas.\n",
    "    - Se após ler ainda vier 1 coluna, ERRO (não 'adivinha' formato).\n",
    "    - Remove colunas 'unnamed' (índices salvos).\n",
    "    \"\"\"\n",
    "    first_line = _peek_first_line(path)\n",
    "    if \",\" not in first_line:\n",
    "        raise ValueError(\n",
    "            \"ERRO: O cabeçalho não parece usar vírgula como separador.\\n\"\n",
    "            f\"Primeira linha: {first_line!r}\\n\"\n",
    "            \"Ajuste o arquivo para separador ',' e rode novamente.\"\n",
    "        )\n",
    "\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\"):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path, sep=\",\", encoding=enc, dtype=str, header=0, engine=\"c\",\n",
    "                quoting=csv.QUOTE_MINIMAL, quotechar='\"', doublequote=True, on_bad_lines=\"error\"\n",
    "            )\n",
    "            if df.shape[1] > 1:\n",
    "                df = df.loc[:, ~df.columns.str.match(r\"^unnamed[:_ ]?\\d*$\", case=False)]\n",
    "                return df\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\"):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path, sep=\",\", encoding=enc, dtype=str, header=0, engine=\"python\",\n",
    "                quoting=csv.QUOTE_MINIMAL, quotechar='\"', doublequote=True, on_bad_lines=\"error\"\n",
    "            )\n",
    "            if df.shape[1] > 1:\n",
    "                df = df.loc[:, ~df.columns.str.match(r\"^unnamed[:_ ]?\\d*$\", case=False)]\n",
    "                return df\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise ValueError(\n",
    "        \"ERRO: Falha na leitura com separador vírgula (colunas não separadas).\\n\"\n",
    "        \"Verifique se há aspas envolvendo linhas inteiras ou inconsistência no CSV.\"\n",
    "    )\n",
    "\n",
    "def _strip_all(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            df[c] = df[c].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "            df[c] = df[c].replace({\"nan\": \"\", \"None\": \"\", \"NULL\": \"\"})\n",
    "    return df\n",
    "\n",
    "def _normalize_colnames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza nomes: minúsculas sem acento, espaços→'_', colapsa '__', remove '_' nas bordas.\n",
    "    Corrige casos comuns: '_username'→'username'; 'documento_num_'→'documento_num'.\n",
    "    Aplica RENAME_MAP ao final.\n",
    "    \"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\"_\".join(map(str, tup)).strip() for tup in df.columns.values]\n",
    "    df.columns = [\"\" if (c is None) else str(c) for c in df.columns]\n",
    "\n",
    "    def _rm_accents_lower(s: str) -> str:\n",
    "        s = s.strip()\n",
    "        s = re.sub(r\"[^\\w\\s]\", \"_\", s)\n",
    "        s = re.sub(r\"\\s+\", \"_\", s)\n",
    "        s = s.lower()\n",
    "        s = (s\n",
    "             .replace(\"á\",\"a\").replace(\"à\",\"a\").replace(\"ã\",\"a\").replace(\"â\",\"a\").replace(\"ä\",\"a\")\n",
    "             .replace(\"é\",\"e\").replace(\"ê\",\"e\").replace(\"è\",\"e\").replace(\"ë\",\"e\")\n",
    "             .replace(\"í\",\"i\").replace(\"ì\",\"i\").replace(\"î\",\"i\").replace(\"ï\",\"i\")\n",
    "             .replace(\"ó\",\"o\").replace(\"ô\",\"o\").replace(\"õ\",\"o\").replace(\"ò\",\"o\").replace(\"ö\",\"o\")\n",
    "             .replace(\"ú\",\"u\").replace(\"ù\",\"u\").replace(\"û\",\"u\").replace(\"ü\",\"u\")\n",
    "             .replace(\"ç\",\"c\"))\n",
    "        s = re.sub(r\"_+\", \"_\", s)\n",
    "        s = s.strip(\"_\")\n",
    "        return s\n",
    "\n",
    "    df = df.rename(columns={c: _rm_accents_lower(c) for c in df.columns})\n",
    "\n",
    "    # pós-ajustes específicos e bordas\n",
    "    fix = {}\n",
    "    for c in df.columns:\n",
    "        if c == \"_username\": fix[c] = \"username\"\n",
    "        if c == \"documento_num_\": fix[c] = \"documento_num\"\n",
    "        if c.endswith(\"_\") and c[:-1] in df.columns: fix[c] = c[:-1]\n",
    "        if c.startswith(\"_\") and c[1:] in df.columns: fix[c] = c[1:]\n",
    "    if fix:\n",
    "        df = df.rename(columns=fix)\n",
    "\n",
    "    # aplica RENAME_MAP\n",
    "    df = df.rename(columns={src: dst for src, dst in RENAME_MAP.items() if src in df.columns})\n",
    "    return df\n",
    "\n",
    "def _to_float_br(s: str) -> Optional[float]:\n",
    "    if s is None:\n",
    "        return None\n",
    "    t = str(s).strip()\n",
    "    if t == \"\":\n",
    "        return None\n",
    "    if \",\" in t:\n",
    "        t = t.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    t = t.replace(\" \", \"\")\n",
    "    try:\n",
    "        return float(t)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _normalize_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in NUMERIC_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].map(_to_float_br).astype(float)\n",
    "    for c in DATE_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "            df[c] = df[c].dt.date.astype(\"string\")  # ISO YYYY-MM-DD\n",
    "    if \"dc\" in df.columns:\n",
    "        df[\"dc\"] = df[\"dc\"].astype(str).str.lower().str.strip()\n",
    "        df[\"dc\"] = df[\"dc\"].map(lambda x: DC_MAP.get(x, x))\n",
    "        df.loc[~df[\"dc\"].isin([\"d\",\"c\"]), \"dc\"] = \"\"\n",
    "    if \"contacontabil\" in df.columns:\n",
    "        df[\"contacontabil\"] = df[\"contacontabil\"].astype(str).str.replace(r\"\\D+\", \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "def _report_stats(original_path: Path, df_raw: pd.DataFrame, df_clean: pd.DataFrame, fmt_check: dict) -> dict:\n",
    "    return {\n",
    "        \"source_file\": str(original_path),\n",
    "        \"size_bytes\": original_path.stat().st_size,\n",
    "        \"format_check\": fmt_check,\n",
    "        \"rows_raw\": int(len(df_raw)),\n",
    "        \"cols_raw\": int(df_raw.shape[1]),\n",
    "        \"rows_clean\": int(len(df_clean)),\n",
    "        \"cols_clean\": int(df_clean.shape[1]),\n",
    "        \"na_counts\": {c: int(df_clean[c].isna().sum()) for c in df_clean.columns},\n",
    "        \"empty_counts\": {c: int((df_clean[c].astype(str)==\"\").sum()) for c in df_clean.columns},\n",
    "        \"dc_invalid_count\": int((\"dc\" in df_clean.columns) and (~df_clean[\"dc\"].isin([\"d\",\"c\"])).sum()),\n",
    "        \"valormi_na_count\": int((\"valormi\" in df_clean.columns) and df_clean[\"valormi\"].isna().sum()),\n",
    "        \"conta_non_digit_count\": int((\"contacontabil\" in df_clean.columns) and (~df_clean[\"contacontabil\"].str.fullmatch(r\"\\d+\")).sum())\n",
    "    }\n",
    "\n",
    "# ----------------- Fluxo principal -----------------\n",
    "csvs = sorted(INPUT_DIR.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not csvs:\n",
    "    raise FileNotFoundError(f\"Não há CSVs em {INPUT_DIR}. Coloque um CSV (UTF-8 BOM, ',') e rode novamente.\")\n",
    "\n",
    "print(\"\\nCSVs disponíveis em INPUT_DIR:\")\n",
    "for i, p in enumerate(csvs):\n",
    "    print(f\"[{i:03d}] {p.name}\")\n",
    "\n",
    "idx = None\n",
    "while idx is None:\n",
    "    try:\n",
    "        idx = int(input(\"\\nÍndice do CSV para pré-processar: \").strip())\n",
    "        assert 0 <= idx < len(csvs)\n",
    "    except Exception:\n",
    "        print(\"Índice inválido. Tente novamente.\")\n",
    "        idx = None\n",
    "\n",
    "SRC_PATH = csvs[idx]\n",
    "print(f\"Arquivo selecionado: {SRC_PATH.name}\")\n",
    "\n",
    "# 1) Checagem do header (informativa)\n",
    "first_line = _peek_first_line(SRC_PATH)\n",
    "fmt = {\"comma_in_header\": (\",\" in first_line)}\n",
    "\n",
    "# 2) Leitura estrita com vírgula (erra se não separar)\n",
    "df0 = _strict_read_csv(SRC_PATH)\n",
    "if df0.shape[1] <= 1:\n",
    "    raise RuntimeError(\"ERRO: Leitura resultou em apenas 1 coluna. Verifique separador vírgula e conteúdo do CSV.\")\n",
    "\n",
    "# 3) Normalização de colunas e strings\n",
    "df1 = _normalize_colnames(df0)\n",
    "df1 = _strip_all(df1)\n",
    "\n",
    "# 4) Checagem das obrigatórias (interrompe se faltar)\n",
    "missing = [c for c in REQUIRED_COLS if c not in df1.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"ERRO: colunas obrigatórias ausentes após normalização: \"\n",
    "        f\"{missing}. Ajuste o cabeçalho do CSV e rode novamente.\"\n",
    "    )\n",
    "\n",
    "# 5) Normalização de valores\n",
    "df2 = _normalize_values(df1)\n",
    "\n",
    "# 6) Persistência automática (sem confirmação, pois todas as checagens passaram)\n",
    "stamp   = datetime.now(ZoneInfo(\"America/Sao_Paulo\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "base    = SRC_PATH.stem\n",
    "dst_csv = PRERUN_DIR / f\"{base}-clean-{stamp}.csv\"\n",
    "dst_parq= PRERUN_DIR / f\"{base}-clean-{stamp}.parquet\"\n",
    "dst_rep = RUN_DIR    / f\"preprocess_report_{base}-{stamp}.json\"\n",
    "\n",
    "out = df2.copy()\n",
    "if \"valormi\" in out.columns:\n",
    "    out[\"valormi\"] = out[\"valormi\"].map(lambda x: (\"\" if pd.isna(x) else f\"{x:.2f}\"))\n",
    "\n",
    "out.to_csv(dst_csv, index=False, sep=CSV_SEP_REQ, encoding=CSV_ENCODING_REQ)\n",
    "df2.to_parquet(dst_parq, index=False)\n",
    "\n",
    "report = _report_stats(SRC_PATH, df0, df2, fmt)\n",
    "dst_rep.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nPré-processamento concluído → {dst_csv.name}\")\n",
    "print(f\"Snapshot parquet            → {dst_parq.name}\")\n",
    "print(f\"Relatório JSON              → {dst_rep.name}\")\n",
    "\n",
    "print(\"\\nPré-visualização (5 linhas):\")\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df2.head(5))\n",
    "except Exception:\n",
    "    print(df2.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\nPróximo passo: Etapa 3 — selecionar um arquivo de PRERUN_DIR para TREINO/VAL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de2SoMqgdcRq"
   },
   "source": [
    "# **Etapa 3:** Ingestão de treino\n",
    "\n",
    "---\n",
    "\n",
    "Somente CSVs pré-processados de prerun/ — seleção explícita.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "executionInfo": {
     "elapsed": 17518,
     "status": "ok",
     "timestamp": 1761014954560,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "hfXFgKIbtsl-",
    "outputId": "5ce74327-5280-4bde-9053-7f2ccff70922"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo  # <-- fuso horário SP\n",
    "\n",
    "print(\"Skynet Informa: Ingestão de arquivo pré-processado para treino.\")\n",
    "\n",
    "assert 'RUN_DIR' in globals() and 'PRERUN_DIR' in globals(), \"Execute as Etapas 1 e 2 antes.\"\n",
    "\n",
    "CSV_ENCODING = \"utf-8-sig\"   # FIXO\n",
    "CSV_SEP      = \",\"           # FIXO\n",
    "\n",
    "REQUIRED_COLS = [\"username\", \"lotacao\", \"data_lcto\", \"valormi\", \"dc\", \"contacontabil\"]  # CALIBRÁVEL\n",
    "DC_VALIDOS = {\"d\", \"c\"}  # FIXO\n",
    "\n",
    "def carregar_validar_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=CSV_SEP, encoding=CSV_ENCODING, dtype=str)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    df = df.apply(lambda col: col.str.strip() if col.dtype == object else col)\n",
    "\n",
    "    def _to_float(v):\n",
    "        if v is None: return np.nan\n",
    "        s = str(v).strip()\n",
    "        if s == \"\": return np.nan\n",
    "        if \",\" in s: s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        s = s.replace(\" \", \"\")\n",
    "        try: return float(s)\n",
    "        except Exception: return np.nan\n",
    "\n",
    "    if \"valormi\" in df.columns:\n",
    "        df[\"valormi\"] = df[\"valormi\"].map(_to_float).astype(float)\n",
    "\n",
    "    if \"dc\" in df.columns:\n",
    "        df[\"dc\"] = df[\"dc\"].astype(str).str.lower().str.strip()\n",
    "        df.loc[~df[\"dc\"].isin(DC_VALIDOS), \"dc\"] = \"\"\n",
    "\n",
    "    if \"contacontabil\" in df.columns:\n",
    "        df[\"contacontabil\"] = df[\"contacontabil\"].astype(str).str.replace(r\"\\D+\", \"\", regex=True)\n",
    "\n",
    "    faltantes = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if faltantes:\n",
    "        raise ValueError(f\"Colunas obrigatórias ausentes: {faltantes}\")\n",
    "\n",
    "    problemas = {}\n",
    "    if df[\"dc\"].eq(\"\").any(): problemas[\"dc_invalidos\"] = int(df[\"dc\"].eq(\"\").sum())\n",
    "    if df[\"valormi\"].isna().any(): problemas[\"valormi_na\"] = int(df[\"valormi\"].isna().sum())\n",
    "    if (~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).any():\n",
    "        problemas[\"contacontabil_nao_numerica\"] = int((~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).sum())\n",
    "    if problemas:\n",
    "        rep_path = RUN_DIR / f\"validacao_ingestao_{path.stem}.json\"\n",
    "        rep_path.write_text(json.dumps(problemas, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"Skynet: inconsistências registradas em {rep_path.name}\")\n",
    "    return df\n",
    "\n",
    "def _human_size(b: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if b < 1024: return f\"{b:.1f}{unit}\"\n",
    "        b /= 1024\n",
    "    return f\"{b:.1f}PB\"\n",
    "\n",
    "# ---------- listagem sem filtro ----------\n",
    "pr_list_all = sorted(PRERUN_DIR.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not pr_list_all:\n",
    "    raise FileNotFoundError(\"Nenhum CSV encontrado em prerun/. Rode a Etapa 2 primeiro.\")\n",
    "\n",
    "def _mostrar(lista):\n",
    "    print(\"\\nCSVs em prerun/:\")\n",
    "    for i, p in enumerate(lista):\n",
    "        st = p.stat()\n",
    "        mtime_sp = datetime.fromtimestamp(st.st_mtime, tz=ZoneInfo(\"America/Sao_Paulo\"))\n",
    "        ts = mtime_sp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{i:03d}] {p.name:60s}  { _human_size(st.st_size):>8s}  mtime={ts}\")\n",
    "\n",
    "# ---------- seleção direta por índice (sem filtro) ----------\n",
    "while True:\n",
    "    _mostrar(pr_list_all)\n",
    "    raw = input(\"\\nDigite o ÍNDICE do CSV a usar para TREINO/VAL: \").strip()\n",
    "    try:\n",
    "        idx = int(raw)\n",
    "        assert 0 <= idx < len(pr_list_all)\n",
    "    except Exception:\n",
    "        print(\"Índice inválido. Tente novamente.\")\n",
    "        continue\n",
    "\n",
    "    selecionado = pr_list_all[idx]\n",
    "    print(f\"\\nSelecionado: {selecionado.name}\")\n",
    "    ok = input(\"Confirma este arquivo? (s/n): \").strip().lower()\n",
    "    if ok == \"s\":\n",
    "        SELECTED_CSV = selecionado\n",
    "        break\n",
    "    else:\n",
    "        print(\"Seleção cancelada. Reiniciando…\")\n",
    "\n",
    "print(f\"Arquivo confirmado: {SELECTED_CSV.name}\")\n",
    "\n",
    "# ---------- ingestão e rastreabilidade ----------\n",
    "DF_RAW = carregar_validar_csv(SELECTED_CSV)\n",
    "print(f\"Linhas carregadas: {len(DF_RAW):,}\")\n",
    "\n",
    "pad_path = RUN_DIR / \"selected_source.csv\"\n",
    "DF_RAW.to_csv(pad_path, index=False, sep=CSV_SEP, encoding=CSV_ENCODING)\n",
    "snap_path = RUN_DIR / \"journal_entries.parquet\"\n",
    "DF_RAW.to_parquet(snap_path, index=False)\n",
    "\n",
    "print(f\"cópias salvas: {pad_path.name}, {snap_path.name}\")\n",
    "display(DF_RAW.head(5))\n",
    "\n",
    "print(\"\\nPróximo passo: Etapa 4 — criação do vocabulário de treino.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIhjPdBreCqJ"
   },
   "source": [
    "# **Etapa 4:** Vocabulário de treino\n",
    "\n",
    "---\n",
    "\n",
    "Cria e congela um vocabulário de treino para as colunas categóricas do dataset.\n",
    "\n",
    "Colunas categóricas são transformadas em números de acordo com o dicionário estabelecido nessa Etapa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1761014958585,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "LmgSpNpoX4ga",
    "outputId": "c843a51a-5d0e-4576-add5-caf810d45bee"
   },
   "outputs": [],
   "source": [
    "# @title §A — Utilitários para versionamento do modelo: encoder/ver_N utilitário - SEMPRE EXECUTAR EM TREINO\n",
    "# -*- coding: utf-8 -*-\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json, re, shutil, inspect, hashlib, os\n",
    "\n",
    "print(\"Skynet Informa: Nada a se ver aqui. Apenas definição de funções utilitárias.\")\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _list_versions(encoder_dir: Path):\n",
    "    if not encoder_dir.exists():\n",
    "        return []\n",
    "    vers = sorted([d.name for d in encoder_dir.iterdir() if d.is_dir() and re.match(r\"^ver_\\d+$\", d.name)],\n",
    "                  key=lambda s: int(s.split(\"_\")[1]))\n",
    "    return vers\n",
    "\n",
    "def _next_version_name(encoder_dir: Path):\n",
    "    vers = _list_versions(encoder_dir)\n",
    "    if not vers:\n",
    "        return \"ver_1\"\n",
    "    last_n = int(vers[-1].split(\"_\")[1])\n",
    "    return f\"ver_{last_n+1}\"\n",
    "\n",
    "def _confirm(prompt: str) -> bool:\n",
    "    resp = input(f\"{prompt} [y/N]: \").strip().lower()\n",
    "    return resp in (\"y\", \"yes\", \"s\", \"sim\")\n",
    "\n",
    "def choose_or_create_version(PROJ_ROOT: Path, interactive: bool=True, allow_overwrite: bool=True) -> Path:\n",
    "    \"\"\"\n",
    "    Retorna o caminho encoder/ver_N escolhido/criado.\n",
    "    Se interactive=True: pergunta ao usuário criar nova versão ou selecionar/sobrescrever.\n",
    "    Caso contrário: cria automaticamente a próxima versão.\n",
    "    \"\"\"\n",
    "    encoder_dir = _ensure_dir(PROJ_ROOT / \"encoder\")\n",
    "    vers = _list_versions(encoder_dir)\n",
    "\n",
    "    if not interactive:\n",
    "        ver_name = _next_version_name(encoder_dir)\n",
    "        target = encoder_dir / ver_name\n",
    "        _ensure_dir(target)\n",
    "        print(f\"[encoder] Versão criada automaticamente: {ver_name}\")\n",
    "        return target\n",
    "\n",
    "    print(f\"[encoder] Versões existentes: {vers if vers else '(nenhuma)'}\")\n",
    "    print(\"[1] Criar NOVA versão\")\n",
    "    if vers:\n",
    "        print(\"[2] Usar versão EXISTENTE (sobrescrever)\")\n",
    "\n",
    "    choice = input(\"Escolha [1-2]: \").strip()\n",
    "    if choice == \"2\" and vers:\n",
    "        for i, v in enumerate(vers, 1):\n",
    "            print(f\"{i}) {v}\")\n",
    "        idx = int(input(\"Informe o índice da versão a sobrescrever: \").strip())\n",
    "        ver_name = vers[idx-1]\n",
    "        target = encoder_dir / ver_name\n",
    "        if allow_overwrite and _confirm(f\"Confirma sobrescrever {ver_name}?\"):\n",
    "            print(f\"[encoder] Sobrescrevendo {ver_name}...\")\n",
    "            # limpando conteúdo prévio\n",
    "            for item in target.iterdir():\n",
    "                if item.is_file() or item.is_symlink():\n",
    "                    item.unlink(missing_ok=True)\n",
    "                elif item.is_dir():\n",
    "                    shutil.rmtree(item)\n",
    "            return target\n",
    "        else:\n",
    "            raise RuntimeError(\"Operação cancelada pelo usuário.\")\n",
    "    else:\n",
    "        ver_name = _next_version_name(encoder_dir)\n",
    "        target = encoder_dir / ver_name\n",
    "        _ensure_dir(target)\n",
    "        print(f\"[encoder] Nova versão: {ver_name}\")\n",
    "        return target\n",
    "\n",
    "def latest_version_dir(PROJ_ROOT: Path) -> Path | None:\n",
    "    encoder_dir = PROJ_ROOT / \"encoder\"\n",
    "    vers = _list_versions(encoder_dir)\n",
    "    return (encoder_dir / vers[-1]) if vers else None\n",
    "\n",
    "def _md5_bytes(b: bytes) -> str:\n",
    "    h = hashlib.md5(); h.update(b); return h.hexdigest()\n",
    "\n",
    "def snapshot_pipeline_functions(save_dir: Path, funcs: dict):\n",
    "    \"\"\"\n",
    "    funcs = {\"build_features\": build_features, \"encode_categoricals\": encode_categoricals}\n",
    "    Salva pipeline_snapshot.py com o source das funções, para import posterior na Etapa 8.\n",
    "    \"\"\"\n",
    "    lines = [\"# -*- coding: utf-8 -*-\", \"# snapshot gerado automaticamente — não editar manualmente\", \"\"]\n",
    "    for name, fn in funcs.items():\n",
    "        try:\n",
    "            src = inspect.getsource(fn)\n",
    "        except OSError as e:\n",
    "            raise RuntimeError(f\"Não foi possível obter o source de {name}: {e}\")\n",
    "        lines.append(src)\n",
    "        lines.append(\"\")  # espaçamento\n",
    "    code = \"\\n\".join(lines)\n",
    "    (save_dir / \"pipeline_snapshot.py\").write_text(code, encoding=\"utf-8\")\n",
    "    (save_dir / \"pipeline_snapshot.md5\").write_text(_md5_bytes(code.encode(\"utf-8\")), encoding=\"utf-8\")\n",
    "\n",
    "def write_json(path: Path, obj: dict):\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def save_version_artifacts(\n",
    "    version_dir: Path,\n",
    "    *,\n",
    "    model_state_path: Path,\n",
    "    model_config: dict,\n",
    "    features_pkl_path: Path,\n",
    "    categorical_maps_path: Path | None,\n",
    "    reconstruction_errors_val_path: Path | None,\n",
    "    threshold_json_path: Path | None,\n",
    "    exec_meta: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Copia/gera todos os arquivos necessários para Etapa 8+ dentro da versão.\n",
    "    \"\"\"\n",
    "    _ensure_dir(version_dir)\n",
    "    # 1) modelo\n",
    "    shutil.copy2(model_state_path, version_dir / \"ae.pt\")\n",
    "    write_json(version_dir / \"model_config.json\", model_config)\n",
    "    # 2) features e vocabulário\n",
    "    shutil.copy2(features_pkl_path, version_dir / \"features.pkl\")\n",
    "    if categorical_maps_path and categorical_maps_path.exists():\n",
    "        shutil.copy2(categorical_maps_path, version_dir / \"categorical_maps.json\")\n",
    "    # 3) validação/threshold\n",
    "    if reconstruction_errors_val_path and reconstruction_errors_val_path.exists():\n",
    "        shutil.copy2(reconstruction_errors_val_path, version_dir / \"reconstruction_errors_val.npy\")\n",
    "    if threshold_json_path and threshold_json_path.exists():\n",
    "        shutil.copy2(threshold_json_path, version_dir / \"threshold.json\")\n",
    "    # 4) metadados da versão\n",
    "    write_json(version_dir / \"version_meta.json\", exec_meta)\n",
    "    print(f\"[encoder] Artefatos gravados em: {version_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7827,
     "status": "ok",
     "timestamp": 1761014968680,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "cfNCS5txeBo8",
    "outputId": "161394a0-6c86-4dee-ac5c-b4545395e046"
   },
   "outputs": [],
   "source": [
    "# @title §B — Geração do Dicionário\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Criar e congelar um vocabulário (mapeamento categoria → índice inteiro) para as\n",
    "colunas categóricas do dataset de TREINO (DF_RAW). Transforma nomes em números.\n",
    "\n",
    "exemplo: d = 1 | c = 2 => com isso o modelo passa a ver os lançamentos como um conjunto\n",
    "de [1,2,1,2,2,2,1,1,1..etc]\n",
    "\n",
    "Esta etapa ainda não trata de frequências/cortes temporais (vide Etapa 5)\n",
    "\n",
    "Como funciona\n",
    "-------------\n",
    "1) Define a lista de colunas categóricas (CALIBRÁVEL em CAT_COLS).\n",
    "2) Para cada coluna:\n",
    "   - Calcula frequências no DF_RAW.\n",
    "   - Ordena por (freq desc, depois valor asc) para determinismo.\n",
    "   - Reserva índice 0 para OOV (\"out-of-vocabulary\").\n",
    "   - Atribui índices a partir de 1 às categorias vistas no treino.\n",
    "3) Salva artefatos:\n",
    "   - JSON com mapas por coluna: runs/<RUN_ID>/categorical_maps.json\n",
    "   - Resumo de cardinalidades: runs/<RUN_ID>/categorical_cardinality.json\n",
    "   - (opcional) cópia durável em artifacts/: artifacts/categorical_maps_latest.json\n",
    "4) Expõe helpers:\n",
    "   - encode_categoricals(df) → adiciona colunas *_int usando o vocabulário\n",
    "   - decode_category(col, idx) → retorna string a partir do índice\n",
    "\n",
    "Pontos FIXOS\n",
    "------------\n",
    "- OOV = 0\n",
    "- Ordem determinística: (-freq, categoria)\n",
    "\n",
    "Pontos CALIBRÁVEIS\n",
    "------------------\n",
    "- CAT_COLS: escolha de colunas categóricas\n",
    "- Se incluir 'documento_num' (alta cardinalidade) depende da estratégia de features.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Skynet Informa: Gerando dicionário de dimensões.\")\n",
    "\n",
    "# Pré-checagens\n",
    "assert 'DF_RAW' in globals(), \"DF_RAW não encontrado. Execute a Etapa 3 antes.\"\n",
    "assert 'RUN_DIR' in globals() and 'ARTIF_DIR' in globals(), \"Execute a Etapa 1 antes.\"\n",
    "\n",
    "# ========= Configuração (CALIBRÁVEL) =========\n",
    "# Escolha das colunas categóricas.\n",
    "# Sugestão: incluir identificadores estáveis e sinais contábeis; evitar IDs\n",
    "# muito voláteis caso não sejam úteis ao modelo.\n",
    "CAT_COLS = [\n",
    "    \"username\",\n",
    "    \"lotacao\",\n",
    "    \"dc\",\n",
    "    \"contacontabil\",\n",
    "    # \"nome_conta\",      # ative se quiser usar descrição textual\n",
    "    # \"documento_num\",   # cuidado: alta cardinalidade; ative apenas se fizer sentido\n",
    "]\n",
    "\n",
    "OOV_INDEX = 0   # FIXO: índice reservado para 'desconhecidos'\n",
    "OFFSET    = 1   # FIXO: categorias vistas começam em 1\n",
    "\n",
    "# ========= Construção do vocabulário =========\n",
    "categorical_maps: dict[str, dict[str, int]] = {}\n",
    "categorical_cardinality: dict[str, int] = {}\n",
    "\n",
    "def _build_map(series: pd.Series) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Constrói o mapa categoria->índice:\n",
    "    - Conta frequências (com NaNs tratados como string vazia para consistência).\n",
    "    - Ordena por frequência desc, depois valor asc.\n",
    "    - Reserva 0 para OOV, categorias começam em 1.\n",
    "    \"\"\"\n",
    "    s = series.fillna(\"\").astype(str)\n",
    "    freq = Counter(s.tolist())\n",
    "\n",
    "    # Remove a categoria vazia \"\" do ranking se quiser (opcional):\n",
    "    # freq.pop(\"\", None)\n",
    "\n",
    "    # Ordenação determinística: mais frequentes primeiro; empate por ordem alfabética\n",
    "    ordered = sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "\n",
    "    mapping = {\"__oov__\": OOV_INDEX, \"__offset__\": OFFSET}\n",
    "    idx = OFFSET\n",
    "    for cat, _count in ordered:\n",
    "        mapping[cat] = idx\n",
    "        idx += 1\n",
    "    return mapping\n",
    "\n",
    "# Constrói mapas por coluna\n",
    "for col in CAT_COLS:\n",
    "    if col not in DF_RAW.columns:\n",
    "        raise ValueError(f\"Coluna categórica '{col}' não existe no DF_RAW.\")\n",
    "    cmap = _build_map(DF_RAW[col])\n",
    "    categorical_maps[col] = cmap\n",
    "    # cardinalidade = total de índices \"válidos\" (exclui OOV)\n",
    "    categorical_cardinality[col] = len(cmap) - 2  # remove chaves meta (__oov__, __offset__)\n",
    "\n",
    "# ========= Persistência dos artefatos =========\n",
    "maps_path_run = RUN_DIR / \"categorical_maps.json\"\n",
    "card_path_run = RUN_DIR / \"categorical_cardinality.json\"\n",
    "maps_path_art = ARTIF_DIR / \"categorical_maps_latest.json\"\n",
    "\n",
    "maps_path_run.write_text(json.dumps(categorical_maps, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "card_path_run.write_text(json.dumps(categorical_cardinality, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "maps_path_art.write_text(json.dumps(categorical_maps, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ======= Persistência ampliada e manifesto (com fuso São Paulo) =======\n",
    "from zoneinfo import ZoneInfo\n",
    "import hashlib\n",
    "\n",
    "# carimbo e ids\n",
    "stamp_sp = datetime.now(ZoneInfo(\"America/Sao_Paulo\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_id = Path(RUN_DIR).name\n",
    "\n",
    "# --- 3.1 Snapshot da base de treino (DF_RAW) ---\n",
    "# caminhos\n",
    "train_csv_path   = RUN_DIR / f\"train_base_{stamp_sp}.csv\"\n",
    "train_parq_path  = RUN_DIR / f\"train_base_{stamp_sp}.parquet\"\n",
    "schema_json_path = RUN_DIR / f\"train_schema_{stamp_sp}.json\"\n",
    "\n",
    "# salvar CSV e Parquet\n",
    "DF_RAW.to_csv(train_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "DF_RAW.to_parquet(train_parq_path, index=False)\n",
    "\n",
    "# schema (dtypes) e shape\n",
    "schema_dict = {\n",
    "    \"shape\": {\"rows\": int(len(DF_RAW)), \"cols\": int(DF_RAW.shape[1])},\n",
    "    \"dtypes\": {c: str(DF_RAW[c].dtype) for c in DF_RAW.columns}\n",
    "}\n",
    "schema_json_path.write_text(json.dumps(schema_dict, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# hash da base (conteúdo) — robusto para auditoria\n",
    "def _hash_file(path: Path, algo=\"sha256\", buf=1<<20) -> str:\n",
    "    h = hashlib.new(algo)\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(buf)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "train_csv_hash  = _hash_file(train_csv_path)\n",
    "train_parq_hash = _hash_file(train_parq_path)\n",
    "\n",
    "# --- 3.2 Frequências por coluna categórica (longo) ---\n",
    "freq_rows = []\n",
    "for col in CAT_COLS:\n",
    "    s = DF_RAW[col].fillna(\"\").astype(str)\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    total = int(vc.sum())\n",
    "    # ordenar por (-freq, categoria) para consistência com o vocabulário\n",
    "    vc = vc.sort_values(ascending=False)\n",
    "    # reconstruir índice (rank) e mapear idx do vocabulário\n",
    "    cmap = categorical_maps[col]\n",
    "    # mapa invertido (índices existentes)\n",
    "    for rank, (cat, freq) in enumerate(sorted(vc.items(), key=lambda kv: (-kv[1], kv[0])), start=1):\n",
    "        idx = cmap.get(cat, 0)\n",
    "        freq_rows.append({\n",
    "            \"coluna\": col,\n",
    "            \"categoria\": cat,\n",
    "            \"idx\": int(idx),\n",
    "            \"frequencia\": int(freq),\n",
    "            \"proporcao\": float(freq/total) if total else 0.0,\n",
    "            \"rank\": int(rank),\n",
    "        })\n",
    "\n",
    "freq_df = pd.DataFrame(freq_rows, columns=[\"coluna\",\"categoria\",\"idx\",\"frequencia\",\"proporcao\",\"rank\"])\n",
    "freq_path = RUN_DIR / f\"categorical_frequencies_{stamp_sp}.csv\"\n",
    "freq_df.to_csv(freq_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- 3.3 Dicionários reversos (idx -> categoria) ---\n",
    "rev_maps = {}\n",
    "for col, cmap in categorical_maps.items():\n",
    "    rev_maps[col] = {int(v): k for k, v in cmap.items() if not k.startswith(\"__\")}\n",
    "rev_maps_path = RUN_DIR / f\"categorical_rev_maps_{stamp_sp}.json\"\n",
    "rev_maps_path.write_text(json.dumps(rev_maps, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# --- 3.4 Manifesto do vocabulário ---\n",
    "# hash simples do código desta célula (opcional: cole seu código-fonte em uma string CODE_SRC)\n",
    "CODE_SRC = None  # se quiser, atribua o texto da célula aqui para carimbar\n",
    "code_hash = hashlib.sha256(CODE_SRC.encode(\"utf-8\")).hexdigest() if CODE_SRC else None\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp_sp\": stamp_sp,\n",
    "    \"cat_cols\": CAT_COLS,\n",
    "    \"oov_index\": OOV_INDEX,\n",
    "    \"offset\": OFFSET,\n",
    "    \"train_snapshot\": {\n",
    "        \"csv\": str(train_csv_path),\n",
    "        \"csv_sha256\": train_csv_hash,\n",
    "        \"parquet\": str(train_parq_path),\n",
    "        \"parquet_sha256\": train_parq_hash,\n",
    "        \"schema_json\": str(schema_json_path),\n",
    "        \"shape\": schema_dict[\"shape\"]\n",
    "    },\n",
    "    \"maps\": {\n",
    "        \"run_path\": str(maps_path_run),\n",
    "        \"artifacts_copy\": str(maps_path_art),\n",
    "        \"rev_maps\": str(rev_maps_path),\n",
    "        \"cardinality\": str(card_path_run),\n",
    "        \"frequencies_csv\": str(freq_path)\n",
    "    },\n",
    "    \"code_hash_sha256\": code_hash\n",
    "}\n",
    "manifest_path = RUN_DIR / f\"vocab_manifest_{stamp_sp}.json\"\n",
    "manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Snapshot treino          - {train_csv_path.name}, {train_parq_path.name}, {schema_json_path.name}\")\n",
    "print(f\"Freq categóricas         - {freq_path.name}\")\n",
    "print(f\"Mapas reversos           - {rev_maps_path.name}\")\n",
    "print(f\"Manifesto                - {manifest_path.name}\")\n",
    "print(f\"Vocabulário salvo em     - {maps_path_run.name}  (cópia: artifacts/{maps_path_art.name})\")\n",
    "print(f\"Cardinalidades salvas em - {card_path_run.name}\")\n",
    "\n",
    "# ========= Helpers de codificação/decodificação =========\n",
    "def encode_categoricals(df: pd.DataFrame,\n",
    "                        cat_cols: list[str] = None,\n",
    "                        maps: dict[str, dict[str, int]] = None,\n",
    "                        suffix: str = \"_int\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Projeta colunas categóricas para índices inteiros usando os mapas congelados.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame de entrada\n",
    "    - cat_cols: lista de colunas categóricas (default = CAT_COLS)\n",
    "    - maps: dicionário de mapas (default = categorical_maps deste run)\n",
    "    - suffix: sufixo para colunas codificadas (default: '_int')\n",
    "\n",
    "    Saída:\n",
    "    - DataFrame com novas colunas <col><suffix> (inteiros).\n",
    "    \"\"\"\n",
    "    if cat_cols is None:\n",
    "        cat_cols = CAT_COLS\n",
    "    if maps is None:\n",
    "        maps = categorical_maps\n",
    "\n",
    "    out = df.copy()\n",
    "    for col in cat_cols:\n",
    "        if col not in out.columns:\n",
    "            raise ValueError(f\"Coluna '{col}' não encontrada no DF para codificar.\")\n",
    "        cmap = maps.get(col)\n",
    "        if cmap is None:\n",
    "            raise ValueError(f\"Mapa não encontrado para coluna '{col}'.\")\n",
    "        oov = cmap.get(\"__oov__\", 0)\n",
    "        series = out[col].fillna(\"\").astype(str)\n",
    "        out[col + suffix] = series.map(lambda x: cmap.get(x, oov)).astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def decode_category(col: str, idx: int,\n",
    "                    maps: dict[str, dict[str, int]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Decodifica um índice inteiro de volta à categoria (quando possível).\n",
    "    - Retorna '<OOV>' para índices não mapeados ou OOV=0.\n",
    "    \"\"\"\n",
    "    if maps is None:\n",
    "        maps = categorical_maps\n",
    "    cmap = maps.get(col)\n",
    "    if not cmap:\n",
    "        return \"<OOV>\"\n",
    "    if idx == cmap.get(\"__oov__\", 0):\n",
    "        return \"<OOV>\"\n",
    "    # constrói cache reverso simples\n",
    "    rev = {v: k for k, v in cmap.items() if not k.startswith(\"__\")}\n",
    "    return rev.get(idx, \"<OOV>\")\n",
    "\n",
    "# ========= Demonstração rápida (opcional) =========\n",
    "demo_cols = \", \".join(CAT_COLS)\n",
    "print(f\"Colunas categóricas configuradas - {demo_cols}\")\n",
    "for col in CAT_COLS:\n",
    "    print(f\"{col}: cardinalidade = {categorical_cardinality[col]} (OOV=0, offset={OFFSET})\")\n",
    "\n",
    "# Mantém DF_RAW na memória para próxima etapa\n",
    "print(\"\\nPróximo passo: Etapa 5 — Engenharia de Features (usando os *_int gerados aqui).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBReg_7YeSVj"
   },
   "source": [
    "# **Etapa 5:** Engenharia de features\n",
    "\n",
    "---\n",
    "Transformação tabular para treino.\n",
    "\n",
    "| Indicador / Aspecto                    | Análise                                  | Justificativa                                   |\n",
    "|---------------------------------------|------------------------------------------|------------------------------------------------|\n",
    "| Frequência total de lançamentos       | Mede o ritmo operacional                 | Permite observar padrões de recorrência        |\n",
    "| Valor total / médio                   | Mede o peso financeiro                   | Indica o impacto monetário dos lançamentos     |\n",
    "| Segmentação por DC (débito / crédito) | Distingue natureza contábil do movimento | Essencial para separar comportamentos opostos  |\n",
    "\n",
    "<br>\n",
    "Esta Etapa gera freq. + valor total + valor médio\n",
    "todas discriminadas por D/C.\n",
    "\n",
    "Isso torna o modelo sensível a mudanças na natureza dos lançamentos,\n",
    "não apenas no volume ou no montante - o que é exatamente o tipo de anomalia\n",
    "que mais interessa em auditoria comportamental.\n",
    "\n",
    "Ao final, geramos uma foto (snapshot) em arquivo, para não dependermos da memória/kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "executionInfo": {
     "elapsed": 187023,
     "status": "ok",
     "timestamp": 1761015160503,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "qRtIabdmx31-",
    "outputId": "d1fa2efa-01ec-4926-fee5-8174a4485d25"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Encapsula toda a engenharia de features da Etapa 5 em uma função reutilizável:\n",
    "  build_features(df_in, ..., save_rejects, run_dir, strict_date_threshold)\n",
    "\n",
    "Depois:\n",
    "- Executa sobre DF_RAW (treino) e persiste outputs\n",
    "- Congela a pipeline em artifacts/pipeline_snapshot.py (com imports no topo)\n",
    "\"\"\"\n",
    "\n",
    "import re, json, hashlib, inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "print(\"Skynet Informa: Criando features (wrapper completo) e congelando pipeline.\")\n",
    "\n",
    "# --------- Pré-checagens globais mínimas ----------\n",
    "assert 'DF_RAW'    in globals(), \"DF_RAW não encontrado. Execute as etapas anteriores.\"\n",
    "assert 'RUN_DIR'   in globals() and isinstance(RUN_DIR, Path), \"RUN_DIR não encontrado. Execute a Etapa 1.\"\n",
    "assert 'PROJ_ROOT' in globals() and isinstance(PROJ_ROOT, Path), \"PROJ_ROOT não encontrado. Execute a Etapa 1.\"\n",
    "ARTIFACTS_DIR = Path(globals().get(\"ARTIFACTS_DIR\", PROJ_ROOT / \"artifacts\"))\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "#  WRAPPER COMPLETO\n",
    "# =========================\n",
    "def build_features(\n",
    "    df_in: pd.DataFrame,\n",
    "    *,\n",
    "    save_rejects: bool = False,\n",
    "    run_dir: Path | None = None,\n",
    "    tz: str = \"America/Sao_Paulo\",\n",
    "    strict_date_threshold: float = 1.0  # % máximo de datas inválidas permitido (estrito)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reaplica a engenharia da Etapa 5 sobre df_in e retorna o DataFrame de features.\n",
    "\n",
    "    Parâmetros:\n",
    "      - save_rejects: se True (e run_dir definido), salva CSV/JSON dos registros com data inválida.\n",
    "      - strict_date_threshold: percentual máximo (em %) de datas inválidas; acima disso gera exceção.\n",
    "    \"\"\"\n",
    "    df_feat = df_in.copy()\n",
    "\n",
    "    # --------- colunas obrigatórias ----------\n",
    "    REQ_COLS = [\"data_lcto\", \"username\", \"lotacao\", \"contacontabil\", \"dc\", \"valormi\"]\n",
    "    missing = [c for c in REQ_COLS if c not in df_feat.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"ERRO: colunas obrigatórias ausentes: {missing}\")\n",
    "\n",
    "    # ========= 1) Datas: aaaa-mm-dd com auditoria =========\n",
    "    def _clean_str(x):\n",
    "        if pd.isna(x): return \"\"\n",
    "        s = str(x).strip()\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        return s.replace(\"\\ufeff\",\"\").replace(\"\\u200b\",\"\")\n",
    "\n",
    "    raw_dates = df_feat[\"data_lcto\"].astype(object).map(_clean_str)\n",
    "    df_feat[\"data_lcto_parsed\"] = pd.to_datetime(raw_dates, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "    invalid_mask = df_feat[\"data_lcto_parsed\"].isna()\n",
    "    n_total = len(df_feat)\n",
    "    n_inv = int(invalid_mask.sum())\n",
    "    pct_inv = (100.0 * n_inv / n_total) if n_total else 0.0\n",
    "    print(f\"Qualidade 'data_lcto' (aaaa-mm-dd): registros válidos = ({n_total}-{n_inv})/{n_total} ((1-{pct_inv:.2f})%).\")\n",
    "\n",
    "    # salvar rejeições (opcional, treino/auditoria)\n",
    "    if save_rejects and n_inv > 0 and run_dir is not None:\n",
    "        stamp = datetime.now(ZoneInfo(tz)).strftime(\"%Y%m%d-%H%M%S\")\n",
    "        rej_cols = [\"data_lcto\",\"username\",\"lotacao\",\"contacontabil\",\"dc\",\"valormi\"]\n",
    "        df_rej = df_feat.loc[invalid_mask, rej_cols].copy()\n",
    "        rej_csv  = run_dir / f\"rejects_data_lcto_{stamp}.csv\"\n",
    "        rej_json = run_dir / f\"rejects_data_lcto_{stamp}.json\"\n",
    "        df_rej.to_csv(rej_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        rej_json.write_text(df_rej.to_json(orient=\"records\", force_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"Rejeições salvas: {rej_csv.name} / {rej_json.name}\")\n",
    "\n",
    "    # política estrita para datas\n",
    "    if pct_inv > float(strict_date_threshold):\n",
    "        raise RuntimeError(f\"ERRO: {pct_inv:.2f}% datas inválidas (>{strict_date_threshold}%).\")\n",
    "\n",
    "    df_feat[\"data_lcto\"] = df_feat[\"data_lcto_parsed\"]\n",
    "    df_feat = df_feat.drop(columns=[\"data_lcto_parsed\"]).dropna(subset=[\"data_lcto\"]).reset_index(drop=True)\n",
    "\n",
    "    # ========= 2) Hierarquia da conta: g1/g2/g3 =========\n",
    "    conta_digits = df_feat[\"contacontabil\"].astype(str).str.replace(r\"\\D+\", \"\", regex=True)\n",
    "    df_feat[\"conta_digits\"] = conta_digits\n",
    "    df_feat[\"g1\"] = conta_digits.str.slice(0, 1)\n",
    "    df_feat[\"g2\"] = conta_digits.str.slice(0, 2)\n",
    "    df_feat[\"g3\"] = conta_digits.str.slice(0, 3)\n",
    "    df_feat = df_feat[df_feat[\"g1\"].str.len() == 1].reset_index(drop=True)\n",
    "\n",
    "    # ========= 3) Períodos e valor =========\n",
    "    df_feat[\"mes\"] = df_feat[\"data_lcto\"].dt.to_period(\"M\").dt.to_timestamp()       # início do mês\n",
    "    df_feat[\"trimestre\"] = df_feat[\"data_lcto\"].dt.to_period(\"Q\").dt.start_time     # início do trimestre\n",
    "    df_feat[\"ano\"] = df_feat[\"data_lcto\"].dt.year.astype(\"int16\")\n",
    "    df_feat[\"mes_num\"] = df_feat[\"data_lcto\"].dt.month.astype(\"int8\")\n",
    "    df_feat[\"tri_num\"] = df_feat[\"data_lcto\"].dt.quarter.astype(\"int8\")\n",
    "\n",
    "    df_feat[\"valormi_float\"] = pd.to_numeric(df_feat[\"valormi\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # ========= 4) Helpers (blocos) =========\n",
    "    def _zblock(values: pd.Series, group_df: pd.DataFrame, group_keys: list[str]) -> pd.Series:\n",
    "        aux = group_df[group_keys].copy()\n",
    "        aux = aux.assign(_v=values.values)\n",
    "        grp = aux.groupby(group_keys)[\"_v\"]\n",
    "        mean = grp.transform(\"mean\")\n",
    "        std  = grp.transform(\"std\").replace(0.0, np.nan)\n",
    "        z = (aux[\"_v\"] - mean) / std\n",
    "        return z.fillna(0.0)\n",
    "\n",
    "    def make_block(df: pd.DataFrame, keys_base: list[str], period: str, base_name: str, by_dc: bool) -> pd.DataFrame:\n",
    "        keys = keys_base + ([\"dc\"] if by_dc else []) + [period]\n",
    "        s_freq = df.groupby(keys)[keys[0]].transform(\"size\").astype(\"int32\")\n",
    "        s_val  = df.groupby(keys)[\"valormi_float\"].transform(\"sum\")\n",
    "        s_mean = np.where(s_freq > 0, s_val / s_freq, 0.0)\n",
    "        z_keys = keys_base + ([\"dc\"] if by_dc else [])\n",
    "        s_val_z  = _zblock(pd.Series(s_val, index=df.index),  df, z_keys)\n",
    "        s_mean_z = _zblock(pd.Series(s_mean, index=df.index), df, z_keys)\n",
    "        suffix = (\"_dc\" if by_dc else \"\")\n",
    "        cols = {\n",
    "            f\"freq_{period}_{base_name}{suffix}\": s_freq,\n",
    "            f\"val_{period}_{base_name}{suffix}\": s_val,\n",
    "            f\"val_med_{period}_{base_name}{suffix}\": s_mean,\n",
    "            f\"val_{period}_{base_name}{suffix}_z\": s_val_z,\n",
    "            f\"val_med_{period}_{base_name}{suffix}_z\": s_mean_z,\n",
    "        }\n",
    "        return pd.DataFrame(cols, index=df.index)\n",
    "\n",
    "    # ========= 5) Métricas =========\n",
    "    NEW_BLOCKS = []\n",
    "\n",
    "    # Totais agregados por período (atividade geral)\n",
    "    blk_tot = pd.DataFrame(index=df_feat.index)\n",
    "    blk_tot[\"freq_mes_user_total\"]    = df_feat.groupby([\"username\",\"mes\"])[\"username\"].transform(\"size\").astype(\"int32\")\n",
    "    blk_tot[\"freq_tri_user_total\"]    = df_feat.groupby([\"username\",\"trimestre\"])[\"username\"].transform(\"size\").astype(\"int32\")\n",
    "    blk_tot[\"freq_mes_lotacao_total\"] = df_feat.groupby([\"lotacao\",\"mes\"])[\"lotacao\"].transform(\"size\").astype(\"int32\")\n",
    "    blk_tot[\"freq_tri_lotacao_total\"] = df_feat.groupby([\"lotacao\",\"trimestre\"])[\"lotacao\"].transform(\"size\").astype(\"int32\")\n",
    "    NEW_BLOCKS.append(blk_tot)\n",
    "\n",
    "    # (A) Usuário × Conta × D/C\n",
    "    NEW_BLOCKS.append(make_block(df_feat, [\"username\",\"conta_digits\"], \"mes\",       \"user_conta\", by_dc=True))\n",
    "    NEW_BLOCKS.append(make_block(df_feat, [\"username\",\"conta_digits\"], \"trimestre\", \"user_conta\", by_dc=True))\n",
    "\n",
    "    # (B) Lotação × Conta × D/C\n",
    "    NEW_BLOCKS.append(make_block(df_feat, [\"lotacao\",\"conta_digits\"], \"mes\",       \"lotacao_conta\", by_dc=True))\n",
    "    NEW_BLOCKS.append(make_block(df_feat, [\"lotacao\",\"conta_digits\"], \"trimestre\", \"lotacao_conta\", by_dc=True))\n",
    "\n",
    "    # (C) Hierarquia por USUÁRIO — sem D/C e com D/C\n",
    "    for level in [\"g1\",\"g2\",\"g3\"]:\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"username\", level], \"mes\",       f\"user_{level}\", by_dc=False))\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"username\", level], \"trimestre\", f\"user_{level}\", by_dc=False))\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"username\", level], \"mes\",       f\"user_{level}\", by_dc=True))\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"username\", level], \"trimestre\", f\"user_{level}\", by_dc=True))\n",
    "\n",
    "    # (D) Hierarquia por LOTAÇÃO — sem D/C e com D/C\n",
    "    for level in [\"g1\",\"g2\",\"g3\"]:\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"lotacao\", level], \"mes\",       f\"lotacao_{level}\", by_dc=False))\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"lotacao\", level], \"trimestre\", f\"lotacao_{level}\", by_dc=False))\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"lotacao\", level], \"mes\",       f\"lotacao_{level}\", by_dc=True))\n",
    "        NEW_BLOCKS.append(make_block(df_feat, [\"lotacao\", level], \"trimestre\", f\"lotacao_{level}\", by_dc=True))\n",
    "\n",
    "    # ========= 6) Concatenação única =========\n",
    "    if NEW_BLOCKS:\n",
    "        df_feat = pd.concat([df_feat] + NEW_BLOCKS, axis=1)\n",
    "\n",
    "    # Ordenação e tipos\n",
    "    df_feat = df_feat.sort_values([\"data_lcto\",\"username\",\"conta_digits\"]).reset_index(drop=True)\n",
    "    for c in [c for c in df_feat.columns if c.startswith(\"freq_\")]:\n",
    "        df_feat[c] = df_feat[c].astype(\"int32\")\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "# =========================\n",
    "#  EXECUÇÃO (treino) e persistência\n",
    "# =========================\n",
    "STAMP = datetime.now(ZoneInfo(\"America/Sao_Paulo\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "DF_FEAT = build_features(DF_RAW, save_rejects=True, run_dir=RUN_DIR)\n",
    "\n",
    "feat_parq = RUN_DIR / f\"features_behavior_{STAMP}.parquet\"\n",
    "feat_csv  = RUN_DIR / f\"features_behavior_{STAMP}.csv\"\n",
    "feat_sch  = RUN_DIR / f\"features_schema_{STAMP}.json\"\n",
    "\n",
    "DF_FEAT.to_parquet(feat_parq, index=False)\n",
    "DF_FEAT.to_csv(feat_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "schema = {\n",
    "    \"shape\": {\"rows\": int(len(DF_FEAT)), \"cols\": int(DF_FEAT.shape[1])},\n",
    "    \"dtypes\": {c: str(DF_FEAT[c].dtype) for c in DF_FEAT.columns}\n",
    "}\n",
    "feat_sch.write_text(json.dumps(schema, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"features salvas -> {feat_parq.name} / {feat_csv.name}\")\n",
    "print(f\"schema          -> {feat_sch.name}\")\n",
    "\n",
    "print(\"\\nPré-visualização (5 linhas):\")\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(DF_FEAT.head(5))\n",
    "except Exception:\n",
    "    print(DF_FEAT.head(5).to_string(index=False))\n",
    "\n",
    "# =========================\n",
    "#  CONGELAMENTO (snapshot autossuficiente)\n",
    "# =========================\n",
    "def _file_md5(path: Path, chunk: int = 1<<20) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for b in iter(lambda: f.read(chunk), b\"\"):\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "SNAP_PY  = ARTIFACTS_DIR / \"pipeline_snapshot.py\"\n",
    "SNAP_MD5 = ARTIFACTS_DIR / \"pipeline_snapshot.md5\"\n",
    "\n",
    "# Cabeçalho com imports mínimos — torna o snapshot autossuficiente para Etapa 8+\n",
    "HEADER = \"\"\"# -*- coding: utf-8 -*-\n",
    "# Snapshot gerado pela Etapa 5 — NÃO editar manualmente.\n",
    "# Contém as funções de engenharia/codificação consumidas nas Etapa 8+.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parts = [HEADER]\n",
    "\n",
    "def _get_src(fn):\n",
    "    import inspect\n",
    "    try:\n",
    "        src = inspect.getsource(fn)\n",
    "        # NÃO usar cleandoc/dedent aqui; mantém a indentação do corpo!\n",
    "        if not src.endswith(\"\\n\"):\n",
    "            src += \"\\n\"\n",
    "        return src\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao extrair source de {getattr(fn, '__name__', str(fn))}: {e}\")\n",
    "\n",
    "# build_features (obrigatória)\n",
    "parts.append(_get_src(build_features))\n",
    "\n",
    "# encode_categoricals (opcional, se existir no kernel)\n",
    "if \"encode_categoricals\" in globals() and callable(globals()[\"encode_categoricals\"]):\n",
    "    parts.append(\"\\n# --- opcional: codificação categórica ---\\n\")\n",
    "    parts.append(_get_src(globals()[\"encode_categoricals\"]))\n",
    "\n",
    "# escreve snapshot + md5\n",
    "SNAP_PY.write_text(\"\".join(parts), encoding=\"utf-8\")\n",
    "SNAP_MD5.write_text(_file_md5(SNAP_PY), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Snapshot salvo: {SNAP_PY.name} | md5={_file_md5(SNAP_PY)}\")\n",
    "print(\"Observação: o snapshot agora importa pd/np/re/json no topo (autossuficiente para a Etapa 8).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kUde_I8eik4"
   },
   "source": [
    "# **Etapa 6:** Limpeza, normalização e split treino/validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67705,
     "status": "ok",
     "timestamp": 1761015233283,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "-lDIV60uIu9f",
    "outputId": "bbc3fda0-012f-4780-c552-3cb03c23b518"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Preparar a matriz numérica para o Autoencoder:\n",
    "1) Montar X a partir de DF_FEATURES/FEATURE_COLS\n",
    "2) Split train/val com semente fixa\n",
    "3) Imputação (mediana) treinada somente no treino\n",
    "4) Normalização (StandardScaler) treinada somente no treino\n",
    "5) Persistir artefatos e conjuntos prontos para a Etapa 7\n",
    "\n",
    "Ajustes desta versão:\n",
    "- Aceita DF_FEAT (da Etapa 5) como DF_FEATURES se este não existir.\n",
    "- Autogerar FEATURE_COLS (todas as colunas numéricas) se não foi definida.\n",
    "\"\"\"\n",
    "\n",
    "import json, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "print(\"Skynet Informa: Limpando, normalizando e realizando split treino/validação.\")\n",
    "\n",
    "# --------------------------------\n",
    "# Pré-requisitos e ponte com a Etapa 5\n",
    "# --------------------------------\n",
    "assert 'RUN_DIR' in globals() and 'ARTIF_DIR' in globals(), \"Execute a Etapa 1 antes.\"\n",
    "# Usa DF_FEAT da etapa 5, se DF_FEATURES não existir\n",
    "if 'DF_FEATURES' not in globals():\n",
    "    if 'DF_FEAT' in globals():\n",
    "        DF_FEATURES = DF_FEAT\n",
    "        print(\"DF_FEATURES não encontrado; usando DF_FEAT da Etapa 5.\")\n",
    "    else:\n",
    "        raise AssertionError(\"DF_FEATURES (ou DF_FEAT) não encontrado. Execute a Etapa 5 antes.\")\n",
    "\n",
    "# --------------------------------\n",
    "# Parâmetros\n",
    "# --------------------------------\n",
    "TEST_SIZE     = 0.2     # fração para validação\n",
    "SHUFFLE       = True\n",
    "RANDOM_STATE  = SEED if 'SEED' in globals() else 42\n",
    "OUTPUT_DTYPE  = np.float32  # reduzir memória e acelerar treino\n",
    "\n",
    "CSV_SEP       = \",\"\n",
    "CSV_ENCODING  = \"utf-8-sig\"\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Seleção/Montagem da matriz de features\n",
    "# --------------------------------\n",
    "df_feats = DF_FEATURES.copy()\n",
    "\n",
    "# Autogerar FEATURE_COLS se não existir / estiver vazia\n",
    "autogen_cols = False\n",
    "if 'FEATURE_COLS' not in globals() or not isinstance(FEATURE_COLS, (list, tuple)) or len(FEATURE_COLS) == 0:\n",
    "    # pega somente colunas numéricas (inclui int/float/bool)\n",
    "    num_cols = df_feats.select_dtypes(include=[np.number, \"bool\"]).columns.tolist()\n",
    "    if len(num_cols) == 0:\n",
    "        raise ValueError(\"Não há colunas numéricas em DF_FEATURES para montar FEATURE_COLS.\")\n",
    "    FEATURE_COLS = num_cols\n",
    "    autogen_cols = True\n",
    "    # salva lista autogerada para auditoria\n",
    "    (RUN_DIR / \"feature_cols_autogen.json\").write_text(\n",
    "        json.dumps(FEATURE_COLS, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "    print(f\"FEATURE_COLS autogerado com {len(FEATURE_COLS)} colunas numéricas (salvo em feature_cols_autogen.json).\")\n",
    "\n",
    "# Garante existência das colunas selecionadas\n",
    "missing = [c for c in FEATURE_COLS if c not in df_feats.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colunas de feature ausentes em DF_FEATURES: {missing}\")\n",
    "\n",
    "# Seleciona e força numérico (segurança extra)\n",
    "df_feats = df_feats[FEATURE_COLS]\n",
    "for c in df_feats.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(df_feats[c]):\n",
    "        df_feats[c] = pd.to_numeric(df_feats[c], errors=\"coerce\")\n",
    "\n",
    "# Matriz numpy (float64 para estabilidade na normalização)\n",
    "X_all = df_feats.to_numpy(dtype=np.float64, copy=True)\n",
    "n_rows, n_cols = X_all.shape\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Split train/val\n",
    "# --------------------------------\n",
    "idx_all = np.arange(n_rows, dtype=np.int64)\n",
    "X_train, X_val, idx_train, idx_val = train_test_split(\n",
    "    X_all, idx_all,\n",
    "    test_size=TEST_SIZE, shuffle=SHUFFLE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Imputação (mediana) e aplicação\n",
    "# --------------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)  # apenas no treino\n",
    "X_train_imp = imputer.transform(X_train)\n",
    "X_val_imp   = imputer.transform(X_val)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Normalização (StandardScaler) e aplicação\n",
    "# --------------------------------\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(X_train_imp)         # apenas no treino\n",
    "X_train_std = scaler.transform(X_train_imp)\n",
    "X_val_std   = scaler.transform(X_val_imp)\n",
    "\n",
    "# Cast para dtype final (economia de RAM/tempo de treino)\n",
    "X_train_final = X_train_std.astype(OUTPUT_DTYPE, copy=False)\n",
    "X_val_final   = X_val_std.astype(OUTPUT_DTYPE, copy=False)\n",
    "\n",
    "# --------------------------------\n",
    "# 5) Persistência de artefatos e conjuntos\n",
    "# --------------------------------\n",
    "def _hash_list_str(items) -> str:\n",
    "    m = hashlib.sha256()\n",
    "    for it in items:\n",
    "        m.update(str(it).encode(\"utf-8\"))\n",
    "        m.update(b\"|\")\n",
    "    return m.hexdigest()\n",
    "\n",
    "features_hash = _hash_list_str(FEATURE_COLS)\n",
    "\n",
    "meta = {\n",
    "    \"n_rows\": int(n_rows),\n",
    "    \"n_cols\": int(n_cols),\n",
    "    \"test_size\": TEST_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"dtype\": np.dtype(OUTPUT_DTYPE).name,\n",
    "    \"features_hash\": features_hash,\n",
    "    \"n_train\": int(X_train_final.shape[0]),\n",
    "    \"n_val\": int(X_val_final.shape[0]),\n",
    "    \"autogen_feature_cols\": bool(autogen_cols),\n",
    "}\n",
    "\n",
    "# 5.1 conjuntos\n",
    "npz_path = RUN_DIR / \"dataset_npz.npz\"\n",
    "np.savez_compressed(\n",
    "    npz_path,\n",
    "    X_train=X_train_final,\n",
    "    X_val=X_val_final,\n",
    "    idx_train=idx_train,\n",
    "    idx_val=idx_val,\n",
    ")\n",
    "\n",
    "# 5.2 artefatos (imputer/scaler/feature_cols)\n",
    "feat_pkl_path = RUN_DIR / \"features.pkl\"\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"features_hash\": features_hash,\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"dtype\": np.dtype(OUTPUT_DTYPE).name,\n",
    "        \"meta\": meta,\n",
    "    },\n",
    "    feat_pkl_path\n",
    ")\n",
    "\n",
    "# cópias úteis no artifacts/\n",
    "feat_latest = ARTIF_DIR / \"features_latest.pkl\"\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"features_hash\": features_hash,\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"dtype\": np.dtype(OUTPUT_DTYPE).name,\n",
    "        \"meta\": meta,\n",
    "    },\n",
    "    feat_latest\n",
    ")\n",
    "\n",
    "# 5.3 estatísticas descritivas (para auditoria)\n",
    "def _describe_np(X: np.ndarray) -> dict:\n",
    "    X64 = X.astype(np.float64, copy=False)\n",
    "    return {\n",
    "        \"shape\": list(X.shape),\n",
    "        \"mean\": np.nanmean(X64, axis=0).tolist(),\n",
    "        \"std\":  np.nanstd(X64,  axis=0, ddof=0).tolist(),\n",
    "        \"min\":  np.nanmin(X64,  axis=0).tolist(),\n",
    "        \"p25\":  np.nanpercentile(X64, 25, axis=0).tolist(),\n",
    "        \"p50\":  np.nanpercentile(X64, 50, axis=0).tolist(),\n",
    "        \"p75\":  np.nanpercentile(X64, 75, axis=0).tolist(),\n",
    "        \"max\":  np.nanmax(X64,  axis=0).tolist(),\n",
    "    }\n",
    "\n",
    "desc = {\n",
    "    \"train\": _describe_np(X_train_final),\n",
    "    \"val\":   _describe_np(X_val_final),\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"features_hash\": features_hash,\n",
    "    \"autogen_feature_cols\": bool(autogen_cols),\n",
    "}\n",
    "(RUN_DIR / \"features_desc.json\").write_text(json.dumps(desc, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# --------------------------------\n",
    "# 6) Logs e variáveis para a Etapa 7\n",
    "# --------------------------------\n",
    "print(f\"X_all       : {X_all.shape} (antes de imputar/normalizar)\")\n",
    "print(f\"X_train/val : {X_train_final.shape} / {X_val_final.shape}\")\n",
    "print(f\"dtype final : {X_train_final.dtype}\")\n",
    "print(f\"artefatos   : {feat_pkl_path.name}, {npz_path.name}, features_desc.json\")\n",
    "print(f\"cópia útil  : artifacts/{feat_latest.name}\")\n",
    "print(f\"features_hash = {features_hash[:16]}…\")\n",
    "if autogen_cols:\n",
    "    print(f\"FEATURE_COLS autogeradas ({len(FEATURE_COLS)} colunas). Consulte feature_cols_autogen.json.\")\n",
    "\n",
    "# Mantém em memória para a Etapa 7\n",
    "X_TRAIN = X_train_final\n",
    "X_VAL   = X_val_final\n",
    "FEATURES_HASH = features_hash\n",
    "\n",
    "print(\"\\nPróximo passo: Etapa 7 — Autoencoder com early stopping usando estes conjuntos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1761015238154,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "stXMa-fFJbjI",
    "outputId": "dffda63d-a50c-4c80-dc38-d6f8515c44bf"
   },
   "outputs": [],
   "source": [
    "# @title Print das colunas de treinamento (features)\n",
    "print(\"DF_FEAT:\", DF_FEAT.shape, \"colunas:\", len(DF_FEAT.columns))\n",
    "DF_FEAT.head(3)\n",
    "\n",
    "# Se DF_FEATURES existir (caso tenha customizado manualmente)\n",
    "if 'DF_FEATURES' in globals():\n",
    "    print(\"DF_FEATURES:\", DF_FEATURES.shape, \"colunas:\", len(DF_FEATURES.columns))\n",
    "    DF_FEATURES.head(3)\n",
    "\n",
    "# Depois de rodar a Etapa 6:\n",
    "# FEATURE_COLS deve estar definido e persistido (se autogerado)\n",
    "print(\"Qtd FEATURE_COLS:\", len(FEATURE_COLS))\n",
    "FEATURE_COLS[:20]  # amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kKiIlwrQuwAH"
   },
   "outputs": [],
   "source": [
    "# @title ETAPA 6.5 — Análise de Multicolinearidade, Redundância e Relevância de Features (pós-limpeza/normalização/split)\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Executar, APÓS a etapa 6 (com split já realizado), análises de:\n",
    "1) Correlação (Pearson/Spearman) e correlação média por feature.\n",
    "2) Cluster map da matriz de correlação (para visualizar blocos redundantes).\n",
    "3) VIF (Variance Inflation Factor) para detectar redundância linear.\n",
    "4) PCA (variância explicada e scree plot).\n",
    "5) (Opcional, se y_train existir) Mutual Information e seleção por L1 (coeficiente ~0).\n",
    "\n",
    "O usuário pode optar por gerar 'features_cols_pruned.json' com uma SUGESTÃO\n",
    "de poda não-destrutiva (nada é aplicado automaticamente ao pipeline).\n",
    "\n",
    "Pré-requisitos (variáveis globais):\n",
    "- RUN_DIR: pathlib.Path (diretório do run atual)\n",
    "- X_train: np.ndarray ou pd.DataFrame (conjunto de treino já limpo/normalizado)\n",
    "- (opcional) y_train: pd.Series ou np.ndarray (rótulos do treino; classificação ou regressão)\n",
    "\n",
    "A etapa tentará inferir FEATURES_COLS automaticamente se não estiver definida.\n",
    "\n",
    "Saídas:\n",
    "- Console: resultados + explicações de interpretação.\n",
    "- Arquivos: CSV/JSON/PNG em RUN_DIR/analysis/multicollinearity e RUN_DIR/figures.\n",
    "- Figuras: exibidas diretamente no notebook e também salvas em disco.\n",
    "\"\"\"\n",
    "\n",
    "import json, math, warnings, sys\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Bibliotecas opcionais\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "except Exception:\n",
    "    sm = None\n",
    "try:\n",
    "    import seaborn as sns  # para cluster map (se disponível)\n",
    "except Exception:\n",
    "    sns = None\n",
    "try:\n",
    "    from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "    from scipy.spatial.distance import pdist\n",
    "except Exception:\n",
    "    linkage = None\n",
    "    leaves_list = None\n",
    "    pdist = None\n",
    "\n",
    "# matplotlib usado em várias partes (inclui cluster map fallback)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# -------------------- Verificações básicas --------------------\n",
    "assert 'RUN_DIR' in globals(), \"RUN_DIR não encontrado. Defina RUN_DIR antes de executar esta etapa.\"\n",
    "assert 'X_train' in globals(), \"X_train não encontrado. Esta etapa deve rodar após o split da etapa 6.\"\n",
    "\n",
    "RUN_DIR = Path(RUN_DIR)\n",
    "\n",
    "# -------------------- Inferência/normalização de FEATURES_COLS --------------------\n",
    "def _infer_features_cols() -> list:\n",
    "    # 1) Se já existir FEATURES_COLS, usa diretamente\n",
    "    if 'FEATURES_COLS' in globals():\n",
    "        fs = list(globals()['FEATURES_COLS'])\n",
    "        if len(fs) > 0:\n",
    "            return fs\n",
    "\n",
    "    # 2) Tenta FEATURE_COLS (variação comum)\n",
    "    if 'FEATURE_COLS' in globals():\n",
    "        fs = list(globals()['FEATURE_COLS'])\n",
    "        if len(fs) > 0:\n",
    "            return fs\n",
    "\n",
    "    # 3) Se X_train for DataFrame com colunas, usa as colunas\n",
    "    if isinstance(X_train, pd.DataFrame) and len(X_train.columns) > 0:\n",
    "        return list(X_train.columns)\n",
    "\n",
    "    # 4) Tenta DF_FEATURES: pega apenas colunas numéricas\n",
    "    if 'DF_FEATURES' in globals():\n",
    "        _df = globals()['DF_FEATURES']\n",
    "        if isinstance(_df, pd.DataFrame):\n",
    "            num_cols = [c for c in _df.columns if pd.api.types.is_numeric_dtype(_df[c])]\n",
    "            if len(num_cols) > 0:\n",
    "                return num_cols\n",
    "\n",
    "    # 5) Tenta features_desc.json (caso exista)\n",
    "    fdesc = RUN_DIR / \"features_desc.json\"\n",
    "    if fdesc.exists():\n",
    "        try:\n",
    "            desc = json.load(open(fdesc, \"r\"))\n",
    "            if isinstance(desc, dict):\n",
    "                if \"numeric_features\" in desc and isinstance(desc[\"numeric_features\"], list) and len(desc[\"numeric_features\"])>0:\n",
    "                    return desc[\"numeric_features\"]\n",
    "                if \"features\" in desc and isinstance(desc[\"features\"], list) and len(desc[\"features\"])>0:\n",
    "                    return desc[\"features\"]\n",
    "            elif isinstance(desc, list) and len(desc)>0:\n",
    "                return desc\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return []\n",
    "\n",
    "FEATURES_COLS = _infer_features_cols()\n",
    "assert len(FEATURES_COLS) > 0, (\n",
    "    \"Não foi possível inferir FEATURES_COLS. \"\n",
    "    \"Defina FEATURES_COLS (ou FEATURE_COLS) manualmente, ou forneça DF_FEATURES/ features_desc.json válidos.\"\n",
    ")\n",
    "\n",
    "# Monta DF de treino com as features definidas\n",
    "if isinstance(X_train, np.ndarray):\n",
    "    DF = pd.DataFrame(X_train, columns=FEATURES_COLS)\n",
    "else:\n",
    "    DF = pd.DataFrame(X_train)[FEATURES_COLS].copy()\n",
    "\n",
    "FEATURES = list(FEATURES_COLS)\n",
    "\n",
    "# -------------------- Diretórios de saída --------------------\n",
    "ART_DIR = RUN_DIR / \"figures\"\n",
    "OUT_DIR = RUN_DIR / \"analysis\" / \"multicollinearity\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- Parâmetros (ajustáveis) --------------------\n",
    "PEARSON_THR = 0.90       # Threshold para pares altamente correlacionados (Pearson)\n",
    "SPEARMAN_THR = 0.90      # Threshold para pares altamente correlacionados (Spearman)\n",
    "VIF_THR = 10.0           # Limite usual de VIF (5 ou 10)\n",
    "N_PCA_MAX = min(50, len(FEATURES))\n",
    "MI_TOP_K = 30            # Top-k para exibir no console\n",
    "LASSO_ALPHA = 0.001      # Força de regularização L1 para regressão\n",
    "LR_MAX_ITER = 300        # Iterações para Logistic Regression L1\n",
    "\n",
    "# -------------------- Pergunta ao usuário (poda sugerida) --------------------\n",
    "resp = input(\"Deseja gerar um arquivo 'features_cols_pruned.json' com uma SUGESTÃO de poda? [s/n]: \").strip().lower()\n",
    "MAKE_PRUNED = resp in (\"s\", \"sim\", \"y\", \"yes\")\n",
    "\n",
    "# -------------------- Funções auxiliares --------------------\n",
    "def detect_problem_type(y: pd.Series) -> str:\n",
    "    \"\"\"Heurística simples para classificar tipo de problema.\"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        nunique = pd.Series(y).nunique(dropna=True)\n",
    "        return \"classification\" if nunique <= 10 else \"regression\"\n",
    "    return \"classification\"\n",
    "\n",
    "def top_corr_pairs(corr_mat: pd.DataFrame, thr: float) -> pd.DataFrame:\n",
    "    pairs = []\n",
    "    cols = corr_mat.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            c = corr_mat.iloc[i, j]\n",
    "            if pd.notna(c) and abs(c) >= thr:\n",
    "                pairs.append((cols[i], cols[j], float(c)))\n",
    "    return pd.DataFrame(pairs, columns=[\"feat_i\", \"feat_j\", \"corr\"]).sort_values(by=\"corr\", key=np.abs, ascending=False)\n",
    "\n",
    "def explain(title: str, text: str):\n",
    "    print(f\"\\n[{title}]\")\n",
    "    print(text)\n",
    "\n",
    "print(\"\\n== ETAPA 6.5: Análises de correlação, VIF, PCA, MI e L1 (com explicações) ==\")\n",
    "\n",
    "# -------------------- 1) Correlações --------------------\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    corr_pearson = DF.corr(method=\"pearson\").replace([np.inf, -np.inf], np.nan)\n",
    "    corr_spearman = DF.corr(method=\"spearman\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Correlação média absoluta por feature (Pearson)\n",
    "mean_abs_corr = corr_pearson.abs().replace(1.0, np.nan).mean(skipna=True).sort_values(ascending=False)\n",
    "mean_abs_corr.name = \"mean_abs_corr_pearson\"\n",
    "mean_abs_corr.to_csv(OUT_DIR / \"mean_abs_corr_pearson.csv\")\n",
    "\n",
    "top_p = top_corr_pairs(corr_pearson, PEARSON_THR)\n",
    "top_s = top_corr_pairs(corr_spearman, SPEARMAN_THR)\n",
    "top_p.to_csv(OUT_DIR / f\"top_pairs_pearson_ge_{PEARSON_THR:.2f}.csv\", index=False)\n",
    "top_s.to_csv(OUT_DIR / f\"top_pairs_spearman_ge_{SPEARMAN_THR:.2f}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n[Correlação] Pares com |Pearson| ≥ {PEARSON_THR:.2f}: {len(top_p)} | arquivo: {OUT_DIR / f'top_pairs_pearson_ge_{PEARSON_THR:.2f}.csv'}\")\n",
    "print(f\"[Correlação] Pares com |Spearman| ≥ {SPEARMAN_THR:.2f}: {len(top_s)} | arquivo: {OUT_DIR / f'top_pairs_spearman_ge_{SPEARMAN_THR:.2f}.csv'}\")\n",
    "print(\"[Correlação] Top 10 pares (Pearson):\")\n",
    "print(top_p.head(10).to_string(index=False))\n",
    "print(\"[Correlação] Top 10 pares (Spearman):\")\n",
    "print(top_s.head(10).to_string(index=False))\n",
    "\n",
    "explain(\"Como interpretar — Correlação (Pearson/Spearman)\",\n",
    "\"\"\"Pearson mede relação linear; Spearman mede relação monótona (ordem). Pares com alta correlação indicam\n",
    "redundância informacional. Impactos: instabilidade de modelos lineares e desperdício de capacidade no autoencoder.\n",
    "Avaliação: verifique se features de pares com |r| ≥ limiar carregam conteúdo semelhante; considere manter apenas uma,\n",
    "preferindo a com maior variância informativa ou com melhor justificativa de negócio.\"\"\")\n",
    "\n",
    "# Figura: barras da correlação média (top 30) — salvar e EXIBIR\n",
    "try:\n",
    "    top_n = mean_abs_corr.head(30)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_n[::-1].plot(kind=\"barh\")  # sem especificar cores\n",
    "    plt.title(\"Top 30 — Correlação média absoluta (Pearson)\")\n",
    "    plt.xlabel(\"Correlação média absoluta com demais features\")\n",
    "    plt.tight_layout()\n",
    "    fig_corr_mean = ART_DIR / \"corr_mean_abs_top30.png\"\n",
    "    plt.savefig(fig_corr_mean, dpi=120)\n",
    "    print(f\"[Figura] Correlação média (top 30) salva em: {fig_corr_mean}\")\n",
    "    plt.show()  # <-- EXIBE NO NOTEBOOK\n",
    "    plt.close()\n",
    "    explain(\"Como interpretar — Correlação média por feature\",\n",
    "\"\"\"A barra mostra o quanto, em média, uma feature se correlaciona com todas as outras (em valor absoluto).\n",
    "Valores altos sugerem que a feature está em um bloco redundante; candidatas à revisão/remoção ou agregação.\"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"[Aviso] Falha ao gerar figura 'corr_mean_abs_top30.png': {e}\")\n",
    "\n",
    "# -------------------- 1.1) Cluster map da correlação --------------------\n",
    "fig_cluster = ART_DIR / \"corr_clustermap.png\"\n",
    "try:\n",
    "    corr_for_cluster = corr_pearson.fillna(0.0).astype(float)\n",
    "\n",
    "    if sns is not None:\n",
    "        g = sns.clustermap(\n",
    "            corr_for_cluster,\n",
    "            method=\"average\", metric=\"euclidean\",\n",
    "            cmap=\"vlag\", center=0, linewidths=0.0, figsize=(10, 10)\n",
    "        )\n",
    "        g.fig.suptitle(\"Cluster Map — Matriz de correlação (Pearson)\", y=1.02)\n",
    "        g.savefig(fig_cluster, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"[Figura] Cluster map salvo em: {fig_cluster}\")\n",
    "        plt.show()  # <-- EXIBE NO NOTEBOOK (mostra a figura produzida pelo seaborn)\n",
    "        plt.close(g.fig)\n",
    "    elif linkage is not None and leaves_list is not None and pdist is not None:\n",
    "        # Fallback: reordena por linkage e plota heatmap com matplotlib\n",
    "        D = pdist(corr_for_cluster.values)\n",
    "        Z = linkage(D, method=\"average\")\n",
    "        order = leaves_list(Z)\n",
    "        corr_ord = corr_for_cluster.values[order][:, order]\n",
    "        labels = corr_for_cluster.columns[order]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(corr_ord, aspect='auto', interpolation='none')\n",
    "        plt.colorbar(label=\"Correlação (Pearson)\")\n",
    "        plt.title(\"Cluster Map (fallback) — Matriz de correlação (Pearson)\")\n",
    "        plt.xticks(range(len(labels)), labels, rotation=90, fontsize=6)\n",
    "        plt.yticks(range(len(labels)), labels, fontsize=6)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_cluster, dpi=150)\n",
    "        print(f\"[Figura] Cluster map salvo em: {fig_cluster}\")\n",
    "        plt.show()  # <-- EXIBE NO NOTEBOOK\n",
    "        plt.close()\n",
    "    else:\n",
    "        raise RuntimeError(\"Seaborn e/ou SciPy indisponíveis para gerar o cluster map.\")\n",
    "\n",
    "    explain(\"Como interpretar — Cluster map de correlação\",\n",
    "\"\"\"O cluster map reordena a matriz de correlação para agrupar features com comportamentos semelhantes,\n",
    "formando blocos (clusters). Blocos densos com correlações altas indicam redundância. Avaliação:\n",
    "identifique 'famílias' de variáveis muito parecidas; mantenha representantes mais informativos/explicáveis,\n",
    "reduzindo dimensionalidade sem perder sinal relevante.\"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"[Aviso] Cluster map não gerado: {e}\")\n",
    "    explain(\"Como interpretar — Cluster map (não gerado)\",\n",
    "\"\"\"O cluster map auxilia a visualizar blocos de redundância. Se não foi gerado por ausência de dependências,\n",
    "considere instalar 'seaborn' ou 'scipy' para habilitar essa visualização.\"\"\")\n",
    "\n",
    "# -------------------- 2) VIF --------------------\n",
    "print(\"\\n[VIF] Início do cálculo do Variance Inflation Factor.\")\n",
    "if sm is None:\n",
    "    print(\"[VIF] Indisponível: instale 'statsmodels' para habilitar o VIF.\")\n",
    "    explain(\"Como interpretar — VIF (não calculado)\",\n",
    "\"\"\"VIF estima inflação de variância de um coeficiente devido à colinearidade com outras variáveis.\n",
    "Valores ≥ 10 sugerem forte redundância linear. Se não calculado, considere instalar 'statsmodels'.\"\"\")\n",
    "    vif_df_sorted = pd.DataFrame({\"feature\": FEATURES, \"vif\": np.nan})\n",
    "else:\n",
    "    try:\n",
    "        X_vif = DF.fillna(0.0).to_numpy(dtype=float)\n",
    "        X_vif = np.ascontiguousarray(X_vif)\n",
    "        X_vif = sm.add_constant(X_vif, has_constant='add')\n",
    "        vifs = []\n",
    "        for i in range(1, X_vif.shape[1]):  # pula a constante\n",
    "            v = variance_inflation_factor(X_vif, i)\n",
    "            vifs.append(v if np.isfinite(v) else np.nan)\n",
    "        vif_df = pd.DataFrame({\"feature\": FEATURES, \"vif\": vifs})\n",
    "        vif_df_sorted = vif_df.sort_values(by=\"vif\", ascending=False)\n",
    "        vif_df_sorted.to_csv(OUT_DIR / \"vif.csv\", index=False)\n",
    "        print(f\"[VIF] Arquivo salvo em: {OUT_DIR / 'vif.csv'}\")\n",
    "        print(\"[VIF] Top 15 (maiores VIF):\")\n",
    "        print(vif_df_sorted.head(15).to_string(index=False))\n",
    "        n_high_vif = int((vif_df[\"vif\"] >= VIF_THR).sum())\n",
    "        print(f\"[VIF] Quantidade de features com VIF ≥ {VIF_THR}: {n_high_vif}\")\n",
    "        explain(\"Como interpretar — VIF\",\n",
    "f\"\"\"VIF quantifica o quanto a variância de um coeficiente é inflada devido à colinearidade.\n",
    "Regra prática: VIF ≥ {VIF_THR} sugere redundância linear relevante.\n",
    "Avaliação: considere remover ou combinar variáveis com VIF muito alto, especialmente se também\n",
    "pertencerem a blocos correlacionados no cluster map.\"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"[VIF] Falha no cálculo do VIF: {e}\")\n",
    "        vif_df_sorted = pd.DataFrame({\"feature\": FEATURES, \"vif\": np.nan})\n",
    "        explain(\"Como interpretar — VIF (falha)\",\n",
    "\"\"\"Sem VIF, utilize correlação e cluster map como principais insumos para identificar redundâncias.\"\"\")\n",
    "\n",
    "# -------------------- 3) PCA --------------------\n",
    "print(\"\\n[PCA] Executando PCA no conjunto de treino (padronizado).\")\n",
    "try:\n",
    "    X = DF.fillna(0.0).to_numpy(dtype=float)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    n_comp = min(N_PCA_MAX, Xs.shape[1], max(2, math.ceil(Xs.shape[1]*0.2)))\n",
    "    pca = PCA(n_components=n_comp, random_state=42)\n",
    "    pcs = pca.fit_transform(Xs)\n",
    "\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    evr_cum = np.cumsum(evr)\n",
    "    def n_for(thr: float) -> int:\n",
    "        return int(np.searchsorted(evr_cum, thr) + 1)\n",
    "\n",
    "    n90, n95, n99 = n_for(0.90), n_for(0.95), n_for(0.99)\n",
    "    pd.DataFrame({\n",
    "        \"pc\": [f\"PC{i+1}\" for i in range(len(evr))],\n",
    "        \"evr\": evr,\n",
    "        \"evr_cum\": evr_cum\n",
    "    }).to_csv(OUT_DIR / \"pca_explained_variance.csv\", index=False)\n",
    "\n",
    "    # Scree plot — salvar e EXIBIR\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1, len(evr)+1), evr, marker=\"o\")\n",
    "    plt.xlabel(\"Componente Principal\")\n",
    "    plt.ylabel(\"Variância explicada\")\n",
    "    plt.title(\"PCA — Scree plot\")\n",
    "    plt.tight_layout()\n",
    "    fig_pca = ART_DIR / \"pca_scree.png\"\n",
    "    plt.savefig(fig_pca, dpi=120)\n",
    "    print(f\"[PCA] #componentes p/ 90%: {n90} | 95%: {n95} | 99%: {n99}\")\n",
    "    print(f\"[PCA] Variância explicada por componente: {OUT_DIR / 'pca_explained_variance.csv'}\")\n",
    "    print(f\"[Figura] Scree plot salvo em: {fig_pca}\")\n",
    "    plt.show()  # <-- EXIBE NO NOTEBOOK\n",
    "    plt.close()\n",
    "\n",
    "    explain(\"Como interpretar — PCA\",\n",
    "\"\"\"O PCA resume variação em componentes ortogonais. O número de componentes para 90/95/99% indica\n",
    "o grau de redundância: quanto menor esse número comparado ao total de features, maior a redundância.\n",
    "Avaliação: se poucos PCs explicam quase toda a variância, há espaço para reduzir dimensionalidade ou\n",
    "priorizar variáveis com maior contribuição (sem perder explicabilidade de negócio).\"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"[PCA] Falhou: {e}\")\n",
    "    explain(\"Como interpretar — PCA (falha)\",\n",
    "\"\"\"Sem PCA, foque em correlação e VIF para guiar decisões de redução de dimensionalidade.\"\"\")\n",
    "\n",
    "# -------------------- 4) Mutual Information e L1 (se y_train disponível) --------------------\n",
    "mi_df = None\n",
    "l1_zero_features = []\n",
    "if 'y_train' in globals():\n",
    "    print(\"\\n[Alvo detectado] y_train disponível. Executando MI e L1.\")\n",
    "    y = pd.Series(y_train)\n",
    "    X_aligned = DF.copy()  # mesmo índice/ordem de X_train\n",
    "\n",
    "    def _detect_problem_type(y_):\n",
    "        if pd.api.types.is_numeric_dtype(y_):\n",
    "            nuniq = pd.Series(y_).nunique(dropna=True)\n",
    "            return \"classification\" if nuniq <= 10 else \"regression\"\n",
    "        return \"classification\"\n",
    "\n",
    "    problem = _detect_problem_type(y)\n",
    "    print(f\"[Tarefa] Tipo de problema detectado: {problem}\")\n",
    "\n",
    "    # Mutual Information\n",
    "    try:\n",
    "        if problem == \"classification\":\n",
    "            y_enc = y.astype('category').cat.codes if not pd.api.types.is_integer_dtype(y) else y\n",
    "            mi_vals = mutual_info_classif(X_aligned, y_enc, random_state=42, discrete_features=\"auto\")\n",
    "        else:\n",
    "            y_num = pd.to_numeric(y, errors=\"coerce\").fillna(y.mean())\n",
    "            mi_vals = mutual_info_regression(X_aligned, y_num, random_state=42)\n",
    "        mi_df = pd.DataFrame({\"feature\": FEATURES, \"mutual_information\": mi_vals}).sort_values(\"mutual_information\", ascending=False)\n",
    "        mi_path = OUT_DIR / \"mutual_information.csv\"\n",
    "        mi_df.to_csv(mi_path, index=False)\n",
    "        print(f\"[MI] Top {min(MI_TOP_K, len(FEATURES))} por Mutual Information:\")\n",
    "        print(mi_df.head(MI_TOP_K).to_string(index=False))\n",
    "        print(f\"[MI] Arquivo salvo em: {mi_path}\")\n",
    "        explain(\"Como interpretar — Mutual Information (MI)\",\n",
    "\"\"\"A MI mede dependência (não só linear) entre cada feature e o alvo.\n",
    "Valores maiores indicam maior poder discriminativo. Avaliação: priorize features com MI mais alta e\n",
    "questione a utilidade das de MI muito baixa, especialmente se também redundantes por correlação/VIF.\"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"[MI] Falhou: {e}\")\n",
    "        explain(\"Como interpretar — MI (falha)\",\n",
    "\"\"\"Sem MI, utilize L1 (se disponível) e a análise de redundância (correlação/VIF) para priorização.\"\"\")\n",
    "\n",
    "    # L1 Regularization\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            if problem == \"classification\":\n",
    "                y_enc = y.astype('category').cat.codes if not pd.api.types.is_integer_dtype(y) else y\n",
    "                clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=LR_MAX_ITER)\n",
    "                clf.fit(X_aligned, y_enc)\n",
    "                coefs = clf.coef_\n",
    "                if coefs.ndim == 2:\n",
    "                    coef_abs = np.mean(np.abs(coefs), axis=0)  # média entre classes\n",
    "                else:\n",
    "                    coef_abs = np.abs(coefs)\n",
    "            else:\n",
    "                y_num = pd.to_numeric(y, errors=\"coerce\").fillna(y.mean())\n",
    "                lasso = Lasso(alpha=LASSO_ALPHA, max_iter=2000, random_state=42)\n",
    "                lasso.fit(X_aligned, y_num)\n",
    "                coef_abs = np.abs(lasso.coef_)\n",
    "\n",
    "        l1_df = pd.DataFrame({\"feature\": FEATURES, \"coef_abs\": coef_abs})\n",
    "        l1_zero_features = l1_df.loc[l1_df[\"coef_abs\"] <= 1e-12, \"feature\"].tolist()\n",
    "        l1_path = OUT_DIR / \"l1_selected_features.json\"\n",
    "        json.dump({\"l1_zero_coef_features\": l1_zero_features}, open(l1_path, \"w\"), ensure_ascii=False, indent=2)\n",
    "        print(f\"[L1] Features com coeficiente ~0: {len(l1_zero_features)} | arquivo: {l1_path}\")\n",
    "        explain(\"Como interpretar — L1 (coeficiente ~0)\",\n",
    "\"\"\"A regularização L1 zera coeficientes de variáveis pouco úteis sob o critério do modelo linear.\n",
    "Avaliação: features com coeficiente ~0 são candidatas a remoção, sobretudo se também redundantes por correlação/VIF\n",
    "ou com MI baixa. Atenção: L1 é um critério adicional; mantenha o julgamento de negócio e estabilidade temporal.\"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"[L1] Falhou: {e}\")\n",
    "        explain(\"Como interpretar — L1 (falha)\",\n",
    "\"\"\"Sem L1, utilize MI (se disponível) e os sinais de redundância (correlação/VIF) para apoiar a seleção.\"\"\")\n",
    "else:\n",
    "    print(\"\\n[Alvo ausente] y_train não encontrado. Pulando MI e L1.\")\n",
    "    explain(\"Como interpretar — Ausência de alvo\",\n",
    "\"\"\"Sem rótulo, priorize decisões com base em redundância (correlação/cluster map/VIF) e em critérios de explicabilidade.\n",
    "Se futuramente houver alvo, reexecute MI/L1 para consolidar prioridades.\"\"\")\n",
    "\n",
    "# -------------------- 5) Consolidação de recomendações e poda sugerida --------------------\n",
    "reco_path = OUT_DIR / \"suggested_drops.json\"\n",
    "reco = {\n",
    "    \"thresholds\": {\n",
    "        \"pearson_abs\": PEARSON_THR,\n",
    "        \"spearman_abs\": SPEARMAN_THR,\n",
    "        \"vif\": VIF_THR,\n",
    "        \"lasso_alpha\": LASSO_ALPHA\n",
    "    },\n",
    "    \"drop_candidates\": {\n",
    "        \"high_correlation\": [],\n",
    "        \"high_vif\": [],\n",
    "        \"low_mi_or_zero_coef_L1\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Heurística de sugestão por pares correlacionados: remove a de menor variância no par\n",
    "variances = DF.var(numeric_only=True)\n",
    "def add_corr_suggestions(df_pairs: pd.DataFrame, method: str):\n",
    "    for _, row in df_pairs.iterrows():\n",
    "        i, j, c = row[\"feat_i\"], row[\"feat_j\"], row[\"corr\"]\n",
    "        vi, vj = variances.get(i, np.nan), variances.get(j, np.nan)\n",
    "        if pd.isna(vi) and pd.isna(vj):\n",
    "            drop, keep = j, i\n",
    "        elif pd.isna(vi):\n",
    "            drop, keep = i, j\n",
    "        elif pd.isna(vj):\n",
    "            drop, keep = j, i\n",
    "        else:\n",
    "            drop, keep = (i, j) if vi < vj else (j, i)\n",
    "        reco[\"drop_candidates\"][\"high_correlation\"].append({\n",
    "            \"method\": method, \"pair\": [i, j], \"corr\": float(c), \"suggest_drop\": drop, \"suggest_keep\": keep\n",
    "        })\n",
    "\n",
    "add_corr_suggestions(top_p, method=\"pearson\")\n",
    "add_corr_suggestions(top_s, method=\"spearman\")\n",
    "\n",
    "# VIF candidatos\n",
    "if 'vif_df_sorted' in locals() and isinstance(vif_df_sorted, pd.DataFrame) and (\"vif\" in vif_df_sorted.columns):\n",
    "    for _, r in vif_df_sorted.iterrows():\n",
    "        v = r.get(\"vif\", np.nan)\n",
    "        if pd.notna(v) and v >= VIF_THR:\n",
    "            reco[\"drop_candidates\"][\"high_vif\"].append({\"feature\": r[\"feature\"], \"vif\": float(v)})\n",
    "\n",
    "# MI/L1 candidatos (se disponíveis)\n",
    "if 'mi_df' in locals() and isinstance(mi_df, pd.DataFrame) and (\"mutual_information\" in mi_df.columns):\n",
    "    mi_vals = mi_df[\"mutual_information\"].values\n",
    "    if len(mi_vals) > 0:\n",
    "        q10 = np.quantile(mi_vals, 0.10)\n",
    "        low_mi = mi_df.loc[mi_df[\"mutual_information\"] <= q10, \"feature\"].tolist()\n",
    "    else:\n",
    "        low_mi = []\n",
    "else:\n",
    "    low_mi = []\n",
    "\n",
    "low_signal = sorted(set(low_mi).union(set(l1_zero_features))) if l1_zero_features else low_mi\n",
    "reco[\"drop_candidates\"][\"low_mi_or_zero_coef_L1\"] = low_signal\n",
    "\n",
    "# Salva recomendações\n",
    "json.dump(reco, open(reco_path, \"w\"), ensure_ascii=False, indent=2)\n",
    "print(f\"\\n[Recomendações] Arquivo salvo em: {reco_path}\")\n",
    "explain(\"Como interpretar — Recomendações de poda\",\n",
    "\"\"\"As recomendações consolidam três sinais: (i) alta correlação (redundância), (ii) VIF elevado (redundância linear),\n",
    "(iii) baixa utilidade segundo MI/L1 (se alvo disponível). Avaliação: priorize remoção dentro de blocos redundantes\n",
    "observados no cluster map, confirme consistência de negócio e monitore estabilidade temporal (drift) após a poda.\"\"\")\n",
    "\n",
    "# Geração opcional de features_cols_pruned.json\n",
    "if MAKE_PRUNED:\n",
    "    drops = set()\n",
    "    # 1) Pares correlacionados: aceitar sugestões de 'suggest_drop'\n",
    "    for item in reco[\"drop_candidates\"][\"high_correlation\"]:\n",
    "        drops.add(item[\"suggest_drop\"])\n",
    "    # 2) VIF alto\n",
    "    for item in reco[\"drop_candidates\"][\"high_vif\"]:\n",
    "        drops.add(item[\"feature\"])\n",
    "    # 3) Baixo sinal (MI/L1)\n",
    "    for f in low_signal:\n",
    "        drops.add(f)\n",
    "\n",
    "    # Garante não remover todas as colunas; mantém pelo menos 1\n",
    "    pruned = [c for c in FEATURES if c not in drops]\n",
    "    if len(pruned) == 0:\n",
    "        pruned = FEATURES[:]  # fallback: não remove nada\n",
    "\n",
    "    # Relatório de poda\n",
    "    pruning_report = {\n",
    "        \"original_count\": len(FEATURES),\n",
    "        \"suggested_count\": len(pruned),\n",
    "        \"removed_count\": len(FEATURES) - len(pruned),\n",
    "        \"removed_list\": sorted(list(set(FEATURES) - set(pruned))),\n",
    "        \"kept_list\": pruned,\n",
    "        \"criteria_order\": [\"correlation_suggestion\", \"vif_high\", \"low_signal_mi_or_l1\"]\n",
    "    }\n",
    "\n",
    "    pruned_path = OUT_DIR / \"features_cols_pruned.json\"\n",
    "    report_path = OUT_DIR / \"pruning_report.json\"\n",
    "    json.dump(pruned, open(pruned_path, \"w\"), ensure_ascii=False, indent=2)\n",
    "    json.dump(pruning_report, open(report_path, \"w\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n[Poda sugerida] Lista salva em: {pruned_path}\")\n",
    "    print(f\"[Poda sugerida] Relatório salvo em: {report_path}\")\n",
    "    explain(\"Como interpretar — 'features_cols_pruned.json'\",\n",
    "\"\"\"A lista propõe remoções com base nos sinais combinados. Use-a como INSUMO: avalie impactos na\n",
    "explicabilidade e na estabilidade temporal antes de aplicar. Recomenda-se testar desempenho/reconstrução\n",
    "e drift após a poda, comparando com a lista original.\"\"\")\n",
    "else:\n",
    "    print(\"\\n[Poda sugerida] Usuário optou por NÃO gerar 'features_cols_pruned.json'.\")\n",
    "    explain(\"Próximos passos (sem poda automática)\",\n",
    "\"\"\"Revise os arquivos gerados (pares correlacionados, VIF, MI/L1, PCA e cluster map) e selecione manualmente\n",
    "as features a remover. Registre as decisões para rastreabilidade (auditoria) e reavalie o desempenho.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duK-Dm3-e29D"
   },
   "source": [
    "# **Etapa 7:** Autoencoder com early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "executionInfo": {
     "elapsed": 334685,
     "status": "ok",
     "timestamp": 1761003892860,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "0vCxdaEQqqM1",
    "outputId": "dc68ba2d-8eef-4c61-b2b6-0f48f565b0d1"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from __future__ import annotations\n",
    "import os, json, math, time, random, re, shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(\"Skynet Informa: Treinando o modelo.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Pré-condições\n",
    "# ---------------------------------------------------------------------\n",
    "assert 'RUN_DIR' in globals(), \"Defina RUN_DIR na Etapa 1.\"\n",
    "RUN_DIR = Path(RUN_DIR)\n",
    "ART_DIR = RUN_DIR / \"figures\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_NPZ = RUN_DIR / \"dataset_npz.npz\"\n",
    "assert DATASET_NPZ.exists(), \"dataset_npz.npz não encontrado. Execute o Etapa 6.\"\n",
    "\n",
    "# Diretório artifacts (snapshot da Etapa 5 deve estar aqui)\n",
    "PROJ_ROOT = Path(globals().get(\"PROJ_ROOT\", Path.cwd()))\n",
    "ARTIFACTS_DIR = Path(globals().get(\"ARTIFACTS_DIR\", PROJ_ROOT / \"artifacts\"))\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# (Opcional) configuração existente para reaproveitar algo (não obrigatório)\n",
    "MODEL_CFG_PATH = RUN_DIR / \"model_config.json\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Hiperparâmetros (padrões seguros; pode ajustar livremente)\n",
    "# ---------------------------------------------------------------------\n",
    "SEED            = 42\n",
    "BATCH_SIZE      = 512\n",
    "EPOCHS          = 100\n",
    "PATIENCE        = 10\n",
    "LR              = 1e-3\n",
    "WEIGHT_DECAY    = 0.0\n",
    "HIDDEN_SIZES    = [128, 64]\n",
    "LATENT_DIM      = 16\n",
    "LOSS_FN         = \"mse\"                # ou \"mae\"\n",
    "USE_BN          = True\n",
    "DROPOUT         = 0.0\n",
    "NUM_WORKERS     = 0\n",
    "\n",
    "# Permite sobrescrever via dicionário externo (opcional)\n",
    "if 'MODEL_CFG_OVERRIDE' in globals() and isinstance(MODEL_CFG_OVERRIDE, dict):\n",
    "    locals().update({k:v for k,v in MODEL_CFG_OVERRIDE.items() if k.isupper()})\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Reprodutibilidade\n",
    "# ---------------------------------------------------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo - {device}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Carregar dados (Etapa 6 gerou dataset_npz.npz)\n",
    "# ---------------------------------------------------------------------\n",
    "npz = np.load(DATASET_NPZ)\n",
    "X_train = npz[\"X_train\"].astype(np.float32)\n",
    "X_val   = npz[\"X_val\"].astype(np.float32)\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"Formas: X_train={X_train.shape}, X_val={X_val.shape}\")\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(torch.from_numpy(X_train)),\n",
    "                      batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(TensorDataset(torch.from_numpy(X_val)),\n",
    "                      batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Definição do modelo (MLP simétrico)\n",
    "# ---------------------------------------------------------------------\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: list[int], latent: int,\n",
    "                 use_bn: bool = True, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        layers_enc = []; last = in_dim\n",
    "        for h in hidden:\n",
    "            layers_enc += [nn.Linear(last, h)]\n",
    "            if use_bn: layers_enc += [nn.BatchNorm1d(h)]\n",
    "            layers_enc += [nn.ReLU(inplace=True)]\n",
    "            if dropout > 0: layers_enc += [nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        layers_enc += [nn.Linear(last, latent)]\n",
    "        self.encoder = nn.Sequential(*layers_enc)\n",
    "        layers_dec = []; last = latent\n",
    "        for h in reversed(hidden):\n",
    "            layers_dec += [nn.Linear(last, h)]\n",
    "            if use_bn: layers_dec += [nn.BatchNorm1d(h)]\n",
    "            layers_dec += [nn.ReLU(inplace=True)]\n",
    "            if dropout > 0: layers_dec += [nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        layers_dec += [nn.Linear(last, in_dim)]\n",
    "        self.decoder = nn.Sequential(*layers_dec)\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "model = AE(input_dim, HIDDEN_SIZES, LATENT_DIM, USE_BN, DROPOUT).to(device)\n",
    "criterion = nn.MSELoss(reduction=\"mean\") if LOSS_FN.lower() == \"mse\" else nn.L1Loss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Loop de treinamento com Early Stopping\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"Objetivo - menor train_loss e val_loss\")\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "best_val = float(\"inf\"); best_epoch = -1; pat_left = PATIENCE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    tr_loss_sum, tr_count = 0.0, 0\n",
    "    for (xb,) in train_dl:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        xr = model(xb)\n",
    "        loss = criterion(xr, xb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        bs = xb.size(0)\n",
    "        tr_loss_sum += loss.item() * bs; tr_count += bs\n",
    "    train_loss = tr_loss_sum / max(tr_count, 1)\n",
    "\n",
    "    model.eval()\n",
    "    va_loss_sum, va_count = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for (xb,) in val_dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            xr = model(xb)\n",
    "            loss = criterion(xr, xb)\n",
    "            bs = xb.size(0)\n",
    "            va_loss_sum += loss.item() * bs; va_count += bs\n",
    "    val_loss = va_loss_sum / max(va_count, 1)\n",
    "\n",
    "    history[\"epoch\"].append(epoch); history[\"train_loss\"].append(train_loss); history[\"val_loss\"].append(val_loss)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch {epoch:03d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n",
    "\n",
    "    if val_loss + 1e-12 < best_val:\n",
    "        best_val = val_loss; best_epoch = epoch; pat_left = PATIENCE\n",
    "        torch.save(model.state_dict(), RUN_DIR / \"ae.pt\")\n",
    "    else:\n",
    "        pat_left -= 1\n",
    "        if pat_left <= 0:\n",
    "            print(f\"Early stopping em epoch {epoch} (best_epoch={best_epoch}, best_val={best_val:.6f})\")\n",
    "            break\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Salvar histórico e curva\n",
    "# ---------------------------------------------------------------------\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_csv = RUN_DIR / \"training_history.csv\"; hist_df.to_csv(hist_csv, index=False)\n",
    "print(f\"Salvo - {hist_csv}\")\n",
    "\n",
    "plt.figure(figsize=(8,4.6))\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"Treino\")\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"],   label=\"Validação\")\n",
    "plt.title(\"Curva de treinamento do Autoencoder\")\n",
    "plt.xlabel(\"Época\"); plt.ylabel(\"Erro médio (loss)\")\n",
    "plt.legend(); plt.grid(alpha=0.2)\n",
    "curve_png = ART_DIR / \"training_curve.png\"\n",
    "plt.tight_layout(); plt.savefig(curve_png, dpi=150, bbox_inches=\"tight\"); plt.show(); plt.close()\n",
    "print(f\"Salvo - {curve_png}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Recarregar melhor checkpoint e salvar erros de reconstrução (validação)\n",
    "# ---------------------------------------------------------------------\n",
    "if (RUN_DIR / \"ae.pt\").exists():\n",
    "    model.load_state_dict(torch.load(RUN_DIR / \"ae.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "val_tensor = torch.from_numpy(X_val).to(device)\n",
    "with torch.no_grad():\n",
    "    xr_val = model(val_tensor).cpu().numpy()\n",
    "val_err = ((X_val - xr_val)**2).mean(axis=1).astype(np.float32)\n",
    "recon_err_val_path = RUN_DIR / \"reconstruction_errors_val.npy\"\n",
    "np.save(recon_err_val_path, val_err)\n",
    "print(f\"Salvo - {recon_err_val_path} (shape={val_err.shape})\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Registro do treino (auditoria)\n",
    "# ---------------------------------------------------------------------\n",
    "model_config_train = {\n",
    "    \"created_at\": datetime.now(ZoneInfo(\"America/Sao_Paulo\")).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"device\": str(device),\n",
    "    \"input_dim\": int(input_dim),\n",
    "    \"hidden_sizes\": list(map(int, HIDDEN_SIZES)),\n",
    "    \"latent_dim\": int(LATENT_DIM),\n",
    "    \"use_batchnorm\": bool(USE_BN),\n",
    "    \"dropout\": float(DROPOUT),\n",
    "    \"loss_fn\": LOSS_FN.lower(),\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": float(LR),\n",
    "    \"weight_decay\": float(WEIGHT_DECAY),\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "    \"epochs_requested\": int(EPOCHS),\n",
    "    \"early_stopping_patience\": int(PATIENCE),\n",
    "    \"best_epoch\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val) if math.isfinite(best_val) else None,\n",
    "}\n",
    "(RUN_DIR / \"model_config.train.json\").write_text(json.dumps(model_config_train, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Salvo - {RUN_DIR / 'model_config.train.json'}\")\n",
    "\n",
    "print(\"TREINAMENTO CONCLUÍDO.\")\n",
    "\n",
    "# =====================================================================\n",
    "# ESPELHO + VALIDAÇÃO STRICT + PUBLICAÇÃO (encoder/ver_N)\n",
    "# =====================================================================\n",
    "print(\"\\nDefinir versão do modelo.\")\n",
    "\n",
    "# 1) Espelhar arquitetura do checkpoint -> model_config.json (consumida pela Etapa 8)\n",
    "state_dict = torch.load(RUN_DIR / \"ae.pt\", map_location=\"cpu\")\n",
    "if isinstance(state_dict, dict) and \"state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "pat_enc = re.compile(r\"encoder\\.(\\d+)\\.weight$\")\n",
    "pat_dec = re.compile(r\"decoder\\.(\\d+)\\.weight$\")\n",
    "\n",
    "enc_linear = []; dec_linear = []\n",
    "for k, v in state_dict.items():\n",
    "    if hasattr(v, \"dim\") and v.dim() == 2:  # pesos Linear\n",
    "        m = pat_enc.match(k)\n",
    "        if m:\n",
    "            enc_linear.append((int(m.group(1)), v.shape))\n",
    "        else:\n",
    "            m = pat_dec.match(k)\n",
    "            if m:\n",
    "                dec_linear.append((int(m.group(1)), v.shape))\n",
    "enc_linear.sort(key=lambda x: x[0]); dec_linear.sort(key=lambda x: x[0])\n",
    "if not enc_linear or not dec_linear:\n",
    "    raise RuntimeError(\"Não foi possível inferir camadas lineares do checkpoint.\")\n",
    "\n",
    "def _has_bn(prefix: str, idx: int, sd: dict) -> bool:\n",
    "    return (f\"{prefix}.{idx+1}.running_mean\" in sd) and (f\"{prefix}.{idx+1}.running_var\" in sd)\n",
    "\n",
    "input_dim_inf   = int(enc_linear[0][1][1])\n",
    "enc_outs        = [int(s[1][0]) for s in enc_linear]          # inclui o bottleneck na última posição\n",
    "dec_outs        = [int(s[1][0]) for s in dec_linear]          # inclui a saída final (= input_dim)\n",
    "enc_bn_flags    = [_has_bn(\"encoder\", idx, state_dict) for (idx, _shape) in [(x[0], x[1]) for x in enc_linear]]\n",
    "dec_bn_flags    = [_has_bn(\"decoder\", idx, state_dict) for (idx, _shape) in [(x[0], x[1]) for x in dec_linear]]\n",
    "hidden_list_inf = enc_outs[:-1]\n",
    "bottleneck_inf  = enc_outs[-1]\n",
    "\n",
    "model_config_mirrored = {\n",
    "    # campos canônicos consumidos pela Etapa 8\n",
    "    \"input_dim\": input_dim_inf,\n",
    "    \"hidden_list\": hidden_list_inf,\n",
    "    \"bottleneck\": bottleneck_inf,\n",
    "    \"dropout_p\": float(DROPOUT),\n",
    "    \"use_batchnorm\": bool(any(enc_bn_flags) or any(dec_bn_flags)),\n",
    "    \"enc_bn_flags\": enc_bn_flags,\n",
    "    \"dec_bn_flags\": dec_bn_flags,\n",
    "    # campos completos (eliminam ambiguidade)\n",
    "    \"enc_outs\": enc_outs,\n",
    "    \"dec_outs\": dec_outs\n",
    "}\n",
    "(RUN_DIR / \"model_config.json\").write_text(json.dumps(model_config_mirrored, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"model_config.json espelhado do checkpoint.\")\n",
    "\n",
    "# 2) Validação STRICT do espelho (reconstrói o modelo e carrega strict)\n",
    "class AE_BN_Strict(nn.Module):\n",
    "    def __init__(self, input_dim, enc_outs, enc_bn_flags, dec_outs, dec_bn_flags, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        enc_layers=[]; last=input_dim\n",
    "        for i,h in enumerate(enc_outs):\n",
    "            enc_layers.append(nn.Linear(last,h))\n",
    "            if i < len(enc_bn_flags) and enc_bn_flags[i]:\n",
    "                enc_layers.append(nn.BatchNorm1d(h))\n",
    "            enc_layers.append(nn.ReLU())\n",
    "            if dropout_p and dropout_p>0:\n",
    "                enc_layers.append(nn.Dropout(dropout_p))\n",
    "            last=h\n",
    "        self.encoder=nn.Sequential(*enc_layers)\n",
    "        dec_layers=[]; last=enc_outs[-1]\n",
    "        for i,h in enumerate(dec_outs):\n",
    "            dec_layers.append(nn.Linear(last,h))\n",
    "            if i < len(dec_outs)-1:\n",
    "                if i < len(dec_bn_flags) and dec_bn_flags[i]:\n",
    "                    dec_layers.append(nn.BatchNorm1d(h))\n",
    "                dec_layers.append(nn.ReLU())\n",
    "                if dropout_p and dropout_p>0:\n",
    "                    dec_layers.append(nn.Dropout(dropout_p))\n",
    "            last=h\n",
    "        self.decoder=nn.Sequential(*dec_layers)\n",
    "    def forward(self,x): return self.decoder(self.encoder(x))\n",
    "\n",
    "probe = AE_BN_Strict(model_config_mirrored[\"input_dim\"],\n",
    "                     model_config_mirrored[\"enc_outs\"],\n",
    "                     model_config_mirrored[\"enc_bn_flags\"],\n",
    "                     model_config_mirrored[\"dec_outs\"],\n",
    "                     model_config_mirrored[\"dec_bn_flags\"],\n",
    "                     model_config_mirrored[\"dropout_p\"])\n",
    "probe.load_state_dict(state_dict, strict=True)\n",
    "print(\"Validação STRICT do espelho OK.\")\n",
    "\n",
    "# 3) Selecionar “ver_N”: criar nova ou sobrescrever existente\n",
    "def _list_versions(encoder_dir: Path):\n",
    "    if not encoder_dir.exists(): return []\n",
    "    out=[]\n",
    "    for d in encoder_dir.iterdir():\n",
    "        if d.is_dir() and d.name.startswith(\"ver_\"):\n",
    "            try: int(d.name.split(\"_\")[1]); out.append(d.name)\n",
    "            except: pass\n",
    "    out.sort(key=lambda s: int(s.split(\"_\")[1])); return out\n",
    "\n",
    "def _next_version_name(encoder_dir: Path):\n",
    "    vers = _list_versions(encoder_dir)\n",
    "    return \"ver_1\" if not vers else f\"ver_{int(vers[-1].split('_')[1])+1}\"\n",
    "\n",
    "def _prompt_input(prompt: str, default: str = \"\") -> str:\n",
    "    try:\n",
    "        s = input(prompt);\n",
    "        return default if s is None or s.strip()==\"\" else s.strip()\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "encoder_dir = PROJ_ROOT / \"encoder\"\n",
    "encoder_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "existentes = _list_versions(encoder_dir)\n",
    "print(f\"Versões existentes em encoder/: {existentes if existentes else '(nenhuma)'}\")\n",
    "print(\"[1] Criar NOVA versão\")\n",
    "if existentes: print(\"[2] SOBRESCREVER uma versão existente\")\n",
    "choice = _prompt_input(\"Escolha [1-2] (vazio = 1): \", default=\"1\")\n",
    "\n",
    "if choice == \"2\" and existentes:\n",
    "    for i,v in enumerate(existentes,1): print(f\"{i}) {v}\")\n",
    "    idx_str = _prompt_input(\"Informe o índice da versão a sobrescrever: \")\n",
    "    try: idx = int(idx_str); ver_name = existentes[idx-1]\n",
    "    except: raise RuntimeError(\"Índice inválido.\")\n",
    "    target = encoder_dir / ver_name\n",
    "    confirm = _prompt_input(f\"Confirma sobrescrever {ver_name}? Digite exatamente '{ver_name}': \")\n",
    "    if confirm != ver_name: raise RuntimeError(\"Operação cancelada.\")\n",
    "    # limpar conteúdo\n",
    "    for item in list(target.iterdir()):\n",
    "        if item.is_dir(): shutil.rmtree(item)\n",
    "        else:\n",
    "            try: item.unlink()\n",
    "            except FileNotFoundError: pass\n",
    "else:\n",
    "    ver_name = _next_version_name(encoder_dir)\n",
    "    target = encoder_dir / ver_name\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Criando nova versão: {ver_name}\")\n",
    "\n",
    "# 4) Copiar artefatos para a versão\n",
    "shutil.copy2(RUN_DIR / \"ae.pt\",               target / \"ae.pt\")\n",
    "shutil.copy2(RUN_DIR / \"model_config.json\",   target / \"model_config.json\")\n",
    "features_pkl = RUN_DIR / \"features.pkl\"; assert features_pkl.exists(), \"features.pkl não encontrado na RUN (rode a Etapa 6).\"\n",
    "shutil.copy2(features_pkl,                    target / \"features.pkl\")\n",
    "cat_maps = RUN_DIR / \"categorical_maps.json\"\n",
    "if cat_maps.exists(): shutil.copy2(cat_maps,  target / \"categorical_maps.json\")\n",
    "val_err_np = RUN_DIR / \"reconstruction_errors_val.npy\"\n",
    "if val_err_np.exists(): shutil.copy2(val_err_np, target / \"reconstruction_errors_val.npy\")\n",
    "threshold_json = RUN_DIR / \"threshold.json\"\n",
    "if threshold_json.exists(): shutil.copy2(threshold_json, target / \"threshold.json\")\n",
    "snap_py_src = ARTIFACTS_DIR / \"pipeline_snapshot.py\"\n",
    "assert snap_py_src.exists(), \"pipeline_snapshot.py não encontrado em artifacts/ (rode o congelamento na Etapa 5).\"\n",
    "shutil.copy2(snap_py_src, target / \"pipeline_snapshot.py\")\n",
    "md5_src = ARTIFACTS_DIR / \"pipeline_snapshot.md5\"\n",
    "if md5_src.exists(): shutil.copy2(md5_src, target / \"pipeline_snapshot.md5\")\n",
    "\n",
    "# metadados\n",
    "version_meta = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"run_dir\": str(RUN_DIR),\n",
    "    \"notes\": \"Versão publicada a partir desta run.\",\n",
    "}\n",
    "(target / \"version_meta.json\").write_text(json.dumps(version_meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Versão publicada em: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg72YRNCg6nt"
   },
   "source": [
    "# **Etapa 8:** Pontuação (geração de scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7229,
     "status": "ok",
     "timestamp": 1761005259980,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "paQ16xEkFww6",
    "outputId": "17bc801d-1bdb-495a-f378-41ed7bcb9e02"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os, re, json, shutil, hashlib, gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import importlib.util\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import nullcontext\n",
    "\n",
    "print(\"Skynet Informa: Calculando score de erro e pontuando registros (modo low-RAM).\")\n",
    "\n",
    "# ---------------- contexto base ----------------\n",
    "PROJ_ROOT  = Path(globals().get(\"PROJ_ROOT\", Path.cwd()))\n",
    "RUN_DIR    = Path(globals().get(\"RUN_DIR\", PROJ_ROOT / \"runs\" / datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "PRERUN_DIR = Path(globals().get(\"PRERUN_DIR\", PROJ_ROOT / \"prerun\"))\n",
    "FIGURES_DIR= Path(globals().get(\"FIGURES_DIR\", RUN_DIR / \"figures\"))\n",
    "for p in [RUN_DIR, FIGURES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- utilitários ----------------\n",
    "def _list_versions(enc_dir: Path):\n",
    "    if not enc_dir.exists(): return []\n",
    "    out=[]\n",
    "    for d in enc_dir.iterdir():\n",
    "        if d.is_dir() and d.name.startswith(\"ver_\"):\n",
    "            try:\n",
    "                int(d.name.split(\"_\")[1])\n",
    "                out.append(d.name)\n",
    "            except:\n",
    "                pass\n",
    "    out.sort(key=lambda s: int(s.split(\"_\")[1]))\n",
    "    return out\n",
    "\n",
    "def _latest_version_dir(root: Path) -> Path | None:\n",
    "    enc = root / \"encoder\"\n",
    "    vers = _list_versions(enc)\n",
    "    return None if not vers else enc / vers[-1]\n",
    "\n",
    "def _prompt_input(prompt: str, default: str = \"\") -> str:\n",
    "    try:\n",
    "        s = input(prompt)\n",
    "        return default if s is None or s.strip()==\"\" else s.strip()\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _md5(path: Path, nbytes: int = 1<<20) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(nbytes)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# ---------------- selecionar versão ----------------\n",
    "ENCODER_VERSION = globals().get(\"ENCODER_VERSION\", None)  # ex.: \"ver_2\"\n",
    "encoder_root = PROJ_ROOT / \"encoder\"\n",
    "versions = _list_versions(encoder_root)\n",
    "if not versions:\n",
    "    raise RuntimeError(\"Nenhuma versão encontrada em encoder/. Publique uma versão na Etapa 7.\")\n",
    "\n",
    "if ENCODER_VERSION and (encoder_root / ENCODER_VERSION).exists():\n",
    "    ver_dir = encoder_root / ENCODER_VERSION\n",
    "    print(f\"Usando versão fixa: {ver_dir.name}\")\n",
    "else:\n",
    "    print(f\"Versões disponíveis: {versions}\")\n",
    "    print(\"[1] Usar a ÚLTIMA versão\")\n",
    "    print(\"[2] Selecionar uma versão pelo índice\")\n",
    "    choice = _prompt_input(\"Escolha [1-2] (vazio = 1): \", default=\"1\")\n",
    "    if choice == \"2\":\n",
    "        for i, v in enumerate(versions, 1):\n",
    "            print(f\"{i}) {v}\")\n",
    "        idx_str = _prompt_input(\"Informe o índice da versão desejada: \", default=str(len(versions)))\n",
    "        try:\n",
    "            idx = int(idx_str)\n",
    "            ver_dir = encoder_root / versions[idx-1]\n",
    "        except:\n",
    "            raise RuntimeError(\"Índice inválido.\")\n",
    "    else:\n",
    "        ver_dir = _latest_version_dir(PROJ_ROOT)\n",
    "    print(f\"Versão selecionada: {ver_dir.name}\")\n",
    "\n",
    "# ---------------- artefatos da versão ----------------\n",
    "ae_pt         = ver_dir / \"ae.pt\"\n",
    "model_cfg_json= ver_dir / \"model_config.json\"\n",
    "features_pkl  = ver_dir / \"features.pkl\"\n",
    "cat_maps_path = ver_dir / \"categorical_maps.json\"   # opcional\n",
    "val_err_path  = ver_dir / \"reconstruction_errors_val.npy\"  # opcional\n",
    "snap_py       = ver_dir / \"pipeline_snapshot.py\"    # snapshot da Etapa 5 (obrigatório)\n",
    "\n",
    "assert ae_pt.exists(),          f\"{ae_pt.name} ausente em {ver_dir}\"\n",
    "assert model_cfg_json.exists(), f\"{model_cfg_json.name} ausente em {ver_dir}\"\n",
    "assert features_pkl.exists(),   f\"{features_pkl.name} ausente em {ver_dir}\"\n",
    "assert snap_py.exists(),        f\"{snap_py.name} ausente em {ver_dir} (publique a versão na Etapa 7 após congelar a Etapa 5).\"\n",
    "\n",
    "# ---------------- importar pipeline da versão (robusto) ----------------\n",
    "import types, re, json\n",
    "\n",
    "def _load_pipeline_module(snap_py_path: Path):\n",
    "    code = snap_py_path.read_text(encoding=\"utf-8\")\n",
    "    mod = types.ModuleType(\"pipeline_snapshot\")\n",
    "    # injeta dependências comuns\n",
    "    mod.__dict__.update({\"pd\": pd, \"np\": np, \"re\": re, \"json\": json, \"__file__\": str(snap_py_path)})\n",
    "    exec(compile(code, str(snap_py_path), \"exec\"), mod.__dict__)\n",
    "    return mod\n",
    "\n",
    "if \"build_features\" not in globals() or \"encode_categoricals\" not in globals():\n",
    "    mod = _load_pipeline_module(snap_py)\n",
    "    if \"build_features\" not in globals():\n",
    "        globals()[\"build_features\"] = getattr(mod, \"build_features\")\n",
    "    if hasattr(mod, \"encode_categoricals\") and \"encode_categoricals\" not in globals():\n",
    "        globals()[\"encode_categoricals\"] = getattr(mod, \"encode_categoricals\")\n",
    "    print(\"Pipeline importada do snapshot da versão (com injeção de pd/np/re/json).\")\n",
    "\n",
    "# ---------------- carregar model_config + features.pkl ----------------\n",
    "with open(model_cfg_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "import pickle, joblib\n",
    "def _try_load_features(path: Path):\n",
    "    last = \"\"\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f), \"pickle\"\n",
    "    except Exception as e_pick:\n",
    "        last = f\"pickle:{type(e_pick).__name__} {e_pick}\"\n",
    "    try:\n",
    "        return joblib.load(path), \"joblib\"\n",
    "    except Exception as e_job:\n",
    "        last += f\" | joblib:{type(e_job).__name__} {e_job}\"\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")\n",
    "        return obj, \"torch\"\n",
    "    except Exception as e_t:\n",
    "        last += f\" | torch:{type(e_t).__name__} {e_t}\"\n",
    "    raise RuntimeError(f\"Falha ao carregar features.pkl ({last})\")\n",
    "\n",
    "features_pack, loader_tag = _try_load_features(features_pkl)\n",
    "print(f\"features.pkl | loader={loader_tag} | md5={_md5(features_pkl)} | bytes={features_pkl.stat().st_size}\")\n",
    "\n",
    "for req in (\"feature_cols\", \"imputer\", \"scaler\"):\n",
    "    if req not in features_pack:\n",
    "        raise RuntimeError(f\"features.pkl incompleto; falta '{req}'. Refaça a Etapa 6.\")\n",
    "feature_cols  = features_pack[\"feature_cols\"]\n",
    "imputer       = features_pack[\"imputer\"]\n",
    "scaler        = features_pack[\"scaler\"]\n",
    "dtype_map     = features_pack.get(\"dtype\", None)\n",
    "features_hash = features_pack.get(\"features_hash\", None)\n",
    "if features_hash:\n",
    "    print(f\"features_hash: {features_hash}\")\n",
    "\n",
    "# ---------------- modelo STRICT a partir do config da versão ----------------\n",
    "class AE_BN_Strict(nn.Module):\n",
    "    def __init__(self, input_dim, enc_outs, enc_bn_flags, dec_outs, dec_bn_flags, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        enc_layers=[]; last=input_dim\n",
    "        for i,h in enumerate(enc_outs):\n",
    "            enc_layers.append(nn.Linear(last,h))\n",
    "            if i < len(enc_bn_flags) and enc_bn_flags[i]:\n",
    "                enc_layers.append(nn.BatchNorm1d(h))\n",
    "            enc_layers.append(nn.ReLU())\n",
    "            if dropout_p and dropout_p>0:\n",
    "                enc_layers.append(nn.Dropout(dropout_p))\n",
    "            last=h\n",
    "        self.encoder=nn.Sequential(*enc_layers)\n",
    "        dec_layers=[]; last=enc_outs[-1]\n",
    "        for i,h in enumerate(dec_outs):\n",
    "            dec_layers.append(nn.Linear(last,h))\n",
    "            if i < len(dec_outs)-1:\n",
    "                if i < len(dec_bn_flags) and dec_bn_flags[i]:\n",
    "                    dec_layers.append(nn.BatchNorm1d(h))\n",
    "                dec_layers.append(nn.ReLU())\n",
    "                if dropout_p and dropout_p>0:\n",
    "                    dec_layers.append(nn.Dropout(dropout_p))\n",
    "            last=h\n",
    "        self.decoder=nn.Sequential(*dec_layers)\n",
    "    def forward(self,x): return self.decoder(self.encoder(x))\n",
    "\n",
    "INPUT_DIM   = int(cfg[\"input_dim\"])\n",
    "HIDDEN_LIST = list(cfg[\"hidden_list\"])\n",
    "BOTTLENECK  = int(cfg[\"bottleneck\"])\n",
    "DROPOUT_P   = float(cfg.get(\"dropout_p\", 0.0))\n",
    "enc_outs    = list(cfg.get(\"enc_outs\", HIDDEN_LIST + [BOTTLENECK]))\n",
    "dec_outs    = list(cfg.get(\"dec_outs\", HIDDEN_LIST[::-1] + [INPUT_DIM]))\n",
    "enc_bn      = list(cfg.get(\"enc_bn_flags\", [False]*len(enc_outs)))\n",
    "dec_bn      = list(cfg.get(\"dec_bn_flags\", [False]*len(dec_outs)))\n",
    "\n",
    "# --- Normalização do DEVICE (robusto) ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Modelo será carregado no DEVICE: {DEVICE}\")\n",
    "\n",
    "# carrega state_dict e modelo no device normalizado\n",
    "state_dict = torch.load(ae_pt, map_location=DEVICE)\n",
    "if isinstance(state_dict, dict) and \"state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "model = AE_BN_Strict(INPUT_DIM, enc_outs, enc_bn, dec_outs, dec_bn, DROPOUT_P).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "model.eval()\n",
    "print(f\"Modelo carregado (STRICT) | input_dim={INPUT_DIM}, enc_outs={enc_outs}, dec_outs={dec_outs}, device={DEVICE}\")\n",
    "\n",
    "# autocast condicional\n",
    "amp_ctx = torch.cuda.amp.autocast if DEVICE.type == \"cuda\" else nullcontext\n",
    "use_amp = (DEVICE.type == \"cuda\")\n",
    "\n",
    "# ---------------- escolher CSV de entrada (interativo) ----------------\n",
    "def _list_csvs(folder: Path, max_items: int = 20):\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    # ordena por mtime (mais recente primeiro)\n",
    "    csvs = sorted(folder.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return csvs[:max_items]\n",
    "\n",
    "# 1) Prioridade: variável global SCORE_CSV (se definida e existir)\n",
    "SCORE_CSV = Path(globals().get(\"SCORE_CSV\", \"\")) if \"SCORE_CSV\" in globals() else None\n",
    "if SCORE_CSV and SCORE_CSV.exists():\n",
    "    print(f\"Usando SCORE_CSV definido: {SCORE_CSV}\")\n",
    "else:\n",
    "    # 2) Menu interativo em prerun/\n",
    "    candidates = _list_csvs(PRERUN_DIR, max_items=50)\n",
    "    if not candidates:\n",
    "        # Sem arquivos em prerun/: peça um caminho manual\n",
    "        while True:\n",
    "            user_path = _prompt_input(\"Informe o caminho completo de um CSV para pontuar: \").strip()\n",
    "            SCORE_CSV = Path(user_path)\n",
    "            if SCORE_CSV.exists() and SCORE_CSV.suffix.lower()==\".csv\":\n",
    "                break\n",
    "            print(\"Caminho inválido. Tente novamente.\")\n",
    "    else:\n",
    "        print(\"CSVs disponíveis em prerun/ (mais recentes primeiro):\")\n",
    "        for i, p in enumerate(candidates, 1):\n",
    "            print(f\"  {i:02d}) {p.name}  |  {datetime.fromtimestamp(p.stat().st_mtime).isoformat()}  |  {p.stat().st_size} bytes\")\n",
    "        print(\"  0) Digitar um caminho completo (fora de prerun/)\")\n",
    "        sel = _prompt_input(\"Selecione o índice (vazio = 1): \", default=\"1\")\n",
    "        try:\n",
    "            idx = int(sel)\n",
    "        except Exception:\n",
    "            idx = 1\n",
    "        if idx == 0:\n",
    "            while True:\n",
    "                user_path = _prompt_input(\"Informe o caminho completo do CSV: \").strip()\n",
    "                SCORE_CSV = Path(user_path)\n",
    "                if SCORE_CSV.exists() and SCORE_CSV.suffix.lower()==\".csv\":\n",
    "                    break\n",
    "                print(\"Caminho inválido. Tente novamente.\")\n",
    "        else:\n",
    "            if idx < 1 or idx > len(candidates):\n",
    "                idx = 1\n",
    "            SCORE_CSV = candidates[idx-1]\n",
    "\n",
    "print(f\"Insumo selecionado: {SCORE_CSV.name}\")\n",
    "\n",
    "# ---------------- carregar dados e gerar features ----------------\n",
    "df = pd.read_csv(SCORE_CSV)\n",
    "print(f\"CSV carregado: shape={df.shape}\")\n",
    "\n",
    "categorical_maps = None\n",
    "if cat_maps_path.exists():\n",
    "    try:\n",
    "        with open(cat_maps_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            categorical_maps = json.load(f)\n",
    "        print(\"Mapas categóricos carregados da versão.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Aviso: falha ao carregar categorical_maps.json ({e}). Seguindo sem.\")\n",
    "\n",
    "# Engenharia e codificação\n",
    "if \"build_features\" not in globals() or not callable(globals()[\"build_features\"]):\n",
    "    raise RuntimeError(\"Função build_features não encontrada (importe do snapshot da versão).\")\n",
    "df_feat = build_features(df)  # mantém exatidão da Etapa 5\n",
    "del df; gc.collect()\n",
    "\n",
    "if categorical_maps is not None and \"encode_categoricals\" in globals() and callable(globals()[\"encode_categoricals\"]):\n",
    "    df_feat = encode_categoricals(df_feat, categorical_maps)\n",
    "\n",
    "missing = [c for c in feature_cols if c not in df_feat.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltam colunas de features no insumo processado: {missing[:10]}{'...' if len(missing)>10 else ''}\")\n",
    "\n",
    "# Seleciona apenas as features e libera memória do restante\n",
    "X_df = df_feat.loc[:, feature_cols]\n",
    "# libera df_feat cedo\n",
    "drop_cols = [c for c in df_feat.columns if c not in feature_cols]\n",
    "if drop_cols:\n",
    "    df_feat.drop(columns=drop_cols, inplace=True)\n",
    "del df_feat, drop_cols; gc.collect()\n",
    "\n",
    "# ---------------- pipeline de transformação + inferência em FATIAS ----------------\n",
    "N = X_df.shape[0]\n",
    "BATCH_ROWS = int(globals().get(\"INFER_BATCH_ROWS\", 200_000))  # ajuste fino conforme RAM disponível\n",
    "print(f\"Processando em fatias de até {BATCH_ROWS} linhas (N={N}).\")\n",
    "\n",
    "# Saídas\n",
    "scores_csv_path = RUN_DIR / \"scores.csv\"\n",
    "# prepara CSV (header)\n",
    "pd.DataFrame({\"score\": [], \"recon_error\": []}).to_csv(scores_csv_path, index=False)\n",
    "\n",
    "# memmap para reconstrução (poupa RAM)\n",
    "errs_path = RUN_DIR / \"reconstruction_errors_score.npy\"\n",
    "errs_mm = np.memmap(errs_path, dtype=\"float32\", mode=\"w+\", shape=(N,))\n",
    "\n",
    "# PSI/KS — prepara bins com base na validação (se existir)\n",
    "have_val = val_err_path.exists()\n",
    "if have_val:\n",
    "    val_err = np.load(val_err_path)\n",
    "    q = np.quantile(val_err, np.linspace(0, 1, 11))\n",
    "    q[0], q[-1] = -np.inf, np.inf\n",
    "    exec_hist = np.zeros(len(q)-1, dtype=np.int64)\n",
    "    val_hist, _ = np.histogram(val_err, bins=q)\n",
    "    val_p = np.clip(val_hist / max(1, val_hist.sum()), 1e-8, 1.0)\n",
    "else:\n",
    "    q = None\n",
    "    exec_hist = None\n",
    "    val_p = None\n",
    "\n",
    "# estatísticas correntes (Welford)\n",
    "count = 0\n",
    "mean = 0.0\n",
    "M2 = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, N, BATCH_ROWS):\n",
    "        end = min(start + BATCH_ROWS, N)\n",
    "        X_chunk = X_df.iloc[start:end]  # DataFrame view\n",
    "\n",
    "        # dtypes (evita cópias desnecessárias)\n",
    "        if isinstance(dtype_map, dict):\n",
    "            for col, dt in dtype_map.items():\n",
    "                if col in X_chunk.columns:\n",
    "                    try:\n",
    "                        X_chunk[col] = X_chunk[col].astype(dt, copy=False)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Aviso: dtype {dt} em {col} falhou: {e}\")\n",
    "\n",
    "        # numpy view sem copiar\n",
    "        X_np = X_chunk.to_numpy(copy=False)\n",
    "\n",
    "        # transforma\n",
    "        X_imp = imputer.transform(X_np)\n",
    "        X_scl = scaler.transform(X_imp).astype(\"float32\", copy=False)\n",
    "\n",
    "        xb = torch.from_numpy(X_scl).to(DEVICE)\n",
    "\n",
    "        # forward (AMP se CUDA)\n",
    "        if use_amp:\n",
    "            with amp_ctx(dtype=torch.float16):\n",
    "                xr = model(xb)\n",
    "        else:\n",
    "            xr = model(xb)\n",
    "\n",
    "        err_chunk = torch.mean((xr - xb)**2, dim=1).cpu().numpy().astype(\"float32\")\n",
    "\n",
    "        # escreve no memmap e no CSV (append)\n",
    "        errs_mm[start:end] = err_chunk\n",
    "        pd.DataFrame({\"score\": err_chunk, \"recon_error\": err_chunk}).to_csv(\n",
    "            scores_csv_path, mode=\"a\", header=False, index=False\n",
    "        )\n",
    "\n",
    "        # atualiza estatísticas online (Welford)\n",
    "        k = err_chunk.size\n",
    "        count_new = count + k\n",
    "        delta = err_chunk.mean() - mean\n",
    "        mean += delta * (k / max(1, count_new))\n",
    "        M2 += (err_chunk.var() * k) + (delta**2) * (count * k / max(1, count_new))\n",
    "        count = count_new\n",
    "\n",
    "        # PSI/KS por histograma\n",
    "        if have_val:\n",
    "            h, _ = np.histogram(err_chunk, bins=q)\n",
    "            exec_hist += h\n",
    "\n",
    "        # libera tudo da fatia\n",
    "        del X_chunk, X_np, X_imp, X_scl, xb, xr, err_chunk\n",
    "        gc.collect()\n",
    "\n",
    "# garante flush do memmap\n",
    "del errs_mm\n",
    "gc.collect()\n",
    "\n",
    "# ---------------- estatísticas & metadados ----------------\n",
    "std = float(np.sqrt(M2 / max(1, (count - 1)))) if count > 1 else 0.0\n",
    "stats = {\n",
    "    \"count\": int(count),\n",
    "    \"mean\": float(mean) if count else None,\n",
    "    \"std\": std if count else None,\n",
    "    \"version_dir\": str(ver_dir),\n",
    "    \"source\": {\n",
    "        \"file_name\": SCORE_CSV.name,\n",
    "        \"file_path\": str(SCORE_CSV.resolve()),\n",
    "        \"mtime_iso\": datetime.fromtimestamp(SCORE_CSV.stat().st_mtime).isoformat(),\n",
    "        \"n_rows\": int(N), \"n_cols\": int(len(feature_cols))\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"n_features\": int(len(feature_cols)),\n",
    "        \"features_hash\": features_hash,\n",
    "        \"input_dim_cfg\": int(cfg.get(\"input_dim\", len(feature_cols))),\n",
    "    },\n",
    "}\n",
    "(RUN_DIR / \"score_stats.json\").write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ---------------- PSI/KS a partir de histogramas (low-RAM) ----------------\n",
    "exec_stats = {}\n",
    "if have_val:\n",
    "    exec_p = np.clip(exec_hist / max(1, exec_hist.sum()), 1e-8, 1.0)\n",
    "    psi = float(np.sum((exec_p - val_p) * np.log(exec_p / val_p)))\n",
    "    # KS aprox.: maior diferença entre CDFs por bin\n",
    "    cdf_val  = np.cumsum(val_p)\n",
    "    cdf_exec = np.cumsum(exec_p)\n",
    "    ks = float(np.max(np.abs(cdf_exec - cdf_val)))\n",
    "    exec_stats = {\"psi\": psi, \"ks_approx\": ks, \"val_count\": int(val_err.size), \"exec_count\": int(count)}\n",
    "\n",
    "    # figura (corrigida: garante mesmo comprimento entre x e y)\n",
    "    # internal bins = excluir (-inf, q[1]) e (q[-2], +inf) => 8 centros quando q tem 11 bordas\n",
    "    centers = 0.5 * (q[1:-2] + q[2:-1])          # len = (len(q)-3)\n",
    "    val_plot = val_p[1:-1]                        # remove extremos => len = (len(q)-3)\n",
    "    exec_plot = exec_p[1:-1]\n",
    "\n",
    "    m = min(len(centers), len(val_plot), len(exec_plot))\n",
    "    if m >= 2:\n",
    "        centers = centers[:m]\n",
    "        val_plot = val_plot[:m]\n",
    "        exec_plot = exec_plot[:m]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.step(centers, val_plot, where=\"mid\", label=\"val\")\n",
    "        plt.step(centers, exec_plot, where=\"mid\", label=\"exec\")\n",
    "        plt.legend(); plt.title(\"Distribuição (bins quantílicos) — val vs exec\")\n",
    "        plt.savefig(FIGURES_DIR / \"dist_exec_vs_train.png\", dpi=120, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Aviso: não foi possível plotar distribuição (bins insuficientes).\")\n",
    "\n",
    "(RUN_DIR / \"exec_stats.json\").write_text(json.dumps(exec_stats, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ---------------- limpeza do snapshot local do insumo (opcional) ----------------\n",
    "snapshot_csv = Path(globals().get(\"SNAPSHOT_CSV\", \"\")) if \"SNAPSHOT_CSV\" in globals() else None\n",
    "try:\n",
    "    if snapshot_csv and snapshot_csv.exists():\n",
    "        snapshot_csv.unlink()\n",
    "        print(f\"Snapshot removido: {snapshot_csv.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Aviso: falha ao remover snapshot ({e})\")\n",
    "\n",
    "print(\"Concluído (modo low-RAM).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1761005280668,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "ASFP5YUUE9U_",
    "outputId": "a33085f0-3123-45d3-fb59-ad201e1e4d26"
   },
   "outputs": [],
   "source": [
    "print(f\"DEVICE={DEVICE} | type={getattr(DEVICE,'type', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUa_QRtbiuNg"
   },
   "source": [
    "# **Etapa 9:** Calibração de threshold (budget | meta | costmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25050,
     "status": "ok",
     "timestamp": 1761005377534,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "H339UXDG408z",
    "outputId": "e02c961c-f0db-4c0e-c6e6-3cdd50e7a2ac"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Definir o threshold do score (erro de reconstrução) que será usado para marcar alertas:\n",
    "- Modo 'budget' : taxa de alerta alvo (quantil sobre a distribuição de validação)\n",
    "- Modo 'meta'   : número absoluto de alertas no LOTE ATUAL (usa distribuição atual)\n",
    "- Modo 'costmin': minimiza custo proxy com (c_fp, c_fn, prevalência esperada p)\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- RUN_DIR/reconstruction_errors_val.npy  (Etapa 7)\n",
    "- RUN_DIR/scores.csv                     (Etapa 8)  -> coluna 'score'\n",
    "\n",
    "Saídas\n",
    "------\n",
    "- RUN_DIR/threshold.json                 (modo, parâmetros, threshold, KS/PSI, taxas)\n",
    "- RUN_DIR/figures/drift_hist.png         (histograma val vs atual)   [NOVO]\n",
    "- RUN_DIR/scores_summary.json            (n_alerts, alert_rate, threshold, modo)   [NOVO]\n",
    "- Impressão de resumo para conferência\n",
    "\n",
    "Notas\n",
    "-----\n",
    "- KS e PSI são calculados entre a distribuição de VALIDAÇÃO (baseline) e o LOTE ATUAL.\n",
    "- Para 'budget', o threshold é o quantil de validação: q = 1 - ALERT_RATE.\n",
    "- Para 'meta', o threshold é o quantil do lote atual tal que N_alertas = alvo.\n",
    "- Para 'costmin' (proxy, sem rótulos): busca thresholds por quantis do lote atual\n",
    "  minimizando: C = c_fp * rate_alerts + c_fn * max(0, p - rate_alerts).\n",
    "\"\"\"\n",
    "\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert 'RUN_DIR' in globals(), \"Execute a Etapa 1 para definir RUN_DIR.\"\n",
    "val_err_path = Path(RUN_DIR) / \"reconstruction_errors_val.npy\"\n",
    "scores_path  = Path(RUN_DIR) / \"scores.csv\"\n",
    "assert val_err_path.exists(), \"Arquivo de validação ausente (reconstruction_errors_val.npy). Rode as Etapas 6–7.\"\n",
    "assert scores_path.exists(),  \"scores.csv ausente. Rode a Etapa 8.\"\n",
    "\n",
    "# ---------- carregar dados ----------\n",
    "val_err = np.load(val_err_path)          # distribuição baseline (validação)\n",
    "df_sc   = pd.read_csv(scores_path, sep=\",\", encoding=\"utf-8-sig\")\n",
    "assert \"score\" in df_sc.columns, \"scores.csv não possui coluna 'score'.\"\n",
    "scores  = df_sc[\"score\"].to_numpy(dtype=float)\n",
    "\n",
    "# ---------- métricas de drift (KS e PSI) ----------\n",
    "def ks_stat(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a[~np.isnan(a)]\n",
    "    b = b[~np.isnan(b)]\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return float(\"nan\")\n",
    "    xa = np.sort(a)\n",
    "    xb = np.sort(b)\n",
    "    grid = np.unique(np.concatenate([xa, xb], axis=0))\n",
    "    def _ecdf(x, g):\n",
    "        return np.searchsorted(x, g, side=\"right\") / x.size\n",
    "    Fa = np.array([_ecdf(xa, g) for g in grid], dtype=float)\n",
    "    Fb = np.array([_ecdf(xb, g) for g in grid], dtype=float)\n",
    "    return float(np.max(np.abs(Fa - Fb)))\n",
    "\n",
    "def psi_stat(expected: np.ndarray, actual: np.ndarray, bins: int = 20) -> float:\n",
    "    e = expected[~np.isnan(expected)]\n",
    "    a = actual[~np.isnan(actual)]\n",
    "    if e.size == 0 or a.size == 0:\n",
    "        return float(\"nan\")\n",
    "    qs = np.linspace(0, 1, bins + 1)\n",
    "    cuts = np.quantile(e, qs)\n",
    "    cuts = np.unique(cuts)\n",
    "    if cuts.size < 3:\n",
    "        return 0.0\n",
    "    e_hist, _ = np.histogram(e, bins=cuts)\n",
    "    a_hist, _ = np.histogram(a, bins=cuts)\n",
    "    e_prop = np.clip(e_hist / max(e_hist.sum(), 1), 1e-8, 1.0)\n",
    "    a_prop = np.clip(a_hist / max(a_hist.sum(), 1), 1e-8, 1.0)\n",
    "    psi = np.sum((a_prop - e_prop) * np.log(a_prop / e_prop))\n",
    "    return float(psi)\n",
    "\n",
    "KS  = ks_stat(val_err, scores)\n",
    "PSI = psi_stat(val_err, scores, bins=20)\n",
    "\n",
    "print(f\"Skynet: KS(val vs atual) = {KS:.4f} | PSI = {PSI:.4f}\")\n",
    "\n",
    "# ---------- modos de calibração ----------\n",
    "def calib_budget(val_err: np.ndarray, alert_rate: float) -> dict:\n",
    "    alert_rate = float(alert_rate)\n",
    "    alert_rate = min(max(alert_rate, 1e-6), 0.99)\n",
    "    thr = float(np.quantile(val_err, 1.0 - alert_rate))\n",
    "    rate_val = float((val_err >= thr).mean())\n",
    "    return {\"threshold\": thr, \"rate_val_expected\": rate_val, \"params\": {\"mode\":\"budget\",\"alert_rate\": alert_rate}}\n",
    "\n",
    "def calib_meta_current(scores: np.ndarray, n_alerts: int) -> dict:\n",
    "    n_alerts = int(max(0, n_alerts))\n",
    "    n = scores.size\n",
    "    if n_alerts <= 0:\n",
    "        thr = float(np.inf)\n",
    "    elif n_alerts >= n:\n",
    "        thr = float(-np.inf)\n",
    "    else:\n",
    "        s = np.sort(scores)[::-1]\n",
    "        thr = float(s[n_alerts - 1])\n",
    "    rate_curr = float((scores >= thr).mean())\n",
    "    return {\"threshold\": thr, \"rate_current\": rate_curr, \"params\": {\"mode\":\"meta\",\"n_alerts\": n_alerts, \"n_rows\": n}}\n",
    "\n",
    "def calib_costmin_proxy(scores: np.ndarray, c_fp: float, c_fn: float, prevalence: float) -> dict:\n",
    "    c_fp = float(max(c_fp, 0.0))\n",
    "    c_fn = float(max(c_fn, 0.0))\n",
    "    p = float(min(max(prevalence, 0.0), 1.0))\n",
    "    if scores.size == 0:\n",
    "        return {\"threshold\": float(\"nan\"), \"rate_current\": float(\"nan\"),\n",
    "                \"params\": {\"mode\":\"costmin\",\"c_fp\": c_fp, \"c_fn\": c_fn, \"prevalence\": p}}\n",
    "    qs = np.linspace(0.0, 1.0, 1001)\n",
    "    thrs = np.quantile(scores, 1.0 - qs)\n",
    "    s_sorted = np.sort(scores)\n",
    "    n = s_sorted.size\n",
    "    best = None; best_thr = None; best_rate = None\n",
    "    for thr in thrs:\n",
    "        idx = np.searchsorted(s_sorted, thr, side=\"left\")\n",
    "        rate = (n - idx) / n\n",
    "        cost = c_fp * rate + c_fn * max(0.0, p - rate)\n",
    "        if (best is None) or (cost < best):\n",
    "            best, best_thr, best_rate = cost, float(thr), float(rate)\n",
    "    return {\"threshold\": best_thr, \"rate_current\": best_rate,\n",
    "            \"params\": {\"mode\":\"costmin\",\"c_fp\": c_fp, \"c_fn\": c_fn, \"prevalence\": p, \"grid\": \"q=0..1 step 0.001\"}}\n",
    "\n",
    "# ---------- interação com o usuário ----------\n",
    "print(\"\\nSelecione o MODO de threshold:\")\n",
    "print(\"  [1] budget  — Threshold por taxa de alerta alvo (ALERT_RATE, ex.: 0.03)\")\n",
    "print(\"  [2] meta    — Threshold por número de alertas desejado no lote atual (N_ALERTS)\")\n",
    "print(\"  [3] costmin — Threshold por custo proxy mínimo (c_fp, c_fn, prevalência p)\")\n",
    "mode_raw = input(\"Digite o índice do modo [1-3]: \").strip()\n",
    "\n",
    "if mode_raw == \"1\":\n",
    "    a_raw = input(\"ALERT_RATE (fração, ex.: 0.03) [default=0.03]: \").strip()\n",
    "    ALERT_RATE = float(a_raw) if a_raw else 0.03\n",
    "    result = calib_budget(val_err, ALERT_RATE)\n",
    "    MODE = \"budget\"\n",
    "\n",
    "elif mode_raw == \"2\":\n",
    "    default_n = max(1, int(0.02 * scores.size))\n",
    "    n_raw = input(f\"N_ALERTS (inteiro, 0..{scores.size}) [default={default_n}]: \").strip()\n",
    "    N_ALERTS = int(n_raw) if n_raw else default_n\n",
    "    result = calib_meta_current(scores, N_ALERTS)\n",
    "    MODE = \"meta\"\n",
    "\n",
    "elif mode_raw == \"3\":\n",
    "    cfp_raw = input(\"c_fp (custo do falso positivo) [default=1.0]: \").strip()\n",
    "    cfn_raw = input(\"c_fn (custo do falso negativo) [default=5.0]: \").strip()\n",
    "    p_raw   = input(\"prevalência esperada p (0..1) [default=0.01]: \").strip()\n",
    "    c_fp = float(cfp_raw) if cfp_raw else 1.0\n",
    "    c_fn = float(cfn_raw) if cfn_raw else 5.0\n",
    "    p    = float(p_raw)   if p_raw   else 0.01\n",
    "    result = calib_costmin_proxy(scores, c_fp, c_fn, p)\n",
    "    MODE = \"costmin\"\n",
    "\n",
    "else:\n",
    "    print(\"Entrada inválida. Usando modo [1] budget com ALERT_RATE=0.03 por padrão.\")\n",
    "    ALERT_RATE = 0.03\n",
    "    result = calib_budget(val_err, ALERT_RATE)\n",
    "    MODE = \"budget\"\n",
    "\n",
    "THRESHOLD = float(result[\"threshold\"])\n",
    "\n",
    "# taxas estimadas (validação e lote atual)\n",
    "rate_val     = float((val_err >= THRESHOLD).mean())\n",
    "rate_current = float((scores  >= THRESHOLD).mean())\n",
    "\n",
    "# ---------- salvar threshold.json (mantendo compatibilidade) ----------\n",
    "summary = {\n",
    "    \"mode\": MODE,\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"ks_val_vs_current\": KS,\n",
    "    \"psi_val_vs_current\": PSI,\n",
    "    \"rate_val_expected\": rate_val,\n",
    "    \"rate_current_estimated\": rate_current,\n",
    "    \"mode_config\": result.get(\"params\", {}),   # <— ajuda a Etapa 12 a descrever o método\n",
    "    \"drift\": {\"ks_stat\": KS, \"psi\": PSI},      # <— bloco amigável para leitura posterior\n",
    "    \"inputs\": {\n",
    "        \"val_err_path\": str(val_err_path),\n",
    "        \"scores_path\": str(scores_path),\n",
    "        \"n_val\": int(val_err.shape[0]),\n",
    "        \"n_current\": int(scores.shape[0]),\n",
    "    },\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "out_path = Path(RUN_DIR) / \"threshold.json\"\n",
    "out_path.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"threshold.json salvo em: {out_path.name}\")\n",
    "\n",
    "# ---------- salvar figura de comparação das distribuições (drift_hist.png) ----------\n",
    "fig_dir = Path(RUN_DIR) / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.figure(figsize=(8,4.6))\n",
    "plt.hist(val_err, bins=50, alpha=0.5, label=\"Validação\", density=True)\n",
    "plt.hist(scores,  bins=50, alpha=0.5, label=\"Lote atual\", density=True)\n",
    "plt.axvline(THRESHOLD, color=\"k\", linestyle=\"--\", linewidth=1.2, label=f\"Threshold = {THRESHOLD:.5f}\")\n",
    "plt.title(\"Comparação de distribuições de erro (validação vs lote atual)\")\n",
    "plt.xlabel(\"Erro de reconstrução\")\n",
    "plt.ylabel(\"Densidade\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "drift_png = fig_dir / \"drift_hist.png\"\n",
    "plt.savefig(drift_png, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"Figura salva: {drift_png.name}\")\n",
    "\n",
    "# ---------- salvar scores_summary.json (não altera scores.csv; Etapa 10 materializa 'alert') ----------\n",
    "n_alerts = int((scores >= THRESHOLD).sum())\n",
    "scores_summary = {\n",
    "    \"n_linhas\": int(scores.shape[0]),\n",
    "    \"n_alerts\": n_alerts,\n",
    "    \"alert_rate\": float(n_alerts / max(scores.shape[0], 1)),\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"mode\": MODE,\n",
    "    \"mode_config\": result.get(\"params\", {}),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "(Path(RUN_DIR) / \"scores_summary.json\").write_text(json.dumps(scores_summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"scores_summary.json salvo.\")\n",
    "\n",
    "print(\"\\nCalibração concluída.\")\n",
    "print(f\"threshold = {THRESHOLD:.6f}\")\n",
    "print(f\"taxas: val={rate_val:.4%}  |  atual={rate_current:.4%}\")\n",
    "print(f\"KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "print(\"Próximo passo: Etapa 10 — materializar alerts no scores.csv usando este threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc3Pu1kRjWyt"
   },
   "source": [
    "# **Etapa 10:** Marcação dos alertas de anomalia nos registros e geração do arquivo de output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11984,
     "status": "ok",
     "timestamp": 1761006561178,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "ruevdUuIKiis",
    "outputId": "c627961b-feed-419a-ca54-2f283eaf0f42"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "- Ler os artefatos de inferência da §8 (scores + threshold) a partir de RUN_DIR.\n",
    "- Descobrir o CSV de origem usado na §8 e copiá-lo para output/ com timestamp.\n",
    "- Se não encontrar automaticamente, perguntar ao usuário e listar os CSVs em PROJ_ROOT/prerun/.\n",
    "- Anexar colunas: anom_score, rank_desc (ordem decrescente de score) e alert (0/1).\n",
    "- Garantir a existência da coluna 'username' no CSV de saída.\n",
    "- Salvar CSV final em PROJ_ROOT/output.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(\"Skynet Informa: Marcando score de anomalia nos registros originais.\")\n",
    "\n",
    "# -------------------- Diretórios esperados no ambiente ----------------------\n",
    "assert 'PROJ_ROOT' in globals(), \"Defina PROJ_ROOT (Path) no ambiente.\"\n",
    "assert 'RUN_DIR'   in globals(), \"Defina RUN_DIR (Path) no ambiente.\"\n",
    "PROJ_ROOT = Path(PROJ_ROOT)\n",
    "RUN_DIR   = Path(RUN_DIR)\n",
    "PRERUN_DIR = PROJ_ROOT / \"prerun\"\n",
    "OUTPUT_DIR = PROJ_ROOT / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- (Opcional) Forçar caminho do CSV de origem -----------\n",
    "SOURCE_CSV_OVERRIDE = None  # exemplo: Path(PROJ_ROOT / \"prerun\" / \"meu_dataset_validado.csv\")\n",
    "\n",
    "# -------------------- Localização de threshold e scores ---------------------\n",
    "thr_path = RUN_DIR / \"threshold.json\"\n",
    "scores_parquet = RUN_DIR / \"scores.parquet\"\n",
    "scores_csv     = RUN_DIR / \"scores.csv\"\n",
    "assert thr_path.exists(), f\"threshold.json não encontrado em {thr_path}\"\n",
    "\n",
    "if scores_parquet.exists():\n",
    "    scores_path = scores_parquet\n",
    "elif scores_csv.exists():\n",
    "    scores_path = scores_csv\n",
    "else:\n",
    "    raise FileNotFoundError(\"Arquivo de scores não encontrado em RUN_DIR (scores.parquet ou scores.csv).\")\n",
    "\n",
    "print(f\"Usando scores: {scores_path.name} | threshold: {thr_path.name}\")\n",
    "\n",
    "# -------------------- Ler threshold ----------------------------------------\n",
    "with open(thr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    thr_obj = json.load(f)\n",
    "thr_candidates = [\"threshold\", \"thr\", \"cutoff\", \"score_threshold\"]\n",
    "thr_value = None\n",
    "for k in thr_candidates:\n",
    "    if k in thr_obj:\n",
    "        thr_value = float(thr_obj[k])\n",
    "        break\n",
    "assert thr_value is not None, f\"threshold.json não contém nenhuma das chaves esperadas: {thr_candidates}\"\n",
    "print(f\"Threshold carregado: {thr_value:.6f}\")\n",
    "\n",
    "# -------------------- Ler scores -------------------------------------------\n",
    "if scores_path.suffix.lower() == \".parquet\":\n",
    "    df_scores = pd.read_parquet(scores_path)\n",
    "else:\n",
    "    df_scores = pd.read_csv(scores_path)\n",
    "\n",
    "# Normalizar nome da coluna de score\n",
    "score_col_candidates = [\"score\", \"anom_score\", \"recon_error\", \"reconstruction_error\"]\n",
    "score_col = None\n",
    "for c in score_col_candidates:\n",
    "    if c in df_scores.columns:\n",
    "        score_col = c\n",
    "        break\n",
    "assert score_col is not None, f\"Coluna de score não encontrada. Esperado uma de: {score_col_candidates}\"\n",
    "if score_col != \"anom_score\":\n",
    "    df_scores = df_scores.rename(columns={score_col: \"anom_score\"})\n",
    "\n",
    "# Detectar coluna de chave/índice (se existir)\n",
    "row_id_col = None\n",
    "for c in [\"row_id\", \"row_idx\", \"index_original\", \"_row_id\"]:\n",
    "    if c in df_scores.columns:\n",
    "        row_id_col = c\n",
    "        break\n",
    "\n",
    "# Se não houver row_id, usa a posição (0..n-1)\n",
    "if row_id_col is None:\n",
    "    df_scores = df_scores.reset_index(drop=False).rename(columns={\"index\": \"row_id\"})\n",
    "    row_id_col = \"row_id\"\n",
    "\n",
    "# Garantir tipos e ordenação\n",
    "df_scores[\"row_id\"] = pd.to_numeric(df_scores[row_id_col], errors=\"coerce\").astype(\"Int64\")\n",
    "df_scores = df_scores.sort_values([\"row_id\"]).reset_index(drop=True)\n",
    "\n",
    "# -------------------- Calcular rank_desc e alert ---------------------------\n",
    "df_scores[\"rank_desc\"] = df_scores[\"anom_score\"].rank(method=\"first\", ascending=False).astype(int)\n",
    "df_scores[\"alert\"] = (df_scores[\"anom_score\"] >= thr_value).astype(int)\n",
    "df_scores_short = df_scores[[\"row_id\", \"anom_score\", \"rank_desc\", \"alert\"]]\n",
    "\n",
    "# -------------------- Helpers: detectar/selecionar CSV da §8 ----------------\n",
    "def _probe_source_csv_from_meta(run_dir):\n",
    "    # 1) inference_meta.json\n",
    "    cand = run_dir / \"inference_meta.json\"\n",
    "    if cand.exists():\n",
    "        try:\n",
    "            obj = json.loads(cand.read_text(encoding=\"utf-8\"))\n",
    "            for k in [\"source_csv\", \"input_csv\", \"csv_path\"]:\n",
    "                if k in obj and obj[k]:\n",
    "                    p = Path(obj[k])\n",
    "                    if p.exists():\n",
    "                        return p\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 2) run_meta.json\n",
    "    cand = run_dir / \"run_meta.json\"\n",
    "    if cand.exists():\n",
    "        try:\n",
    "            obj = json.loads(cand.read_text(encoding=\"utf-8\"))\n",
    "            for k in [\"source_csv\", \"input_csv\", \"csv_path\"]:\n",
    "                if k in obj and obj[k]:\n",
    "                    p = Path(obj[k])\n",
    "                    if p.exists():\n",
    "                        return p\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 3) source_csv.txt\n",
    "    cand = run_dir / \"source_csv.txt\"\n",
    "    if cand.exists():\n",
    "        try:\n",
    "            p = Path(cand.read_text(encoding=\"utf-8\").strip())\n",
    "            if p.exists():\n",
    "                return p\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _pick_file_from_prerun(prerun_dir):\n",
    "    assert prerun_dir.exists(), f\"Pasta não encontrada: {prerun_dir}\"\n",
    "    files = sorted([p for p in prerun_dir.glob(\"*.csv\") if p.is_file()])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Não há arquivos .csv em {prerun_dir}.\")\n",
    "    print(\"Selecione o CSV de origem (listado a partir de PROJ_ROOT/prerun):\")\n",
    "    for i, p in enumerate(files, start=1):\n",
    "        print(f\"  [{i}] {p.name}\")\n",
    "    print(\"Pressione ENTER para aceitar [1].\")\n",
    "    while True:\n",
    "        sel = input(\"Digite o índice do arquivo desejado: \").strip()\n",
    "        if sel == \"\":\n",
    "            idx = 1\n",
    "        else:\n",
    "            if not sel.isdigit():\n",
    "                print(\"Entrada inválida. Informe um número.\")\n",
    "                continue\n",
    "            idx = int(sel)\n",
    "        if 1 <= idx <= len(files):\n",
    "            choice = files[idx - 1]\n",
    "            print(f\"Arquivo selecionado: {choice}\")\n",
    "            return choice\n",
    "        else:\n",
    "            print(f\"Índice fora do intervalo (1..{len(files)}). Tente novamente.\")\n",
    "\n",
    "# -------------------- Determinar source_csv --------------------\n",
    "if SOURCE_CSV_OVERRIDE is not None:\n",
    "    source_csv = Path(SOURCE_CSV_OVERRIDE)\n",
    "else:\n",
    "    source_csv = _probe_source_csv_from_meta(RUN_DIR)\n",
    "\n",
    "if not source_csv or not source_csv.exists():\n",
    "    print(\"CSV de origem da §8 não encontrado automaticamente.\")\n",
    "    source_csv = _pick_file_from_prerun(PRERUN_DIR)\n",
    "\n",
    "print(f\"CSV de origem: {source_csv}\")\n",
    "\n",
    "# -------------------- Ler CSV de origem -------------------\n",
    "df_src = pd.read_csv(source_csv)\n",
    "df_src = df_src.reset_index(drop=True).reset_index(drop=False).rename(columns={\"index\": \"row_id\"})\n",
    "df_src[\"row_id\"] = pd.to_numeric(df_src[\"row_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# -------------------- Garantir coluna 'username' -------------------\n",
    "def _ensure_username_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mapa lower->original\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    # lista de candidatos por prioridade\n",
    "    candidates = [\n",
    "        \"username\", \"user\", \"usuario\", \"user_name\", \"usernm\", \"nm_usuario\",\n",
    "        \"login\", \"matricula\", \"id_usuario\", \"idusuario\", \"usr\", \"employee\",\n",
    "        \"employee_id\", \"user_id\"\n",
    "    ]\n",
    "    # busca direta por nome exato (case-insensitive)\n",
    "    for key in candidates:\n",
    "        if key in lower_map:\n",
    "            src_col = lower_map[key]\n",
    "            df[\"username\"] = df[src_col].astype(str)\n",
    "            print(f\"Coluna 'username' criada a partir de '{src_col}'.\")\n",
    "            return df\n",
    "    # busca por padrões comuns\n",
    "    rx = re.compile(r'\\b(user(name)?|usuario|login|matr(icula)?|id_?usuario|user_?id)\\b', flags=re.I)\n",
    "    for c in df.columns:\n",
    "        if rx.search(c):\n",
    "            df[\"username\"] = df[c].astype(str)\n",
    "            print(f\"Coluna 'username' criada a partir de '{c}' (padrão detectado).\")\n",
    "            return df\n",
    "    # fallback: cria coluna com \"NA\"\n",
    "    df[\"username\"] = \"NA\"\n",
    "    print(\"Aviso: nenhuma coluna equivalente a 'username' encontrada. Criada 'username' preenchida com 'NA'.\")\n",
    "    return df\n",
    "\n",
    "df_src = _ensure_username_column(df_src)\n",
    "\n",
    "# -------------------- Merge com scores -------------------\n",
    "if len(df_src) != len(df_scores_short):\n",
    "    print(f\"Atenção: tamanhos diferentes — origem={len(df_src)} vs scores={len(df_scores_short)}. \"\n",
    "          \"O merge será feito por 'row_id' (posição). Verifique consistência se necessário.\")\n",
    "\n",
    "df_out = df_src.merge(df_scores_short, on=\"row_id\", how=\"left\").drop(columns=[\"row_id\"])\n",
    "\n",
    "# -------------------- Salvar cópia em output/ com timestamp ----------------\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "out_name = f\"{source_csv.stem}_etapa10_{ts}.csv\"\n",
    "out_path = OUTPUT_DIR / out_name\n",
    "df_out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Etapa 10 concluída.\")\n",
    "print(f\"- CSV final: {out_path}\")\n",
    "print(\"- Colunas adicionadas: ['anom_score', 'rank_desc', 'alert']\")\n",
    "print(f\"- Registros: {len(df_out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1761005405462,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "1zdhONwc5La7",
    "outputId": "85320d52-aa2f-4bc7-f162-adb832a95b42"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "1) Ler RUN_DIR/scores.csv (Etapa 8) e RUN_DIR/threshold.json (Etapa 9).\n",
    "2) Aplicar threshold → coluna 'alert' (0/1).\n",
    "3) Gerar ranking estável por 'score' (desc) e metadados de auditoria.\n",
    "4) Salvar:\n",
    "   - runs/<RUN_ID>/scores_alerts.csv         (dataset completo com alert=0/1)\n",
    "   - runs/<RUN_ID>/scores_alerts_top1000.csv (amostra priorizada)\n",
    "   - runs/<RUN_ID>/alerts_summary.json       (sumário de métricas)\n",
    "   - runs/<RUN_ID>/alerts_by_username.csv    (agregação útil para triagem, se houver 'username')\n",
    "   - runs/<RUN_ID>/alerts_top100.csv         (TOP 100 alertas, p/ relatório Etapa 12)   [NOVO]\n",
    "   - runs/<RUN_ID>/scores_summary.json       (n_alerts/alert_rate/threshold/mode)  [NOVO]\n",
    "\n",
    "Pontos FIXOS:\n",
    "- Separador ';' e encoding 'utf-8-sig'\n",
    "- Ordenação estável (mergesort) por score desc para ranking\n",
    "- Colunas de metadados adicionadas: threshold_used, mode, ks, psi, rank_desc\n",
    "\n",
    "Pontos CALIBRÁVEIS:\n",
    "- TOP_K exportado (por padrão 1000)\n",
    "- Colunas de contexto extra no início do CSV final (ORDER_FRONT)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- Pré-checagens --------\n",
    "assert 'RUN_DIR' in globals(), \"Execute a Etapa 1 antes (RUN_DIR).\"\n",
    "run_dir = Path(RUN_DIR)\n",
    "scores_path = run_dir / \"scores.csv\"\n",
    "th_path     = run_dir / \"threshold.json\"\n",
    "assert scores_path.exists(),  \"scores.csv ausente (rode a Etapa 8).\"\n",
    "assert th_path.exists(),      \"threshold.json ausente (rode a Etapa 9).\"\n",
    "\n",
    "# -------- Carregar dados --------\n",
    "CSV_SEP, CSV_ENC = \",\", \"utf-8-sig\"\n",
    "df = pd.read_csv(scores_path, sep=CSV_SEP, encoding=CSV_ENC, dtype=str)\n",
    "\n",
    "# garante colunas base\n",
    "assert \"score\" in df.columns, \"scores.csv precisa ter coluna 'score'.\"\n",
    "# preserva uma cópia do índice original (útil p/ rastreio)\n",
    "df.insert(0, \"_rowid\", np.arange(len(df), dtype=np.int64))\n",
    "\n",
    "# coerção numérica do score\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "# -------- Ler threshold e metadados --------\n",
    "cfg = json.loads(th_path.read_text(encoding=\"utf-8\"))\n",
    "THRESHOLD = float(cfg[\"threshold\"])\n",
    "MODE      = cfg.get(\"mode\", \"?\")\n",
    "KS        = float(cfg.get(\"ks_val_vs_current\", np.nan))\n",
    "PSI       = float(cfg.get(\"psi_val_vs_current\", np.nan))\n",
    "\n",
    "# -------- Aplicar threshold → alert --------\n",
    "df[\"alert\"] = (df[\"score\"] >= THRESHOLD).astype(\"int8\")\n",
    "rate_current = float(df[\"alert\"].mean())\n",
    "\n",
    "# -------- Ordenação estável por score desc (ranking) --------\n",
    "# mergesort é estável → se empatar score, mantém a ordem original (_rowid)\n",
    "df_sorted = df.sort_values(by=[\"score\", \"_rowid\"], ascending=[False, True], kind=\"mergesort\").copy()\n",
    "df_sorted.insert(1, \"rank_desc\", np.arange(1, len(df_sorted) + 1, dtype=np.int64))\n",
    "\n",
    "# -------- Metadados úteis --------\n",
    "df_sorted.insert(2, \"threshold_used\", THRESHOLD)\n",
    "df_sorted.insert(3, \"mode\", MODE)\n",
    "df_sorted.insert(4, \"ks_val_vs_current\", KS)\n",
    "df_sorted.insert(5, \"psi_val_vs_current\", PSI)\n",
    "\n",
    "# -------- Reorganizar colunas (frente com contexto) --------\n",
    "ORDER_FRONT = [\n",
    "    \"alert\", \"rank_desc\", \"score\", \"threshold_used\", \"mode\",\n",
    "    \"ks_val_vs_current\", \"psi_val_vs_current\",\n",
    "    \"_rowid\",\n",
    "]\n",
    "cols_final = ORDER_FRONT + [c for c in df_sorted.columns if c not in ORDER_FRONT]\n",
    "df_final = df_sorted[cols_final].copy()\n",
    "\n",
    "# -------- Salvar saídas principais --------\n",
    "out_full = run_dir / \"scores_alerts.csv\"\n",
    "out_topk = run_dir / \"scores_alerts_top1000.csv\"\n",
    "TOP_K = 1000  # CALIBRÁVEL\n",
    "\n",
    "df_final.to_csv(out_full, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "df_final.head(TOP_K).to_csv(out_topk, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "\n",
    "# -------- Agregação por usuário (se existir coluna 'username') --------\n",
    "if \"username\" in df_final.columns:\n",
    "    by_user = (\n",
    "        df_final.groupby(\"username\", dropna=False)\n",
    "                .agg(alerts=(\"alert\", \"sum\"),\n",
    "                     total=(\"alert\", \"count\"),\n",
    "                     pct_alerts=(\"alert\", \"mean\"),\n",
    "                     max_score=(\"score\", \"max\"))\n",
    "                .reset_index()\n",
    "                .sort_values([\"alerts\",\"max_score\"], ascending=[False, False])\n",
    "    )\n",
    "    by_user[\"pct_alerts\"] = (by_user[\"pct_alerts\"] * 100).round(2)\n",
    "    out_by_user = run_dir / \"alerts_by_username.csv\"\n",
    "    by_user.to_csv(out_by_user, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "    print(f\"Salvo {out_by_user.name}\")\n",
    "else:\n",
    "    print(\"coluna 'username' ausente — pulando alerts_by_username.csv\")\n",
    "\n",
    "# -------- Top 100 alertas (p/ relatório Etapa 12) — NOVO --------\n",
    "top100 = df_final[df_final[\"alert\"] == 1].copy()\n",
    "top100 = top100.sort_values(by=[\"score\", \"_rowid\"], ascending=[False, True], kind=\"mergesort\").head(100)\n",
    "out_top100 = run_dir / \"alerts_top100.csv\"\n",
    "top100.to_csv(out_top100, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "print(f\"Salvo {out_top100.name}\")\n",
    "\n",
    "# -------- Sumários --------\n",
    "summary = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"mode\": MODE,\n",
    "    \"ks_val_vs_current\": KS,\n",
    "    \"psi_val_vs_current\": PSI,\n",
    "    \"n_rows\": int(len(df_final)),\n",
    "    \"n_alerts\": int(df_final[\"alert\"].sum()),\n",
    "    \"alert_rate\": float(rate_current),\n",
    "    \"top_k\": TOP_K,\n",
    "    \"outputs\": {\n",
    "        \"scores_alerts_csv\": str(out_full),\n",
    "        \"scores_alerts_topk_csv\": str(out_topk),\n",
    "        \"alerts_top100_csv\": str(out_top100),\n",
    "        \"alerts_by_username_csv\": str(run_dir / \"alerts_by_username.csv\"),\n",
    "    }\n",
    "}\n",
    "(run_dir / \"alerts_summary.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Skynet: salvo alerts_summary.json\")\n",
    "\n",
    "# -------- Atualizar/criar scores_summary.json (p/ Etapa 12) — NOVO --------\n",
    "scores_summary_path = run_dir / \"scores_summary.json\"\n",
    "scores_summary = {\n",
    "    \"n_linhas\": int(len(df_final)),\n",
    "    \"n_alerts\": int(df_final[\"alert\"].sum()),\n",
    "    \"alert_rate\": float(rate_current),\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"mode\": MODE,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "try:\n",
    "    # mantém 'mode_config' se já existir (do Etapa 9)\n",
    "    if scores_summary_path.exists():\n",
    "        existing = json.loads(scores_summary_path.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(existing, dict):\n",
    "            for k in (\"mode_config\",):\n",
    "                if k in existing and k not in scores_summary:\n",
    "                    scores_summary[k] = existing[k]\n",
    "    scores_summary_path.write_text(json.dumps(scores_summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Salvo {scores_summary_path.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Aviso - falha ao salvar scores_summary.json: {e}\")\n",
    "\n",
    "print(\"\\nALERTS materializados com sucesso.\")\n",
    "print(f\"threshold={THRESHOLD:.6f}  modo={MODE}  KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "print(f\"taxa de alertas no lote atual: {rate_current:.2%}\")\n",
    "print(f\"Salvo {out_full.name}, {out_topk.name}\")\n",
    "print(\"Próximo passo: Etapa 11 — monitoramento de drift (distribuição do score e do erro).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFQskoCtjs_K"
   },
   "source": [
    "# **Etapa 11:** Monitoramento de drifts\n",
    "\n",
    "---\n",
    "\n",
    "Score atual vs erro de validação.\n",
    "\n",
    "Séries diárias: exibe e salva figuras.\n",
    "\n",
    "KS - Kolmogorov-Smirnov\n",
    "\n",
    "PSI - Population Stability Index\n",
    "\n",
    "**Critério de retreino: PSI>0,25**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "executionInfo": {
     "elapsed": 7207,
     "status": "ok",
     "timestamp": 1761005796764,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "Ze7nMCJsH6BB",
    "outputId": "2ac67c6a-d84b-4079-de25-5b0e6c5a7208"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "1) Carregar:\n",
    "   - RUN_DIR/reconstruction_errors_val.npy  (baseline - Etapa 7)\n",
    "   - RUN_DIR/scores.csv                     (lote atual - Etapas 8/10)\n",
    "2) Calcular métricas de drift: KS e PSI\n",
    "3) Gerar, SALVAR e EXIBIR figuras:\n",
    "   - Histograma comparativo (baseline vs atual)\n",
    "   - CDF comparativa (ECDF baseline vs ECDF atual)\n",
    "   - Boxplot MENSAL (se existir 'data_lcto' + 'recon_error')\n",
    "4) Salvar artefatos:\n",
    "   - runs/<RUN_ID>/drift_metrics.json\n",
    "   - runs/<RUN_ID>/figures/drift_hist.png\n",
    "   - runs/<RUN_ID>/figures/drift_cdf.png\n",
    "   - runs/<RUN_ID>/figures/drift_monthly_box.png (se aplicável)\n",
    "   - runs/<RUN_ID>/drift_bins_psi.csv\n",
    "   - runs/<RUN_ID>/images_base64.json\n",
    "   - runs/<RUN_ID>/drift_monitoring.json   [compatível com Etapa 12]\n",
    "\"\"\"\n",
    "\n",
    "import json, io, base64\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display  # para exibir PNGs gerados\n",
    "\n",
    "# --- parâmetros gráficos ---\n",
    "NUM_BINS = 50\n",
    "FIG_DPI  = 140\n",
    "\n",
    "assert 'RUN_DIR' in globals(), \"Execute a Etapa 1 para definir RUN_DIR.\"\n",
    "run_dir = Path(RUN_DIR)\n",
    "fig_dir = run_dir / \"figures\"   # Etapa 12 espera figuras aqui\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "val_err_path = run_dir / \"reconstruction_errors_val.npy\"\n",
    "scores_path  = run_dir / \"scores.csv\"\n",
    "assert val_err_path.exists(), \"reconstruction_errors_val.npy ausente (rode Etapa 7).\"\n",
    "assert scores_path.exists(),  \"scores.csv ausente (rode Etapas 8–10).\"\n",
    "\n",
    "# ----------------- carregar dados -----------------\n",
    "val_err = np.load(val_err_path)                                  # baseline (val)\n",
    "df_sc   = pd.read_csv(scores_path, sep=\",\", encoding=\"utf-8-sig\")\n",
    "assert \"score\" in df_sc.columns, \"scores.csv precisa ter coluna 'score'.\"\n",
    "scores  = pd.to_numeric(df_sc[\"score\"], errors=\"coerce\").to_numpy()\n",
    "\n",
    "# ----------------- helpers métricas -----------------\n",
    "def ks_stat(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a[~np.isnan(a)]\n",
    "    b = b[~np.isnan(b)]\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return float(\"nan\")\n",
    "    xa, xb = np.sort(a), np.sort(b)\n",
    "    grid = np.unique(np.concatenate([xa, xb]))\n",
    "    def _ecdf(x, g): return np.searchsorted(x, g, side=\"right\") / x.size\n",
    "    Fa = np.array([_ecdf(xa, g) for g in grid], dtype=float)\n",
    "    Fb = np.array([_ecdf(xb, g) for g in grid], dtype=float)\n",
    "    return float(np.max(np.abs(Fa - Fb)))\n",
    "\n",
    "def psi_stat(expected: np.ndarray, actual: np.ndarray, bins: int = 20):\n",
    "    \"\"\"Population Stability Index com bins por quantis do baseline.\"\"\"\n",
    "    e = expected[~np.isnan(expected)]\n",
    "    a = actual[~np.isnan(actual)]\n",
    "    if e.size == 0 or a.size == 0:\n",
    "        return float(\"nan\"), pd.DataFrame()\n",
    "    qs = np.linspace(0, 1, bins + 1)\n",
    "    cuts = np.quantile(e, qs)\n",
    "    cuts = np.unique(cuts)\n",
    "    if cuts.size < 3:\n",
    "        # não há bins suficientes\n",
    "        return 0.0, pd.DataFrame()\n",
    "    e_hist, edges = np.histogram(e, bins=cuts)\n",
    "    a_hist, _     = np.histogram(a, bins=cuts)\n",
    "    e_prop = np.clip(e_hist / max(1, e_hist.sum()), 1e-8, 1.0)\n",
    "    a_prop = np.clip(a_hist / max(1, a_hist.sum()), 1e-8, 1.0)\n",
    "    contrib = (a_prop - e_prop) * np.log(a_prop / e_prop)\n",
    "    psi = float(np.sum(contrib))\n",
    "    bins_df = pd.DataFrame({\n",
    "        \"bin_left\": edges[:-1],\n",
    "        \"bin_right\": edges[1:],\n",
    "        \"expected_count\": e_hist,\n",
    "        \"actual_count\": a_hist,\n",
    "        \"expected_prop\": e_prop,\n",
    "        \"actual_prop\": a_prop,\n",
    "        \"psi_contrib\": contrib,\n",
    "    })\n",
    "    return psi, bins_df\n",
    "\n",
    "# ----------------- métricas KS/PSI -----------------\n",
    "KS  = ks_stat(val_err, scores)\n",
    "PSI, psi_bins = psi_stat(val_err, scores, bins=20)\n",
    "\n",
    "# ----------------- figuras: histograma comparativo -----------------\n",
    "fig1, ax1 = plt.subplots(figsize=(9.6, 4.8), dpi=FIG_DPI)\n",
    "ax1.hist(val_err, bins=NUM_BINS, density=True, alpha=0.5, label=\"Validação (baseline)\")\n",
    "ax1.hist(scores,  bins=NUM_BINS, density=True, alpha=0.5, label=\"Lote atual (scores)\")\n",
    "ax1.set_title(f\"Distribuições — baseline vs atual  |  KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "ax1.set_xlabel(\"Erro / Score\")\n",
    "ax1.set_ylabel(\"Densidade\")\n",
    "ax1.legend()\n",
    "hist_path = fig_dir / \"drift_hist.png\"\n",
    "fig1.tight_layout()\n",
    "fig1.savefig(hist_path, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "plt.close(fig1)\n",
    "\n",
    "# EXIBIR histograma\n",
    "display(Image(filename=str(hist_path)))\n",
    "\n",
    "# ----------------- figuras: CDF comparativa -----------------\n",
    "def _ecdf_values(x: np.ndarray):\n",
    "    x = x[~np.isnan(x)]\n",
    "    x = np.sort(x)\n",
    "    y = np.arange(1, x.size + 1) / x.size if x.size else np.array([])\n",
    "    return x, y\n",
    "\n",
    "xv, yv = _ecdf_values(val_err)\n",
    "xs, ys = _ecdf_values(scores)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(9.6, 4.8), dpi=FIG_DPI)\n",
    "if xv.size: ax2.step(xv, yv, where=\"post\", label=\"ECDF validação\")\n",
    "if xs.size: ax2.step(xs, ys, where=\"post\", label=\"ECDF atual\")\n",
    "ax2.set_title(\"CDF acumulada — baseline vs atual\")\n",
    "ax2.set_xlabel(\"Erro / Score\")\n",
    "ax2.set_ylabel(\"Proporção ≤ x\")\n",
    "ax2.legend()\n",
    "cdf_path = fig_dir / \"drift_cdf.png\"\n",
    "fig2.tight_layout()\n",
    "fig2.savefig(cdf_path, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "plt.close(fig2)\n",
    "\n",
    "# EXIBIR CDF\n",
    "display(Image(filename=str(cdf_path)))\n",
    "\n",
    "# ----------------- figura: boxplot MENSAL do erro de reconstrução -----------------\n",
    "monthly_path = None\n",
    "if {\"data_lcto\", \"recon_error\"}.issubset(set(df_sc.columns)):\n",
    "    dt = pd.to_datetime(df_sc[\"data_lcto\"], errors=\"coerce\")\n",
    "    err = pd.to_numeric(df_sc[\"recon_error\"], errors=\"coerce\")\n",
    "    ok = dt.notna() & err.notna()\n",
    "    if ok.any():\n",
    "        df_m = pd.DataFrame({\"ym\": dt.dt.to_period(\"M\").astype(str), \"recon_error\": err}).loc[ok]\n",
    "        groups = df_m.groupby(\"ym\")[\"recon_error\"].apply(list)\n",
    "        labels = list(groups.index)\n",
    "        data   = list(groups.values)\n",
    "\n",
    "        fig3, ax3 = plt.subplots(figsize=(9.6, 4.8), dpi=FIG_DPI)\n",
    "        ax3.boxplot(data, showfliers=False)\n",
    "        ax3.set_title(\"Distribuição MENSAL do erro de reconstrução (boxplot sem outliers)\")\n",
    "        ax3.set_xlabel(\"mês (YYYY-MM)\")\n",
    "        ax3.set_ylabel(\"erro de reconstrução\")\n",
    "\n",
    "        # rótulos enxutos (no máx. ~20 rótulos no eixo X)\n",
    "        step = max(1, len(labels)//20)\n",
    "        ax3.set_xticks(range(1, len(labels)+1)[::step], labels[::step], rotation=45, ha=\"right\")\n",
    "\n",
    "        fig3.tight_layout()\n",
    "        monthly_path = fig_dir / \"drift_monthly_box.png\"\n",
    "        fig3.savefig(monthly_path, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "        plt.close(fig3)\n",
    "\n",
    "        display(Image(filename=str(monthly_path)))\n",
    "\n",
    "# ----------------- salvar tabelas auxiliares (PSI bins) -----------------\n",
    "psi_bins_path = None\n",
    "if isinstance(psi_bins, pd.DataFrame) and not psi_bins.empty:\n",
    "    psi_bins_path = run_dir / \"drift_bins_psi.csv\"\n",
    "    psi_bins.to_csv(psi_bins_path, index=False, sep=\",\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# ----------------- export base64 para Etapa 12 (HTML) -----------------\n",
    "def _png_to_b64(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"ascii\")\n",
    "\n",
    "images_b64 = {}\n",
    "images_b64[\"drift_hist.png\"] = _png_to_b64(hist_path)\n",
    "images_b64[\"drift_cdf.png\"]  = _png_to_b64(cdf_path)\n",
    "if monthly_path:\n",
    "    images_b64[\"drift_monthly_box.png\"] = _png_to_b64(monthly_path)\n",
    "\n",
    "(run_dir / \"images_base64.json\").write_text(json.dumps(images_b64), encoding=\"utf-8\")\n",
    "\n",
    "# ----------------- salvar métricas (compat) -----------------\n",
    "metrics = {\n",
    "    \"ks_val_vs_current\": float(KS),\n",
    "    \"psi_val_vs_current\": float(PSI),\n",
    "    \"n_val\": int(np.sum(~np.isnan(val_err))),\n",
    "    \"n_current\": int(np.sum(~np.isnan(scores))),\n",
    "    \"hist_png\": str(hist_path),\n",
    "    \"cdf_png\": str(cdf_path),\n",
    "    \"monthly_box_png\": (str(monthly_path) if monthly_path else None),\n",
    "    \"psi_bins_csv\": (str(psi_bins_path) if psi_bins_path else None),\n",
    "}\n",
    "(run_dir / \"drift_metrics.json\").write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ----------------- salvar drift_monitoring.json (formato simples p/ Etapa 12) -----------------\n",
    "drift_monitoring = {\n",
    "    \"kpis\": {\"KS\": float(KS), \"PSI\": float(PSI)},\n",
    "    \"figures\": {\n",
    "        \"hist\": str(hist_path),\n",
    "        \"cdf\": str(cdf_path),\n",
    "        \"monthly_box\": (str(monthly_path) if monthly_path else None)\n",
    "    },\n",
    "    \"bins_psi_csv\": (str(psi_bins_path) if psi_bins_path else None)\n",
    "}\n",
    "(run_dir / \"drift_monitoring.json\").write_text(json.dumps(drift_monitoring, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nSkynet Informa: Monitoramento de drift concluído.\")\n",
    "print(f\"KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "print(f\"Figuras salvas e exibidas: {hist_path.name}, {cdf_path.name}\" + (f\", {Path(monthly_path).name}\" if monthly_path else \"\"))\n",
    "if psi_bins_path:\n",
    "    print(f\"Tabela de bins do PSI: {Path(psi_bins_path).name}\")\n",
    "print(\"images_base64.json, drift_metrics.json e drift_monitoring.json prontos para a Etapa 12.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jXlzTkKkusq"
   },
   "source": [
    "# **Etapa 12:** Relatório HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3081,
     "status": "ok",
     "timestamp": 1761013916421,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "zDxyQR1PmbX9",
    "outputId": "bd6794fc-748f-4573-be4a-446c8f122b80"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "Etapa 12 — Geração de Relatório HTML (executivo + técnico, imagens embutidas)\n",
    "\n",
    "Atualizações desta versão:\n",
    "- Salva em RUN_DIR/report\n",
    "- Completa “utilidade” na lista de artefatos (famílias por prefixo)\n",
    "- Fallback de features: features_config.json → feature_cols_autogen.json (lista/objeto) → features.pkl\n",
    "- Remove a tabela estatística por feature do treino (mantém formas/épocas/curvas)\n",
    "- Incorpora figuras existentes como base64 (largura máx. 500 px)\n",
    "- Top 15 a partir do CSV mais recente em PROJ_ROOT/output com colunas: rank_desc, anom_score, username, lotacao, dc, contacontabil, nome_conta, valormi, data_lcto\n",
    "- Textos e fundamentações adicionais (introdução, pipeline, métricas, drift, conclusão)\n",
    "- Explicitar divisão de dados (prevenção de vazamento temporal) e nota de multicolinearidade/seleção de features\n",
    "- Exibir JSON de arquitetura/hiperparâmetros a partir de model_config.train.json (se existir)\n",
    "- Explicitar método de normalização (Z-score) em texto\n",
    "- Incluir quantis do erro de reconstrução (p50/p90/p95/p99), se vetor existir\n",
    "- Garantir embed das figuras drift_hist.png e drift_cdf.png em RUN_DIR/figures\n",
    "- NOVO: calcular e exibir “correlação média absoluta” por feature (amostra leve), com pequena visualização\n",
    "- Removidas menções a “linguagem simples/simples”\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, io, json, base64, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# --------------------------\n",
    "# Pré-checagens MÍNIMAS\n",
    "# --------------------------\n",
    "assert 'RUN_DIR' in globals(), \"Execute Etapa 1 antes (RUN_DIR).\"\n",
    "assert 'PROJ_ROOT' in globals(), \"Execute Etapa 1 antes (PROJ_ROOT).\"\n",
    "\n",
    "RUN_DIR = Path(RUN_DIR)\n",
    "PROJ_ROOT = Path(PROJ_ROOT)\n",
    "# Salva na pasta correta: RUN_DIR/report\n",
    "REPORTS_DIR = RUN_DIR / \"report\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pasta de figuras do run (para salvar o gráfico leve de correlação, se gerado)\n",
    "FIG_DIR = RUN_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Utilidades\n",
    "# --------------------------\n",
    "def _b64_img(path: Path, max_width_px: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Retorna uma <img> com base64 inline. O redimensionamento por largura é feito via atributo HTML/CSS.\n",
    "    (Não reamostra o arquivo — usa width para layout.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = path.read_bytes()\n",
    "        mime = \"image/png\" if path.suffix.lower()==\".png\" else \"image/jpeg\"\n",
    "        b64 = base64.b64encode(data).decode(\"ascii\")\n",
    "        return f'<img src=\"data:{mime};base64,{b64}\" alt=\"{path.name}\" style=\"max-width:{max_width_px}px;width:100%;height:auto;border:1px solid #ddd;border-radius:6px;\"/>'\n",
    "    except Exception as e:\n",
    "        return f'<div style=\"color:#b00;\">(Falha ao embutir imagem {path.name}: {e})</div>'\n",
    "\n",
    "def _fmt_money(v) -> str:\n",
    "    try:\n",
    "        f = float(v)\n",
    "        return f\"{f:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "def _fmt_stat(v) -> str:\n",
    "    try:\n",
    "        f = float(v)\n",
    "        s = f\"{f:.5f}\"\n",
    "        s = re.sub(r\"(\\.[0-9]*?)0+$\", r\"\\1\", s)\n",
    "        s = re.sub(r\"\\.$\", \"\", s)\n",
    "        return s\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "def _safe_json(path: Path) -> Any | None:\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _safe_csv(path: Path, **kw) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_csv(path, **kw)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _list_files_human(folder: Path) -> List[tuple[str, str]]:\n",
    "    out = []\n",
    "    for p in sorted(folder.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            size = p.stat().st_size\n",
    "            n = size\n",
    "            for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "                if n < 1024:\n",
    "                    try:\n",
    "                        rel = str(p.relative_to(PROJ_ROOT))\n",
    "                    except Exception:\n",
    "                        rel = str(p)\n",
    "                    out.append((rel, f\"{n:.1f}{u}\"))\n",
    "                    break\n",
    "                n /= 1024.0\n",
    "    return out\n",
    "\n",
    "def _section(title: str, body_html: str, anchor_id: str | None = None) -> str:\n",
    "    _id = f' id=\"{anchor_id}\"' if anchor_id else \"\"\n",
    "    return f\"\"\"\n",
    "    <section{_id} style=\"margin:24px 0;\">\n",
    "      <h2 style=\"margin:0 0 8px 0;font-family:Inter,Arial;font-weight:700;font-size:16px;\">{title}</h2>\n",
    "      <div style=\"font-family:Inter,Arial;line-height:1.6;font-size:14px;color:#222;\">\n",
    "        {body_html}\n",
    "      </div>\n",
    "    </section>\n",
    "    \"\"\"\n",
    "\n",
    "def _table_dicts(rows: List[Dict[str, Any]], col_order: List[str]|None=None, monetary_cols: List[str]|None=None, max_rows:int=1000) -> str:\n",
    "    if not rows:\n",
    "        return \"<div style='color:#555;'>Sem dados.</div>\"\n",
    "    # normaliza numpy types\n",
    "    norm_rows = []\n",
    "    for r in rows[:max_rows]:\n",
    "        nr = {}\n",
    "        for k,v in r.items():\n",
    "            if isinstance(v, (np.floating, np.integer)):\n",
    "                v = v.item()\n",
    "            nr[k] = v\n",
    "        norm_rows.append(nr)\n",
    "    rows = norm_rows\n",
    "\n",
    "    if col_order is None:\n",
    "        col_order = list(rows[0].keys())\n",
    "    monetary_cols = set(monetary_cols or [])\n",
    "    head = \"\".join(f\"<th style='text-align:left;padding:6px 8px;background:#f5f5f5;border-bottom:1px solid #ddd;'>{c}</th>\" for c in col_order)\n",
    "    body = []\n",
    "    for r in rows[:max_rows]:\n",
    "        tds = []\n",
    "        for c in col_order:\n",
    "            v = r.get(c, \"\")\n",
    "            if c in monetary_cols:\n",
    "                v = _fmt_money(v)\n",
    "            elif isinstance(v, (int, float, np.floating)) and c not in monetary_cols:\n",
    "                v = _fmt_stat(v)\n",
    "            tds.append(f\"<td style='padding:6px 8px;border-bottom:1px solid #eee;'>{v}</td>\")\n",
    "        body.append(\"<tr>\" + \"\".join(tds) + \"</tr>\")\n",
    "    return f\"<div style='overflow:auto;'><table style='border-collapse:collapse;width:100%;min-width:480px;'><thead><tr>{head}</tr></thead><tbody>{''.join(body)}</tbody></table></div>\"\n",
    "\n",
    "# --------------------------\n",
    "# Localiza artefatos\n",
    "# --------------------------\n",
    "paths = {\n",
    "    \"run_json\"            : RUN_DIR / \"run.json\",\n",
    "    \"selected_source_csv\" : RUN_DIR / \"selected_source.csv\",\n",
    "    \"features_config\"     : RUN_DIR / \"features_config.json\",\n",
    "    \"feature_cols_autogen\": RUN_DIR / \"feature_cols_autogen.json\",      # fallback\n",
    "    \"features_pkl\"        : RUN_DIR / \"features.pkl\",                   # fallback\n",
    "    \"features_desc\"       : RUN_DIR / \"features_desc.json\",\n",
    "    \"categorical_maps\"    : RUN_DIR / \"categorical_maps.json\",\n",
    "    \"training_history\"    : RUN_DIR / \"training_history.csv\",            # Etapa 7\n",
    "    \"model_config_train\"  : RUN_DIR / \"model_config.train.json\",         # preferível\n",
    "    \"model_config\"        : RUN_DIR / \"model_config.json\",               # alternativo\n",
    "    \"ae_weights\"          : RUN_DIR / \"ae.pt\",                           # Etapa 7\n",
    "    \"recon_err_val\"       : RUN_DIR / \"reconstruction_errors_val.npy\",   # Etapa 7\n",
    "    \"recon_err_score\"     : RUN_DIR / \"reconstruction_errors_score.npy\", # Etapa 10/exec\n",
    "    \"scores_summary\"      : RUN_DIR / \"scores_summary.json\",             # Etapa 9/10\n",
    "    \"threshold_json\"      : RUN_DIR / \"threshold.json\",                  # Etapa 9/10\n",
    "    \"alerts_top_csv\"      : RUN_DIR / \"alerts_top100.csv\",               # legado\n",
    "    \"exec_stats_json\"     : RUN_DIR / \"exec_stats.json\",                 # Etapa 8\n",
    "    \"dist_compare_png\"    : RUN_DIR / \"figures/dist_exec_vs_train.png\",  # Etapa 8\n",
    "    \"drift_json\"          : RUN_DIR / \"drift_monitoring.json\",           # Etapa 11\n",
    "    \"drift_metrics_json\"  : RUN_DIR / \"drift_metrics.json\",              # alternativa\n",
    "    \"drift_fig_dir\"       : RUN_DIR / \"figures\",                         # pasta com gráficos\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 1) informações da execução\n",
    "# --------------------------\n",
    "run_meta = _safe_json(paths[\"run_json\"]) or {}\n",
    "created_at = run_meta.get(\"created_at\")\n",
    "paths_meta = run_meta.get(\"paths\", {})\n",
    "lista_arquivos = _list_files_human(RUN_DIR)\n",
    "\n",
    "# utilidades completas (inclui famílias/prefixos e itens citados)\n",
    "util_map = {\n",
    "    # snapshots e configs\n",
    "    \"run.json\": \"Metadados da execução (datas, timezone, caminhos).\",\n",
    "    \"selected_source.csv\": \"Snapshot da base usada para treino/val.\",\n",
    "    \"journal_entries.parquet\": \"Snapshot parquet da base (registros).\",\n",
    "    \"features_config.json\": \"Configuração manual de colunas de features.\",\n",
    "    \"feature_cols_autogen.json\": \"Lista de features gerada automaticamente (fallback).\",\n",
    "    \"features.pkl\": \"Pacote de pré-processamento (feature_cols, imputação, normalização, dtypes).\",\n",
    "    \"features_desc.json\": \"Estatísticas descritivas de X_train/X_val (formas, médias, desvios).\",\n",
    "    \"categorical_maps.json\": \"Vocabulário categórico congelado e mapeamentos.\",\n",
    "    # treino\n",
    "    \"training_history.csv\": \"Histórico de perdas (treino/val) por época.\",\n",
    "    \"model_config.train.json\": \"Arquitetura/hiperparâmetros efetivos do AE no treino.\",\n",
    "    \"model_config.json\": \"Configuração do modelo (alternativa/legado).\",\n",
    "    \"ae.pt\": \"Pesos do modelo treinado.\",\n",
    "    \"reconstruction_errors_val.npy\": \"Erros de reconstrução no conjunto de validação.\",\n",
    "    # execução/calibração\n",
    "    \"scores_summary.json\": \"Sumário de pontuações/limiar/alertas.\",\n",
    "    \"threshold.json\": \"Calibração do limiar (valor, modo, quantis/budget).\",\n",
    "    \"reconstruction_errors_score.npy\": \"Vetor de erros de reconstrução no lote pontuado.\",\n",
    "    \"alerts_top100.csv\": \"Top 100 alertas (legado da Etapa 10).\",\n",
    "    # drift e figuras\n",
    "    \"drift_monitoring.json\": \"KPIs e caminhos de figuras de drift (Etapa 11).\",\n",
    "    \"drift_metrics.json\": \"Métricas de drift (PSI/KS/p-valor) entre referência e lote atual.\",\n",
    "    \"training_curve.png\": \"Curva de perda por época (treino/val).\",\n",
    "    \"loss_history.png\": \"Curva alternativa de perda.\",\n",
    "    \"dist_exec_vs_train.png\": \"Comparação de distribuições (execução vs treino).\",\n",
    "    \"drift_hist.png\": \"Histograma comparativo (baseline vs atual).\",\n",
    "    \"drift_cdf.png\": \"CDF acumulada comparativa (baseline vs atual).\",\n",
    "    \"drift_daily_box.png\": \"Boxplot temporal do score/erro.\",\n",
    "    \"images_base64.json\": \"Export auxiliar de imagens para o HTML.\",\n",
    "    # pontuações/alertas agregados\n",
    "    \"scores_alerts.csv\": \"Scores com coluna alert (0/1).\",\n",
    "    \"alerts_summary.json\": \"Sumário de alertas por usuário/conta.\",\n",
    "    \"alerts_by_username.csv\": \"Agregado de alertas por usuário.\",\n",
    "    # famílias com sufixo de data\n",
    "    \"categorical_cardinality.json\": \"Cardinalidade por coluna categórica.\",\n",
    "    \"categorical_frequencies_*\": \"Frequências dos valores categóricos (ordenam/fixam vocabulário).\",\n",
    "    \"categorical_rev_maps_*\": \"Mapas reversos índice→rótulo para decodificação em relatórios.\",\n",
    "    \"features_behavior_*\": \"Features comportamentais agregadas (CSV/Parquet).\",\n",
    "    \"features_schema_*\": \"Esquema/dtypes da base de features comportamentais.\",\n",
    "    \"preprocess_report_BASE-*\": \"Relatório do pré-processo da base BASE.\",\n",
    "    \"preprocess_report_DESAFIO-*\": \"Relatório do pré-processo da base DESAFIO.\",\n",
    "    \"train_base_*\": \"Snapshot do conjunto de treino (CSV/Parquet).\",\n",
    "    \"train_schema_*\": \"Esquema/dtypes do snapshot de treino.\",\n",
    "    \"vocab_manifest_*\": \"Manifesto do vocabulário categórico congelado.\",\n",
    "}\n",
    "\n",
    "def _util_for_rel(rel_path: str) -> str:\n",
    "    base = os.path.basename(rel_path)\n",
    "    if base in util_map:\n",
    "        return util_map[base]\n",
    "    # famílias por prefixo\n",
    "    prefixes = [\n",
    "        (\"categorical_frequencies_\", \"categorical_frequencies_*\"),\n",
    "        (\"categorical_rev_maps_\", \"categorical_rev_maps_*\"),\n",
    "        (\"features_behavior_\", \"features_behavior_*\"),\n",
    "        (\"features_schema_\", \"features_schema_*\"),\n",
    "        (\"preprocess_report_BASE-\", \"preprocess_report_BASE-*\"),\n",
    "        (\"preprocess_report_DESAFIO-\", \"preprocess_report_DESAFIO-*\"),\n",
    "        (\"train_base_\", \"train_base_*\"),\n",
    "        (\"train_schema_\", \"train_schema_*\"),\n",
    "        (\"vocab_manifest_\", \"vocab_manifest_*\"),\n",
    "    ]\n",
    "    for pref, key in prefixes:\n",
    "        if base.startswith(pref):\n",
    "            return util_map.get(key, \"\")\n",
    "    return util_map.get(base, \"\")\n",
    "\n",
    "html_exec = []\n",
    "# Introdução (neutra)\n",
    "intro_exec = \"\"\"\n",
    "<div style=\"background:#f6f8fa;padding:12px;border-radius:8px;\">\n",
    "  <p><b>Objetivo.</b> Apresentar resultados do Autoencoder Tabular aplicado a lançamentos contábeis/financeiros, consolidando artefatos gerados nas etapas anteriores.</p>\n",
    "  <p><b>Interpretação.</b> “Alertas” indicam prioridade de revisão com base em comportamento atípico; não significam, por si, erro de registro.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "html_exec.append(f\"<p><b>Data/hora da execução:</b> {created_at or '(desconhecido)'} &nbsp; <b>Timezone:</b> {run_meta.get('timezone','?')}</p>\")\n",
    "if paths_meta:\n",
    "    html_exec.append(\"<p><b>Pastas relevantes</b></p>\")\n",
    "    html_exec.append(_table_dicts(\n",
    "        [{\"chave\": k, \"caminho\": v} for k,v in paths_meta.items()],\n",
    "        col_order=[\"chave\",\"caminho\"]\n",
    "    ))\n",
    "\n",
    "if lista_arquivos:\n",
    "    rows = []\n",
    "    for rel, sz in lista_arquivos:\n",
    "        rows.append({\"arquivo\": rel, \"tamanho\": sz, \"utilidade\": _util_for_rel(rel)})\n",
    "    html_exec.append(\"<p><b>Arquivos gerados nesta execução</b></p>\")\n",
    "    html_exec.append(_table_dicts(rows, col_order=[\"arquivo\",\"tamanho\",\"utilidade\"]))\n",
    "else:\n",
    "    html_exec.append(\"<p style='color:#b00;'>Aviso: não foi possível listar arquivos em RUN_DIR.</p>\")\n",
    "\n",
    "sec1 = _section(\"1) Informações da execução\", \"\".join(html_exec), anchor_id=\"sec1\")\n",
    "\n",
    "# --------------------------\n",
    "# 2) contextualização AE Tabular (texto)\n",
    "# --------------------------\n",
    "ctx = \"\"\"\n",
    "<p><b>Autoencoder (AE) tabular.</b> Modelo que aprende a reconstruir os dados de entrada, capturando padrões de referência.\n",
    "Registros que se afastam do padrão tendem a apresentar erro de reconstrução maior e podem ser priorizados para verificação.</p>\n",
    "\n",
    "<p><b>Funcionamento resumido.</b> O AE comprime as informações em uma camada central (gargalo) e tenta reconstruir os dados originais.\n",
    "Desvios significativos na reconstrução sinalizam comportamento atípico a ser analisado.</p>\n",
    "\"\"\"\n",
    "sec2 = _section(\"2) Contextualização do AE Tabular\", ctx, anchor_id=\"sec2\")\n",
    "\n",
    "# --------------------------\n",
    "# 2.1) Fluxo simplificado do processo (pipeline a partir de artefatos)\n",
    "# --------------------------\n",
    "pipeline_steps = []\n",
    "if list(RUN_DIR.glob(\"preprocess_report_BASE-*.json\")) or list(RUN_DIR.glob(\"preprocess_report_DESAFIO-*.json\")):\n",
    "    pipeline_steps.append(\"1) Preparação de dados e padronização dos campos.\")\n",
    "if (RUN_DIR / \"categorical_maps.json\").exists() or list(RUN_DIR.glob(\"vocab_manifest_*.json\")):\n",
    "    pipeline_steps.append(\"2) Conversão de colunas categóricas em representações numéricas (vocabulário congelado).\")\n",
    "if (RUN_DIR / \"features_config.json\").exists() or (RUN_DIR / \"feature_cols_autogen.json\").exists() or (RUN_DIR / \"features.pkl\").exists():\n",
    "    pipeline_steps.append(\"3) Engenharia de features (imputação e normalização).\")\n",
    "if (RUN_DIR / \"training_history.csv\").exists() or (RUN_DIR / \"model_config.train.json\").exists():\n",
    "    pipeline_steps.append(\"4) Treinamento do AE com dados históricos.\")\n",
    "if (RUN_DIR / \"scores_summary.json\").exists() or (RUN_DIR / \"threshold.json\").exists():\n",
    "    pipeline_steps.append(\"5) Definição do limiar (threshold) para destacar casos atípicos.\")\n",
    "if (RUN_DIR / \"reconstruction_errors_score.npy\").exists() or (RUN_DIR / \"scores_summary.json\").exists():\n",
    "    pipeline_steps.append(\"6) Aplicação na base de execução e geração de alertas.\")\n",
    "if (RUN_DIR / \"drift_monitoring.json\").exists() or (RUN_DIR / \"drift_metrics.json\").exists():\n",
    "    pipeline_steps.append(\"7) Monitoramento de mudança de padrão (drift).\")\n",
    "\n",
    "pipe_html = \"<ul>\" + \"\".join(f\"<li>{s}</li>\" for s in pipeline_steps) + \"</ul>\" if pipeline_steps else \"<p class='muted'>Pipeline não pôde ser inferido a partir dos artefatos.</p>\"\n",
    "sec2a = _section(\"2.1) Fluxo do processo\", pipe_html, anchor_id=\"sec2a\")\n",
    "\n",
    "# --------------------------\n",
    "# 3) features — descrição com fallback + notas (normalização e multicolinearidade)\n",
    "# --------------------------\n",
    "def _read_features_fallback(paths: Dict[str, Path]) -> tuple[list[str], str]:\n",
    "    # 1) features_config.json\n",
    "    cfg = _safe_json(paths[\"features_config\"])\n",
    "    if isinstance(cfg, dict):\n",
    "        fc = cfg.get(\"feature_cols\")\n",
    "        if isinstance(fc, list) and all(isinstance(x, str) for x in fc):\n",
    "            return fc, \"features_config.json\"\n",
    "\n",
    "    # 2) feature_cols_autogen.json (pode ser lista ou {\"feature_cols\":[...]})\n",
    "    aut = _safe_json(paths[\"feature_cols_autogen\"])\n",
    "    if isinstance(aut, list) and all(isinstance(x, str) for x in aut):\n",
    "        return aut, \"feature_cols_autogen.json (lista)\"\n",
    "    if isinstance(aut, dict):\n",
    "        fc = aut.get(\"feature_cols\")\n",
    "        if isinstance(fc, list) and all(isinstance(x, str) for x in fc):\n",
    "            return fc, \"feature_cols_autogen.json\"\n",
    "\n",
    "    # 3) features.pkl (dict com \"feature_cols\")\n",
    "    try:\n",
    "        import pickle\n",
    "        if paths[\"features_pkl\"].exists():\n",
    "            with open(paths[\"features_pkl\"], \"rb\") as f:\n",
    "                obj = pickle.load(f)\n",
    "            if isinstance(obj, dict):\n",
    "                fc = obj.get(\"feature_cols\")\n",
    "                if isinstance(fc, (list, tuple)) and all(isinstance(x, str) for x in fc):\n",
    "                    return list(fc), \"features.pkl\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return [], \"\"\n",
    "\n",
    "feature_cols, feat_source = _read_features_fallback(paths)\n",
    "\n",
    "desc_features_html = []\n",
    "desc_features_html.append(\"\"\"\n",
    "<div style=\"background:#f6f8fa;padding:8px;border-radius:6px;\">\n",
    "  <p><b>Features.</b> São as variáveis de entrada que representam cada lançamento. Exemplos: conta contábil, débito/crédito, unidade, valor, data, agregações e derivados.</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "# Normalização (Z-score) – explicação textual\n",
    "desc_features_html.append(\"\"\"\n",
    "<p><b>Normalização.</b> Valores numéricos padronizados por Z-score (média e desvio do treino em <code>features_desc.json</code>).\n",
    "Essa padronização evita distorções entre variáveis em escalas distintas.</p>\n",
    "\"\"\")\n",
    "# Nota sobre multicolinearidade/seleção (texto)\n",
    "desc_features_html.append(\"\"\"\n",
    "<p><b>Correlação e redundância.</b> Variáveis derivadas podem apresentar correlações elevadas.\n",
    "A etapa de pré-processamento pode reduzir colunas altamente correlacionadas (|ρ| ≥ 0,95), favorecendo estabilidade e interpretabilidade.</p>\n",
    "\"\"\")\n",
    "\n",
    "if feature_cols:\n",
    "    desc_features_html.append(f\"<p><b>Fonte das features:</b> {feat_source or 'não informada'}</p>\")\n",
    "    desc_features_html.append(\"<p>Colunas com sufixo <code>_int</code> são categorias codificadas; prefixo <code>feat_</code> indica derivadas numéricas.</p>\")\n",
    "    rows = [{\"#\": i+1, \"feature\": c, \"tipo\": (\"categórica codificada\" if c.endswith(\"_int\") else \"derivada/numérica\")} for i,c in enumerate(feature_cols)]\n",
    "    desc_features_html.append(_table_dicts(rows, col_order=[\"#\",\"feature\",\"tipo\"]))\n",
    "else:\n",
    "    desc_features_html.append(\"<p style='color:#b00;'>Aviso: não foi possível identificar a lista de features. Verifique <code>features_config.json</code>, <code>feature_cols_autogen.json</code> ou <code>features.pkl</code>.</p>\")\n",
    "\n",
    "sec3 = _section(\"3) Features do modelo\", \"\".join(desc_features_html), anchor_id=\"sec3\")\n",
    "\n",
    "# --------------------------\n",
    "# 3.1) Correlação média absoluta por feature (amostra leve)\n",
    "# --------------------------\n",
    "def _compute_light_corr(feature_cols: List[str]) -> Tuple[pd.DataFrame, Path | None]:\n",
    "    \"\"\"\n",
    "    Procura um arquivo leve com dados de features para estimar correlações:\n",
    "      - RUN_DIR/features_behavior_*.parquet ou *.csv (preferência)\n",
    "      - Caso não haja, tenta RUN_DIR/train_base_*.parquet/csv (como fallback)\n",
    "    Lê no máximo 10.000 linhas e até 200 colunas numéricas para limitar custo.\n",
    "    Retorna (tabela_meanabs_corr, caminho_figura_heatmap | None).\n",
    "    \"\"\"\n",
    "    # candidatos\n",
    "    cand = []\n",
    "    cand += sorted(RUN_DIR.glob(\"features_behavior_*.parquet\"))\n",
    "    cand += sorted(RUN_DIR.glob(\"features_behavior_*.csv\"))\n",
    "    cand += sorted(RUN_DIR.glob(\"train_base_*.parquet\"))\n",
    "    cand += sorted(RUN_DIR.glob(\"train_base_*.csv\"))\n",
    "\n",
    "    target = cand[-1] if cand else None\n",
    "    if target is None:\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    # leitura leve\n",
    "    try:\n",
    "        if target.suffix.lower() == \".parquet\":\n",
    "            import pyarrow  # noqa: F401\n",
    "            import pyarrow.parquet as pq  # noqa: F401\n",
    "            df = pd.read_parquet(target)\n",
    "        else:\n",
    "            df = pd.read_csv(target)\n",
    "    except Exception:\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    # seleção de colunas numéricas que estejam nas features (se informado)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if feature_cols:\n",
    "        num_cols = [c for c in num_cols if c in feature_cols]\n",
    "    # limitar quantidade de colunas para heatmap leve\n",
    "    if len(num_cols) > 200:\n",
    "        num_cols = num_cols[:200]\n",
    "\n",
    "    if not num_cols:\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    # amostragem de linhas\n",
    "    N = min(10000, len(df))\n",
    "    df_s = df.loc[:N-1, num_cols].copy()\n",
    "\n",
    "    # correlação absoluta\n",
    "    corr = df_s.corr().abs()\n",
    "\n",
    "    # correlação média absoluta (exclui diagonal)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        mean_abs_corr = corr.where(~np.eye(len(corr), dtype=bool)).mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "    tab = pd.DataFrame({\n",
    "        \"feature\": mean_abs_corr.index,\n",
    "        \"corr_média_absoluta\": mean_abs_corr.values\n",
    "    })\n",
    "\n",
    "    # pequena figura (heatmap) para top-K\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        topk = min(20, len(num_cols))\n",
    "        top_feats = mean_abs_corr.index[:topk].tolist()\n",
    "        C = corr.loc[top_feats, top_feats].values\n",
    "\n",
    "        fig_path = FIG_DIR / \"corr_heatmap_top20.png\"\n",
    "        plt.figure(figsize=(6, 5), dpi=120)\n",
    "        plt.imshow(C, aspect=\"auto\")\n",
    "        plt.xticks(range(len(top_feats)), top_feats, rotation=90, fontsize=7)\n",
    "        plt.yticks(range(len(top_feats)), top_feats, fontsize=7)\n",
    "        plt.title(\"Correlação absoluta — Top 20 por correlação média\")\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    except Exception:\n",
    "        fig_path = None\n",
    "\n",
    "    return tab.reset_index(drop=True), fig_path\n",
    "\n",
    "corr_tab, corr_fig = _compute_light_corr(feature_cols)\n",
    "\n",
    "corr_html = []\n",
    "corr_html.append(\"\"\"\n",
    "<p><b>Correlação média absoluta.</b> Indicador de redundância entre variáveis numéricas (amostra limitada).\n",
    "Valores altos sugerem grupos de colunas com informação semelhantes; pode-se reduzir colunas muito correlacionadas.</p>\n",
    "\"\"\")\n",
    "if not corr_tab.empty:\n",
    "    # mostrar top 20\n",
    "    corr_tab_view = corr_tab.head(20).copy()\n",
    "    corr_html.append(_table_dicts(\n",
    "        corr_tab_view.to_dict(orient=\"records\"),\n",
    "        col_order=[\"feature\", \"corr_média_absoluta\"]\n",
    "    ))\n",
    "    if corr_fig and Path(corr_fig).exists():\n",
    "        corr_html.append(_b64_img(Path(corr_fig), 500))\n",
    "else:\n",
    "    corr_html.append(\"<p style='color:#555;'>Sem dados adequados para cálculo leve de correlação (artefatos ausentes ou não numéricos).</p>\")\n",
    "\n",
    "sec3b = _section(\"3.1) Correlação entre variáveis (amostra)\", \"\".join(corr_html), anchor_id=\"sec3b\")\n",
    "\n",
    "# --------------------------\n",
    "# 3.2) Arquitetura/Hiperparâmetros do AE (exibe JSON se existir)\n",
    "# --------------------------\n",
    "model_cfg = _safe_json(paths[\"model_config_train\"]) or _safe_json(paths[\"model_config\"]) or {}\n",
    "arch_html = []\n",
    "if model_cfg:\n",
    "    pretty = json.dumps(model_cfg, indent=2, ensure_ascii=False)\n",
    "    arch_html.append(\"<p><b>Arquitetura e hiperparâmetros</b> (extraídos de <code>model_config.train.json</code> ou equivalente).</p>\")\n",
    "    arch_html.append(f\"<pre style='background:#f6f8fa;padding:8px;border-radius:6px;font-size:12px;white-space:pre-wrap;'>{pretty[:4000]}</pre>\")\n",
    "else:\n",
    "    arch_html.append(\"<p><b>Arquitetura do modelo.</b> Recomenda-se expor no arquivo <code>model_config.train.json</code> o número de camadas, tamanho do gargalo (bottleneck), função de ativação e dropout, critérios de early-stopping, otimizador, taxa de aprendizado e batch size.</p>\")\n",
    "\n",
    "sec3a = _section(\"3.2) Arquitetura e hiperparâmetros do AE\", \"\".join(arch_html), anchor_id=\"sec3a\")\n",
    "\n",
    "# --------------------------\n",
    "# 4) treino/validação: formas, épocas e curvas (sem tabela por feature) + nota de divisão\n",
    "# --------------------------\n",
    "feat_desc = _safe_json(paths[\"features_desc\"]) or {}\n",
    "hist_df = _safe_csv(paths[\"training_history\"])\n",
    "html_tv = []\n",
    "\n",
    "html_tv.append(\"\"\"\n",
    "<div style=\"background:#f6f8fa;padding:8px;border-radius:6px;margin-bottom:8px;\">\n",
    "  <p><b>Estabilidade do treinamento.</b> Avalie a curva de perda; a estabilização com validação consistente indica aprendizado de padrão sem memorização indevida.</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "if feat_desc:\n",
    "    shape_tr = feat_desc.get(\"train\", {}).get(\"shape\")\n",
    "    shape_va = feat_desc.get(\"val\", {}).get(\"shape\")\n",
    "    html_tv.append(f\"<p><b>Formas:</b> train={shape_tr}, val={shape_va}</p>\")\n",
    "else:\n",
    "    html_tv.append(\"<p style='color:#b00;'>Aviso: ausente <code>features_desc.json</code> (gerado na Etapa 6).</p>\")\n",
    "\n",
    "# Nota sobre divisão dos dados / vazamento temporal\n",
    "html_tv.append(\"\"\"\n",
    "<p><b>Divisão dos dados.</b> A separação entre treino e validação considera períodos distintos\n",
    "e/ou estratificação por atributos relevantes (ex.: usuário, unidade), reduzindo a possibilidade de vazamento temporal.</p>\n",
    "\"\"\")\n",
    "\n",
    "if hist_df is not None and not hist_df.empty:\n",
    "    n_epochs = int(hist_df[\"epoch\"].max()) + 1 if \"epoch\" in hist_df.columns else len(hist_df)\n",
    "    html_tv.append(f\"<p><b>Épocas de treino:</b> {n_epochs}</p>\")\n",
    "    last = hist_df.sort_values(\"epoch\").iloc[-1].to_dict()\n",
    "    train_loss = last.get(\"train_loss\", last.get(\"loss\", None))\n",
    "    val_loss   = last.get(\"val_loss\",   last.get(\"val\",  None))\n",
    "    parts = []\n",
    "    if train_loss is not None: parts.append(f\"<b>erro (treino)</b>: {_fmt_stat(train_loss)}\")\n",
    "    if val_loss   is not None: parts.append(f\"<b>erro (val)</b>: {_fmt_stat(val_loss)}\")\n",
    "    if parts:\n",
    "        html_tv.append(\"<p>\" + \" &nbsp;•&nbsp; \".join(parts) + \"</p>\")\n",
    "    # curvas (se existirem)\n",
    "    for cand in [RUN_DIR / \"figures\" / \"training_curve.png\", RUN_DIR / \"figures\" / \"loss_history.png\"]:\n",
    "        if cand.exists():\n",
    "            html_tv.append(_b64_img(cand, 500))\n",
    "else:\n",
    "    html_tv.append(\"<p style='color:#b00;'>Aviso: ausente <code>training_history.csv</code> (gerado na Etapa 7).</p>\")\n",
    "\n",
    "sec4 = _section(\"4) Base de treino e validação\", \"\".join(html_tv), anchor_id=\"sec4\")\n",
    "\n",
    "# --------------------------\n",
    "# 5) estatística da base de EXECUÇÃO (Etapa 8) + comparação\n",
    "# --------------------------\n",
    "html_execset = []\n",
    "html_execset.append(\"\"\"\n",
    "<div style=\"background:#f6f8fa;padding:8px;border-radius:6px;margin-bottom:8px;\">\n",
    "  <p><b>Comparabilidade.</b> Esta seção verifica se os dados atuais permanecem condizentes com o padrão de referência utilizado no treinamento.</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "exec_stats = _safe_json(paths[\"exec_stats_json\"]) or {}\n",
    "\n",
    "if exec_stats:\n",
    "    basic = exec_stats.get(\"basic\", {})\n",
    "    if basic:\n",
    "        rows = [{\"métrica\": k, \"valor\": _fmt_stat(v)} for k, v in basic.items()]\n",
    "        html_execset.append(\"<p><b>Resumo estatístico da base de execução</b></p>\")\n",
    "        html_execset.append(_table_dicts(rows, col_order=[\"métrica\",\"valor\"]))\n",
    "\n",
    "    num_rows = exec_stats.get(\"numeric\", [])\n",
    "    if num_rows:\n",
    "        html_execset.append(\"<p><b>Colunas numéricas</b></p>\")\n",
    "        html_execset.append(_table_dicts(num_rows, col_order=[\"col\",\"count\",\"missing\",\"mean\",\"std\",\"min\",\"max\"]))\n",
    "\n",
    "    cat_rows = exec_stats.get(\"categorical\", [])\n",
    "    if cat_rows:\n",
    "        def _pack(d):\n",
    "            if not d: return \"\"\n",
    "            v = d.get(\"value\", \"\")\n",
    "            f = d.get(\"freq\", \"\")\n",
    "            return f\"{v} ({f})\"\n",
    "        for r in cat_rows:\n",
    "            r[\"most_freq\"]  = _pack(r.get(\"most_freq\"))\n",
    "            r[\"least_freq\"] = _pack(r.get(\"least_freq\"))\n",
    "        html_execset.append(\"<p><b>Colunas categóricas</b></p>\")\n",
    "        html_execset.append(_table_dicts(cat_rows, col_order=[\"col\",\"n_distinct\",\"missing\",\"most_freq\",\"least_freq\"]))\n",
    "else:\n",
    "    html_execset.append(\"<p style='color:#555;'>Sem <code>exec_stats.json</code> (Etapa 8).</p>\")\n",
    "\n",
    "# figura de comparação execução vs treino\n",
    "if paths[\"dist_compare_png\"].exists():\n",
    "    html_execset.append(\"<p><b>Comparação de distribuição</b></p>\")\n",
    "    html_execset.append(_b64_img(paths[\"dist_compare_png\"], 500))\n",
    "else:\n",
    "    html_execset.append(\"<p style='color:#b00;'>Gráfico de comparação de distribuição ausente.</p>\"\n",
    "                        \"<p>Para gerar, execute a Etapa 8 e salve em \"\n",
    "                        \"<code>figures/dist_exec_vs_train.png</code>.</p>\")\n",
    "\n",
    "sec5 = _section(\"5) Base de execução: estatística e comparação de distribuição\", \"\".join(html_execset), anchor_id=\"sec5\")\n",
    "\n",
    "# --------------------------\n",
    "# 6) Métricas de erro e calibração do limiar (+ quantis)\n",
    "# --------------------------\n",
    "html_metrics = []\n",
    "html_metrics.append(\"\"\"\n",
    "<div style=\"background:#f6f8fa;padding:8px;border-radius:6px;margin-bottom:8px;\">\n",
    "  <p><b>Medições.</b> O erro de reconstrução quantifica o afastamento do padrão. O limiar separa observações típicas das atípicas e pode ser definido por percentil ou por orçamento de alertas.</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "scores_sum = _safe_json(paths[\"scores_summary\"]) or {}\n",
    "thr_json   = _safe_json(paths[\"threshold_json\"]) or {}\n",
    "stats_rows = []\n",
    "\n",
    "# Estatísticas disponíveis em scores_summary.json\n",
    "for k in [\"mean\",\"std\",\"p50\",\"p75\",\"p90\",\"p95\",\"p99\",\"min\",\"max\"]:\n",
    "    if k in scores_sum:\n",
    "        stats_rows.append({\"métrica\": k, \"valor\": _fmt_stat(scores_sum[k])})\n",
    "\n",
    "# MAE/MSE + QUANTIS se existir vetor de erros (score ou val)\n",
    "err_paths = [paths[\"recon_err_score\"], paths[\"recon_err_val\"]]\n",
    "mae_mse_rows = []\n",
    "quantis_html = \"\"\n",
    "for ep in err_paths:\n",
    "    if ep and ep.exists():\n",
    "        try:\n",
    "            arr = np.load(ep)\n",
    "            arr = np.array(arr).reshape(-1)\n",
    "            mae = float(np.mean(np.abs(arr)))\n",
    "            mse = float(np.mean(np.square(arr)))\n",
    "            src = ep.name\n",
    "            mae_mse_rows.append({\"métrica\": f\"MAE ({src})\", \"valor\": _fmt_stat(mae)})\n",
    "            mae_mse_rows.append({\"métrica\": f\"MSE ({src})\", \"valor\": _fmt_stat(mse)})\n",
    "            qs = np.quantile(arr, [0.5, 0.9, 0.95, 0.99])\n",
    "            qrows = [{\"quantil\": lab, \"erro\": _fmt_stat(val)} for lab, val in zip([\"50%\",\"90%\",\"95%\",\"99%\"], qs)]\n",
    "            quantis_html += \"<p><b>Quantis do erro de reconstrução — \" + src + \"</b></p>\" + _table_dicts(qrows, col_order=[\"quantil\",\"erro\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if stats_rows:\n",
    "    html_metrics.append(\"<p><b>Estatísticas do score/erro</b></p>\")\n",
    "    html_metrics.append(_table_dicts(stats_rows, col_order=[\"métrica\",\"valor\"]))\n",
    "\n",
    "if mae_mse_rows:\n",
    "    html_metrics.append(\"<p><b>Métricas derivadas (MAE/MSE)</b></p>\")\n",
    "    html_metrics.append(_table_dicts(mae_mse_rows, col_order=[\"métrica\",\"valor\"]))\n",
    "\n",
    "if quantis_html:\n",
    "    html_metrics.append(quantis_html)\n",
    "\n",
    "# Limiar/alertas\n",
    "thr_lines = []\n",
    "source_thr = scores_sum if \"threshold\" in scores_sum else thr_json\n",
    "if source_thr:\n",
    "    if \"threshold\" in source_thr: thr_lines.append(f\"<b>Limiar</b>: {_fmt_stat(source_thr['threshold'])}\")\n",
    "    if \"mode\" in source_thr: thr_lines.append(f\"<b>Método</b>: {source_thr['mode']}\")\n",
    "    if \"quantile\" in source_thr: thr_lines.append(f\"<b>Quantil</b>: {_fmt_stat(source_thr['quantile'])}\")\n",
    "    if \"budget\" in source_thr: thr_lines.append(f\"<b>Budget de alertas</b>: {source_thr['budget']}\")\n",
    "if \"n_alerts\" in scores_sum and \"n_linhas\" in scores_sum:\n",
    "    try:\n",
    "        rate = float(scores_sum.get(\"alert_rate\", 0.0)) * 100\n",
    "        thr_lines.append(f\"<b>Alertas</b>: {int(scores_sum['n_alerts']):,} de {int(scores_sum['n_linhas']):,} ({rate:.2f}%)\".replace(\",\", \".\"))\n",
    "    except Exception:\n",
    "        thr_lines.append(f\"<b>Alertas</b>: {int(scores_sum['n_alerts']):,} de {int(scores_sum['n_linhas']):,}\".replace(\",\", \".\"))\n",
    "\n",
    "if thr_lines:\n",
    "    html_metrics.append(\"<p>\" + \" &nbsp;•&nbsp; \".join(thr_lines) + \"</p>\")\n",
    "\n",
    "sec6 = _section(\"6) Métricas de erro e calibração do limiar\", \"\".join(html_metrics), anchor_id=\"sec6\")\n",
    "\n",
    "# --------------------------\n",
    "# 7) Monitoramento de drift (KS/PSI, figuras)\n",
    "# --------------------------\n",
    "html_drift = []\n",
    "html_drift.append(\"\"\"\n",
    "<div style=\"background:#f6f8fa;padding:8px;border-radius:6px;margin-bottom:8px;\">\n",
    "  <p><b>Mudança de padrão (drift).</b> KS e PSI medem a diferença entre distribuições de referência e atuais. PSI baixo (&le; 0,10) sugere estabilidade; entre 0,10 e 0,25 monitoramento; acima de 0,25 alteração relevante.</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "drift_obj = _safe_json(paths[\"drift_json\"]) or {}\n",
    "drift_metrics = _safe_json(paths[\"drift_metrics_json\"]) or {}\n",
    "\n",
    "# agrega kpis\n",
    "kpis = {}\n",
    "if isinstance(drift_obj.get(\"kpis\"), dict):\n",
    "    kpis.update(drift_obj[\"kpis\"])\n",
    "for key in [\"KS\",\"PSI\",\"ks\",\"psi\",\"pvalue\",\"n_bins\",\"ref_period\",\"cur_period\"]:\n",
    "    if key in drift_metrics:\n",
    "        kpis[key] = drift_metrics[key]\n",
    "\n",
    "if kpis:\n",
    "    psi = kpis.get(\"PSI\", kpis.get(\"psi\"))\n",
    "    ks  = kpis.get(\"KS\",  kpis.get(\"ks\"))\n",
    "    rows = [{\"métrica\": k, \"valor\": _fmt_stat(v)} for k,v in kpis.items()]\n",
    "    html_drift.append(_table_dicts(rows, col_order=[\"métrica\",\"valor\"]))\n",
    "\n",
    "    # interpretação prática\n",
    "    if isinstance(psi, (int,float)):\n",
    "        if psi > 0.25:\n",
    "            sev_txt = \"PSI alto: recomenda-se revisão/calibração do limiar e análise de processo.\"\n",
    "        elif psi > 0.10:\n",
    "            sev_txt = \"Mudança moderada: monitorar próximos lotes e avaliar ajustes, se persistente.\"\n",
    "        else:\n",
    "            sev_txt = \"Estabilidade observada.\"\n",
    "        html_drift.append(f\"<p><b>Interpretação do PSI</b>: {sev_txt}</p>\")\n",
    "    if isinstance(ks, (int,float)):\n",
    "        html_drift.append(\"<p><b>Leitura do KS.</b> Valores mais altos indicam maior diferença entre as distribuições acumuladas; na prática, valores acima de ~0,30 sugerem alteração relevante.</p>\")\n",
    "else:\n",
    "    html_drift.append(\"<p style='color:#555;'>Sem métricas de drift (<code>drift_monitoring.json</code> ou <code>drift_metrics.json</code>).</p>\")\n",
    "\n",
    "# figuras de drift em base64 — garantir caminhos corretos: RUN_DIR/figures/drift_hist.png e RUN_DIR/figures/drift_cdf.png\n",
    "drift_imgs = []\n",
    "for name in [\"drift_hist.png\",\"drift_cdf.png\",\"drift_daily_box.png\"]:\n",
    "    p = paths[\"drift_fig_dir\"] / name\n",
    "    if p.exists():\n",
    "        drift_imgs.append(_b64_img(p, 500))\n",
    "if drift_imgs:\n",
    "    html_drift.append(\"<p><b>Gráficos de drift</b></p>\" + \"\".join(drift_imgs))\n",
    "else:\n",
    "    html_drift.append(\"<p style='color:#b00;'>Figuras de drift ausentes.</p>\"\n",
    "                      \"<p>Para análise visual, garanta que os arquivos <code>drift_hist.png</code> e <code>drift_cdf.png</code> \"\n",
    "                      \"estejam em <code>RUN_DIR/figures</code>.</p>\")\n",
    "\n",
    "sec7 = _section(\"7) Monitoramento de mudança de padrão (drift)\", \"\".join(html_drift), anchor_id=\"sec7\")\n",
    "\n",
    "# --------------------------\n",
    "# 8) Top 15 lançamentos (CSV em PROJ_ROOT/output)\n",
    "# --------------------------\n",
    "html_alerts = []\n",
    "html_alerts.append(\"\"\"\n",
    "<div style=\"background:#f6f8fa;padding:8px;border-radius:6px;margin-bottom:8px;\">\n",
    "  <p><b>Prioridades de revisão.</b> Lançamentos mais atípicos conforme o modelo. “anom_score” é a intensidade do desvio estimado.</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "OUTPUT_DIR = PROJ_ROOT / \"output\"\n",
    "wanted_cols = [\"rank_desc\",\"anom_score\",\"username\",\"lotacao\",\"dc\",\"contacontabil\",\"nome_conta\",\"valormi\",\"data_lcto\"]\n",
    "top15_html = \"<p style='color:#555;'>Nenhum CSV em <code>output/</code> com as colunas exigidas foi encontrado.</p>\"\n",
    "\n",
    "if OUTPUT_DIR.exists():\n",
    "    csvs = sorted(OUTPUT_DIR.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    target = None\n",
    "    for csvp in csvs:\n",
    "        try:\n",
    "            head = pd.read_csv(csvp, nrows=0)\n",
    "            if all(c in head.columns for c in wanted_cols):\n",
    "                target = csvp\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if target is not None:\n",
    "        df = pd.read_csv(target)\n",
    "        if \"rank_desc\" in df.columns:\n",
    "            df[\"rank_desc\"] = pd.to_numeric(df[\"rank_desc\"], errors=\"coerce\")\n",
    "            df = df[df[\"rank_desc\"].notna()]\n",
    "            df = df.sort_values(\"rank_desc\", ascending=True)\n",
    "            df = df[df[\"rank_desc\"] <= 15]\n",
    "        df = df.loc[:, [c for c in wanted_cols if c in df.columns]]\n",
    "        if not df.empty:\n",
    "            top15_html = (\n",
    "                f\"<p><b>Fonte:</b> <code>{str(target.relative_to(PROJ_ROOT)) if PROJ_ROOT in target.parents else str(target)}</code></p>\"\n",
    "                + _table_dicts(df.to_dict(orient=\"records\"), col_order=[c for c in wanted_cols if c in df.columns], monetary_cols=[\"valormi\"], max_rows=15)\n",
    "            )\n",
    "\n",
    "sec8 = _section(\"8) Top 15 lançamentos (prioridade de revisão)\", top15_html, anchor_id=\"sec8\")\n",
    "\n",
    "# --------------------------\n",
    "# 9) Conclusão\n",
    "# --------------------------\n",
    "psi_val = None\n",
    "for source in (drift_obj.get(\"kpis\", {}) if isinstance(drift_obj.get(\"kpis\"), dict) else {}, drift_metrics):\n",
    "    if isinstance(source, dict):\n",
    "        if \"PSI\" in source and isinstance(source[\"PSI\"], (int,float)):\n",
    "            psi_val = float(source[\"PSI\"])\n",
    "            break\n",
    "        if \"psi\" in source and isinstance(source[\"psi\"], (int,float)):\n",
    "            psi_val = float(source[\"psi\"])\n",
    "            break\n",
    "\n",
    "conclusion_txt = []\n",
    "if psi_val is None:\n",
    "    conclusion_txt.append(\"Não foi possível avaliar a mudança de padrão (PSI ausente). Recomenda-se manter a monitoração nas próximas execuções.\")\n",
    "else:\n",
    "    if psi_val > 0.25:\n",
    "        conclusion_txt.append(\"Foram identificadas alterações relevantes nos padrões dos dados recentes. Recomenda-se revisar a calibração do limiar e analisar possíveis mudanças de processo.\")\n",
    "    elif psi_val > 0.10:\n",
    "        conclusion_txt.append(\"Foram observadas mudanças moderadas nos dados recentes. Recomenda-se acompanhar em execuções subsequentes e avaliar ajustes finos, se necessário.\")\n",
    "    else:\n",
    "        conclusion_txt.append(\"Os dados recentes permanecem estáveis em relação ao padrão aprendido neste ciclo.\")\n",
    "\n",
    "conclusion_txt.append(\"Os casos listados no Top 15 devem ser tratados como prioridades de verificação, sem prejulgar correção contábil.\")\n",
    "\n",
    "sec9 = _section(\"9) Conclusão\", \"<p>\" + \"</p><p>\".join(conclusion_txt) + \"</p>\", anchor_id=\"sec9\")\n",
    "\n",
    "# --------------------------\n",
    "# Montagem final do HTML\n",
    "# --------------------------\n",
    "now_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "title = f\"Relatório AE Tabular — Run {RUN_DIR.name}\"\n",
    "\n",
    "STYLE = \"\"\"\n",
    "<style>\n",
    "  @media (prefers-color-scheme: dark){\n",
    "    body{ background:#0f1115; color:#e6e6e6; }\n",
    "    table{ color:#e6e6e6; }\n",
    "    a{ color:#9ecbff; }\n",
    "  }\n",
    "  a{ text-decoration:none; }\n",
    "  a:hover{ text-decoration:underline; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Sumário (navegação interna)\n",
    "NAV = \"\"\"\n",
    "<nav style=\"margin:12px 0; font-size:14px;\">\n",
    "  <b>Sumário:</b>\n",
    "  <a href=\"#sec1\">1. Execução</a> •\n",
    "  <a href=\"#sec2\">2. AE Tabular</a> •\n",
    "  <a href=\"#sec2a\">2.1. Processo</a> •\n",
    "  <a href=\"#sec3\">3. Features</a> •\n",
    "  <a href=\"#sec3b\">3.1. Correlação</a> •\n",
    "  <a href=\"#sec3a\">3.2. Arquitetura</a> •\n",
    "  <a href=\"#sec4\">4. Treino</a> •\n",
    "  <a href=\"#sec5\">5. Execução</a> •\n",
    "  <a href=\"#sec6\">6. Métricas</a> •\n",
    "  <a href=\"#sec7\">7. Drift</a> •\n",
    "  <a href=\"#sec8\">8. Top 15</a> •\n",
    "  <a href=\"#sec9\">9. Conclusão</a>\n",
    "</nav>\n",
    "\"\"\"\n",
    "\n",
    "html_full = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"pt-br\">\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<title>{title}</title>\n",
    "{STYLE}\n",
    "</head>\n",
    "<body style=\"margin:24px; font-family:Inter,Arial;\">\n",
    "  <header style=\"margin-bottom:16px;\">\n",
    "    <h1 style=\"margin:0 0 4px 0;\">{title}</h1>\n",
    "    <div style=\"color:#666;font-size:12px;\">Gerado em {now_str}</div>\n",
    "    <hr style=\"margin-top:12px;border:none;border-top:1px solid #ddd;\"/>\n",
    "  </header>\n",
    "\n",
    "  {intro_exec}\n",
    "  {NAV}\n",
    "\n",
    "  {sec1}\n",
    "  {sec2}\n",
    "  {sec2a}\n",
    "  {sec3}\n",
    "  {sec3b}\n",
    "  {sec3a}\n",
    "  {sec4}\n",
    "  {sec5}\n",
    "  {sec6}\n",
    "  {sec7}\n",
    "  {sec8}\n",
    "  {sec9}\n",
    "\n",
    "  <footer style=\"margin-top:24px;color:#888;font-size:12px;\">\n",
    "    <hr style=\"border:none;border-top:1px solid #ddd;\"/>\n",
    "    <div>Este relatório agrega artefatos existentes no <code>RUN_DIR</code>; nenhuma etapa de processamento foi reexecutada além de cálculos leves para sumarização.</div>\n",
    "  </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "out_html = REPORTS_DIR / f\"relatorio_run_{RUN_DIR.name}.html\"\n",
    "out_html.write_text(html_full, encoding=\"utf-8\")\n",
    "print(f\"Relatório HTML gerado: {out_html}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4i2ECs2xJDB"
   },
   "source": [
    "# **Etapa 13:** Revisão por LLM\n",
    "---\n",
    "\n",
    "Comentários de uma LLM sobre o relatório gerado pelo modelo, sem visibilidade dos dados (apenas parâmetros, distribuições e resultados estatísticos).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32956,
     "status": "ok",
     "timestamp": 1761014023746,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "XZIx0kA7ht6h",
    "outputId": "86fbfa06-1f25-4f35-9aaf-ff82f7990014"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# ============================\n",
    "# Etapa 13 — Avaliação por LLM (OpenRouter): crítica estatística do relatório\n",
    "# ============================\n",
    "# Esta versão:\n",
    "#  - Lista RUN_DIR existentes e permite selecionar um\n",
    "#  - Busca o relatório exclusivamente em RUN_DIR/report/*.html\n",
    "#  - Se não encontrar HTML, informa claramente e encerra (falha cedo)\n",
    "#  - Remove menções a \"Skynet\" e usa prints simples\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ---------------------- Diretórios base ----------------------\n",
    "CWD = Path.cwd()\n",
    "\n",
    "# Raízes candidatas para descoberta automática (expandida mais adiante)\n",
    "ROOT_CANDIDATES = [\n",
    "    CWD,\n",
    "    CWD / \"ae-tabular\",\n",
    "    Path(\"/content\"),\n",
    "    Path(\"/content/ae-tabular\"),\n",
    "    Path(\"/content/drive/MyDrive\"),\n",
    "    Path(\"/content/drive/MyDrive/ae-tabular\"),\n",
    "]\n",
    "\n",
    "def _first_existing(path: Path) -> Path | None:\n",
    "    return path if path.exists() else None\n",
    "\n",
    "# Pasta reports/evaluations para salvar a crítica da LLM (não é o relatório HTML)\n",
    "REPORTS_DIR = (\n",
    "    _first_existing(CWD / \"reports\")\n",
    "    or _first_existing(CWD / \"ae-tabular\" / \"reports\")\n",
    "    or _first_existing(Path(\"/content/ae-tabular/reports\"))\n",
    "    or (CWD / \"reports\")\n",
    ")\n",
    "EVAL_DIR = REPORTS_DIR / \"evaluations\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------- Dependências sob demanda ----------------------\n",
    "def _ensure_pkg(pkg: str, pip_name: str | None = None):\n",
    "    import importlib.util\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        if \"google.colab\" in sys.modules:\n",
    "            get_ipython().run_line_magic(\"pip\", f\"install -q {pip_name or pkg}\")\n",
    "        else:\n",
    "            os.system(f\"{sys.executable} -m pip install -q {pip_name or pkg}\")\n",
    "\n",
    "_bs4_ready = False\n",
    "_pdfminer_ready = False\n",
    "\n",
    "def _need_bs4():\n",
    "    global _bs4_ready\n",
    "    if not _bs4_ready:\n",
    "        _ensure_pkg(\"bs4\", \"beautifulsoup4\")\n",
    "        _bs4_ready = True\n",
    "\n",
    "def _need_pdfminer():\n",
    "    global _pdfminer_ready\n",
    "    if not _pdfminer_ready:\n",
    "        _ensure_pkg(\"pdfminer\", \"pdfminer.six\")\n",
    "        _pdfminer_ready = True\n",
    "\n",
    "# ---------------------- Cliente OpenRouter via HTTP ----------------------\n",
    "def _ensure_http_client():\n",
    "    \"\"\"\n",
    "    Retorna tuple: (post_func, default_model, default_temperature, headers_base)\n",
    "    onde post_func(url, json, headers) faz POST e retorna dict.\n",
    "    \"\"\"\n",
    "    _ensure_pkg(\"requests\", \"requests\")\n",
    "    import requests\n",
    "\n",
    "    key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if not key:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            key = userdata.get(\"OPENROUTER_API_KEY\")\n",
    "        except Exception:\n",
    "            key = None\n",
    "    if not key:\n",
    "        raise RuntimeError(\n",
    "            \"OPENROUTER_API_KEY ausente. No Colab, defina em Secrets.\\n\"\n",
    "            \"Alternativa: os.environ['OPENROUTER_API_KEY']='sk-or-...'\"\n",
    "        )\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = key\n",
    "\n",
    "    headers_base = {\n",
    "        \"Authorization\": f\"Bearer {key}\",\n",
    "        \"HTTP-Referer\": os.getenv(\"OPENROUTER_HTTP_REFERER\", \"https://colab.research.google.com\"),\n",
    "        \"X-Title\": os.getenv(\"OPENROUTER_X_TITLE\", \"AE-Tabular-LLM-Eval\"),\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    default_model = os.getenv(\"OPENROUTER_MODEL\", \"x-ai/grok-4-fast\")\n",
    "    try:\n",
    "        default_temperature = float(os.getenv(\"OPENROUTER_TEMPERATURE\", \"0.0\"))\n",
    "    except Exception:\n",
    "        default_temperature = 0.0\n",
    "\n",
    "    def _post(url: str, payload: dict, headers: dict):\n",
    "        resp = requests.post(url, json=payload, headers=headers, timeout=120)\n",
    "        if resp.status_code >= 400:\n",
    "            try:\n",
    "                data = resp.json()\n",
    "            except Exception:\n",
    "                data = {\"error\": resp.text}\n",
    "            raise RuntimeError(f\"HTTP {resp.status_code} — {data}\")\n",
    "        try:\n",
    "            return resp.json()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Falha ao decodificar JSON da resposta: {e}\")\n",
    "\n",
    "    return _post, default_model, default_temperature, headers_base\n",
    "\n",
    "# ---------------------- Leitura de texto do relatório ----------------------\n",
    "def _read_text_from_file(path: Path, max_chars: int = 45_000) -> str:\n",
    "    ext = path.suffix.lower()\n",
    "    try:\n",
    "        if ext in [\".md\", \".txt\"]:\n",
    "            text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        elif ext in [\".html\", \".htm\"]:\n",
    "            _need_bs4()\n",
    "            from bs4 import BeautifulSoup\n",
    "            raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            soup = BeautifulSoup(raw, \"html.parser\")\n",
    "            for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "                tag.extract()\n",
    "            text = soup.get_text(separator=\"\\n\")\n",
    "        elif ext == \".pdf\":\n",
    "            _need_pdfminer()\n",
    "            from pdfminer.high_level import extract_text\n",
    "            text = extract_text(str(path)) or \"\"\n",
    "        else:\n",
    "            text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao ler/extrair texto de {path.name}: {e}\") from e\n",
    "\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    if len(text) > max_chars:\n",
    "        head = text[: int(max_chars * 0.6)]\n",
    "        tail = text[-int(max_chars * 0.4):]\n",
    "        text = head + \"\\n\\n[...conteúdo omitido por limite de contexto...]\\n\\n\" + tail\n",
    "    return text.strip()\n",
    "\n",
    "# ---------------------- Artefatos numéricos (opcional) ----------------------\n",
    "def _load_metrics_context() -> str:\n",
    "    parts = []\n",
    "    json_candidates = [\n",
    "        Path(\"scores_summary.json\"),\n",
    "        Path(\"threshold.json\"),\n",
    "        Path(\"artifacts\") / \"scores_summary.json\",\n",
    "        Path(\"artifacts\") / \"threshold.json\",\n",
    "        Path(\"ae-tabular\") / \"scores_summary.json\",\n",
    "        Path(\"ae-tabular\") / \"threshold.json\",\n",
    "        Path(\"ae-tabular\") / \"artifacts\" / \"scores_summary.json\",\n",
    "        Path(\"ae-tabular\") / \"artifacts\" / \"threshold.json\",\n",
    "        Path(\"/content/ae-tabular/scores_summary.json\"),\n",
    "        Path(\"/content/ae-tabular/threshold.json\"),\n",
    "        Path(\"/content/ae-tabular/artifacts/scores_summary.json\"),\n",
    "        Path(\"/content/ae-tabular/artifacts/threshold.json\"),\n",
    "    ]\n",
    "    seen = set()\n",
    "    for p in json_candidates:\n",
    "        if p.exists() and p.suffix.lower() == \".json\" and p not in seen:\n",
    "            try:\n",
    "                d = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "                parts.append(f\"# {p.name}\\n{json.dumps(d, ensure_ascii=False, indent=2)}\")\n",
    "                seen.add(p)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return \"\\n\\n\".join(parts).strip()\n",
    "\n",
    "# ---------------------- Descoberta e seleção de RUN_DIR ----------------------\n",
    "def _known_roots() -> List[Path]:\n",
    "    roots = set()\n",
    "    # CWD e pais imediatos\n",
    "    for up in [CWD, *CWD.parents[:3]]:\n",
    "        roots.add(up)\n",
    "\n",
    "    # PROJ_ROOT (se existir)\n",
    "    pr = globals().get(\"PROJ_ROOT\", None)\n",
    "    if pr:\n",
    "        pr = Path(pr)\n",
    "        if pr.exists():\n",
    "            for up in [pr, *pr.parents[:3]]:\n",
    "                roots.add(up)\n",
    "\n",
    "    # RUN_DIR (se existir)\n",
    "    rd = globals().get(\"RUN_DIR\", None)\n",
    "    if rd:\n",
    "        rd = Path(rd)\n",
    "        if rd.exists():\n",
    "            for up in [rd, rd.parent, *rd.parents[:3]]:\n",
    "                roots.add(up)\n",
    "\n",
    "    # candidatos fixos\n",
    "    for p in ROOT_CANDIDATES:\n",
    "        if p.exists():\n",
    "            roots.add(p)\n",
    "\n",
    "    return [p for p in roots if isinstance(p, Path) and p.exists()]\n",
    "\n",
    "def _discover_run_dirs(max_dirs: int = 200) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Estratégias:\n",
    "      A) <root>/runs/*              → cada subpasta é um RUN_DIR\n",
    "      B) pastas com nome YYYYmmdd-HHMMSS\n",
    "      C) qualquer pasta que contenha report/*.html → o RUN_DIR é o pai dessa pasta\n",
    "    \"\"\"\n",
    "    candidates: List[Path] = []\n",
    "    seen = set()\n",
    "    roots = _known_roots()\n",
    "\n",
    "    # A) <root>/runs/*\n",
    "    for root in roots:\n",
    "        runs_root = root / \"runs\"\n",
    "        if runs_root.exists() and runs_root.is_dir():\n",
    "            for d in runs_root.iterdir():\n",
    "                if d.is_dir():\n",
    "                    key = str(d.resolve())\n",
    "                    if key not in seen:\n",
    "                        candidates.append(d)\n",
    "                        seen.add(key)\n",
    "\n",
    "    # B) pastas com padrão YYYYmmdd-HHMMSS\n",
    "    pat = re.compile(r\"^\\d{8}-\\d{6}$\")\n",
    "    for root in roots:\n",
    "        try:\n",
    "            for d in root.iterdir():\n",
    "                if d.is_dir() and pat.match(d.name):\n",
    "                    key = str(d.resolve())\n",
    "                    if key not in seen:\n",
    "                        candidates.append(d)\n",
    "                        seen.add(key)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # C) qualquer pasta que contenha report/*.html (sobe um nível)\n",
    "    for root in roots:\n",
    "        try:\n",
    "            for report_dir in root.rglob(\"report\"):\n",
    "                if report_dir.is_dir():\n",
    "                    htmls = list(report_dir.glob(\"*.html\"))\n",
    "                    if htmls:\n",
    "                        rd = report_dir.parent\n",
    "                        key = str(rd.resolve())\n",
    "                        if key not in seen:\n",
    "                            candidates.append(rd)\n",
    "                            seen.add(key)\n",
    "                if len(candidates) >= max_dirs:\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Ordena por mtime desc e dedup\n",
    "    candidates = list({str(p.resolve()): p for p in candidates}.values())\n",
    "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return candidates\n",
    "\n",
    "def _prompt_select_run_dir(run_dirs: List[Path]) -> Path:\n",
    "    # Prioriza RUN_DIR global, se houver\n",
    "    rd_global = globals().get(\"RUN_DIR\", None)\n",
    "    ordered = []\n",
    "    if rd_global and Path(rd_global).exists():\n",
    "        rd_global = Path(rd_global).resolve()\n",
    "        ordered.append(rd_global)\n",
    "    for d in run_dirs:\n",
    "        if not ordered or d.resolve() != ordered[0]:\n",
    "            ordered.append(d)\n",
    "\n",
    "    if not ordered:\n",
    "        searched = \"\\n - \" + \"\\n - \".join(str(p) for p in _known_roots())\n",
    "        raise RuntimeError(\n",
    "            \"Nenhum RUN_DIR encontrado nas raízes conhecidas.\\n\"\n",
    "            \"Locais verificados:\" + searched + \"\\n\"\n",
    "            \"Dica: garanta que exista um diretório como runs/AAAAmmdd-HHMMSS \"\n",
    "            \"com subpasta 'report' contendo ao menos um .html.\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== Selecione o RUN_DIR para avaliação ===\")\n",
    "    for i, d in enumerate(ordered, start=1):\n",
    "        ts = datetime.fromtimestamp(d.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        star = \"  (RUN_DIR atual)\" if i == 1 and 'RUN_DIR' in globals() and Path(globals()['RUN_DIR']).resolve() == d.resolve() else \"\"\n",
    "        print(f\"{i:2d}) {d}   (modificado: {ts}){star}\")\n",
    "    print(\" 0) Digitar caminho manualmente\")\n",
    "\n",
    "    try:\n",
    "        opt = input(\"Escolha um número (ou 0 para informar caminho): \").strip()\n",
    "    except EOFError:\n",
    "        opt = \"1\"  # padrão: primeira opção\n",
    "\n",
    "    if opt == \"0\":\n",
    "        manual = input(\"Informe o caminho do RUN_DIR: \").strip()\n",
    "        sel = Path(manual)\n",
    "        if not sel.exists() or not sel.is_dir():\n",
    "            raise RuntimeError(f\"RUN_DIR inválido: {sel}\")\n",
    "        return sel\n",
    "\n",
    "    try:\n",
    "        idx = int(opt)\n",
    "        if 1 <= idx <= len(ordered):\n",
    "            return ordered[idx - 1]\n",
    "        else:\n",
    "            raise ValueError\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"Seleção inválida. Reinicie a etapa e escolha uma opção válida.\")\n",
    "\n",
    "def _find_report_in_run_dir(run_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Procura HTML em RUN_DIR/report/*.html.\n",
    "    Se não houver, alerta e falha cedo (não cria stub).\n",
    "    \"\"\"\n",
    "    report_dir = run_dir / \"report\"\n",
    "    if not report_dir.exists():\n",
    "        raise RuntimeError(f\"Pasta de relatório não encontrada: {report_dir}\")\n",
    "    htmls = sorted(report_dir.glob(\"*.html\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not htmls:\n",
    "        raise RuntimeError(\n",
    "            f\"Nenhum relatório HTML encontrado em {report_dir}.\\n\"\n",
    "            \"Execute a Etapa 12 para gerar o relatório antes de avaliar.\"\n",
    "        )\n",
    "    return htmls[0]\n",
    "\n",
    "# ---------------------- Prompts para a LLM ----------------------\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Você é um cientista de dados e estatístico sênior. \"\n",
    "    \"Sua função é auditar criticamente um RELATÓRIO TÉCNICO de um projeto de \"\n",
    "    \"detecção de anomalias com autoencoder tabular aplicado a lançamentos contábeis/financeiros. \"\n",
    "    \"Não assuma acesso a dados linha-a-linha; avalie somente o que foi fornecido (texto e métricas agregadas). \"\n",
    "    \"Seja preciso, objetivo e tecnicamente rigoroso.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Contexto e confidencialidade:\n",
    "- O conteúdo refere-se a um projeto de detecção de anomalias em registros contábeis/financeiros.\n",
    "- Não utilize exemplos sintéticos sem aviso.\n",
    "- Não solicite dados brutos; sua avaliação deve se limitar ao material fornecido.\n",
    "\n",
    "Materiais fornecidos:\n",
    "1) Relatório do projeto (trechos extraídos; pode estar truncado por limite de contexto. caso isso ocorra, avise).\n",
    "2) Artefatos de métricas agregadas (quando disponíveis).\n",
    "\n",
    "=== RELATÓRIO (TEXTO) ===\n",
    "{report_text}\n",
    "\n",
    "=== ARTEFATOS (MÉTRICAS) ===\n",
    "{metrics_text}\n",
    "\n",
    "Tarefa:\n",
    "Produza uma avaliação crítica, como um parecer técnico independente, cobrindo os pontos:\n",
    "\n",
    "1) CONSISTÊNCIA METODOLÓGICA\n",
    "   - O pipeline e as escolhas estão coerentes? Há lacunas?\n",
    "   - As hipóteses implícitas estão claras e razoáveis?\n",
    "\n",
    "2) PARÂMETROS DO MODELO (AE Tabular)\n",
    "   - Estrutura do AE (camadas, bottleneck, ativação, dropout), critérios de early-stopping.\n",
    "   - Hiperparâmetros: justificativas e possíveis alternativas.\n",
    "   - Normalização/Padronização, codificação categórica, balanceamento de classes (se aplicável).\n",
    "\n",
    "3) MÉTRICAS E ESTATÍSTICA\n",
    "   - Interprete MAE/MSE/quantis do erro de reconstrução, KS/PSI e eventuais taxas de alerta vs. validação.\n",
    "   - Avalie calibração do threshold (budget/meta/cost-min), risco de over/under-alerting e possíveis ajustes.\n",
    "\n",
    "4) DISTRIBUIÇÕES E DRIFT\n",
    "   - Avalie o uso de histogramas e distâncias (KS/PSI); discuta estabilidade e monitoramento em produção.\n",
    "   - Sugira limites de controle (controle estatístico de processo) e checagens periódicas.\n",
    "\n",
    "5) RISCOS\n",
    "   - Riscos de modelo (drift, mudança de regime, dados faltantes, vazamento).\n",
    "\n",
    "6) RECOMENDAÇÕES PRIORIZADAS\n",
    "   - Lista objetiva (curta) em ordem de impacto/esforço, com ações executáveis.\n",
    "   - Sugerir experimentos de baixo custo para próximo ciclo.\n",
    "\n",
    "Formato de saída:\n",
    "- Responda em Markdown com seções e listas.\n",
    "- Seja específico e acionável, evitando jargão desnecessário.\n",
    "- Não invente valores; quando algo não estiver claro, sinalize explicitamente.\n",
    "\n",
    "Lembrete: NÃO ACESSE dados brutos; avalie somente o texto e métricas agregadas acima.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------- Execução principal ----------------------\n",
    "def etapa_13_avaliacao_llm(run_dir: Path,\n",
    "                           model_override: str | None = None,\n",
    "                           temperature: float | None = None,\n",
    "                           default_model: str = \"x-ai/grok-4-fast\") -> Path:\n",
    "    print(\"Iniciando avaliação por LLM...\")\n",
    "\n",
    "    # 1) Cliente HTTP do OpenRouter\n",
    "    post, model_env_default, temp_env_default, headers = _ensure_http_client()\n",
    "    model = model_override or model_env_default or default_model\n",
    "    temp = float(temperature if temperature is not None else temp_env_default)\n",
    "    print(f\"LLM configurada | model={model} | temperature={temp}\")\n",
    "\n",
    "    # 2) Seleção do relatório dentro do RUN_DIR escolhido\n",
    "    rpt = _find_report_in_run_dir(run_dir)\n",
    "    print(f\"Relatório selecionado: {rpt}\")\n",
    "\n",
    "    # 3) Extração do texto\n",
    "    report_text = _read_text_from_file(rpt, max_chars=45_000)\n",
    "    if not report_text:\n",
    "        raise RuntimeError(\"Falha ao extrair texto do relatório ou relatório vazio.\")\n",
    "\n",
    "    # 4) Métricas agregadas (opcional, fora do RUN_DIR)\n",
    "    metrics_text = _load_metrics_context() or \"(sem artefatos JSON encontrados)\"\n",
    "\n",
    "    # 5) Prompt\n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(report_text=report_text, metrics_text=metrics_text)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    # 6) Chamada à LLM (HTTP)\n",
    "    print(\"Enviando relatório à LLM (sem dados brutos)...\")\n",
    "    try:\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temp,\n",
    "        }\n",
    "        data = post(\"https://openrouter.ai/api/v1/chat/completions\", payload, headers)\n",
    "        choices = data.get(\"choices\") or []\n",
    "        if not choices or not choices[0].get(\"message\", {}).get(\"content\"):\n",
    "            raise RuntimeError(f\"Resposta inesperada do OpenRouter: {data}\")\n",
    "        critique = choices[0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha na chamada à LLM: {e}\") from e\n",
    "\n",
    "    # 7) Imprimir no console\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\" + critique + \"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    print(\"Avaliação recebida da LLM – fim\")\n",
    "\n",
    "    # 8) Salvar .md + metadata.json\n",
    "    ts_sp = datetime.now(timezone(timedelta(hours=-3))).strftime(\"%Y-%m-%d_%H-%M-%S\")  # fuso São Paulo\n",
    "    base_name = f\"avaliacao_llm_{ts_sp}\"\n",
    "    out_md = (EVAL_DIR / f\"{base_name}.md\")\n",
    "    out_meta = (EVAL_DIR / f\"{base_name}.metadata.json\")\n",
    "\n",
    "    header = (\n",
    "        \"# Avaliação por LLM — Projeto AE-Tabular\\n\\n\"\n",
    "        f\"- Data/Hora (SP): {ts_sp}\\n\"\n",
    "        f\"- Relatório avaliado: `{rpt.name}`\\n\"\n",
    "        f\"- Modelo: {model}\\n\"\n",
    "        f\"- Temperatura: {temp}\\n\"\n",
    "        f\"- Observação: Sem acesso a dados brutos; somente texto e métricas agregadas.\\n\\n\"\n",
    "        \"---\\n\\n\"\n",
    "    )\n",
    "    out_md.write_text(header + critique + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    metadata = {\n",
    "        \"timestamp_sp\": ts_sp,\n",
    "        \"report_path\": str(rpt),\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"evaluation_path\": str(out_md),\n",
    "        \"model\": model,\n",
    "        \"temperature\": temp,\n",
    "        \"context_chars\": {\n",
    "            \"report\": len(report_text),\n",
    "            \"metrics\": len(metrics_text),\n",
    "        },\n",
    "        \"notes\": \"Avaliação gerada sem acesso a dados linha-a-linha.\",\n",
    "    }\n",
    "    out_meta.write_text(json.dumps(metadata, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Relatório de avaliação salvo: {out_md}\")\n",
    "    print(f\"Metadata salva: {out_meta}\")\n",
    "    return out_md\n",
    "\n",
    "# ---------------------- Interação: listar RUN_DIR e selecionar ----------------------\n",
    "run_dirs = _discover_run_dirs()\n",
    "selected_run_dir = _prompt_select_run_dir(run_dirs)\n",
    "\n",
    "DEFAULT_OPENROUTER_MODEL = \"x-ai/grok-4-fast\"\n",
    "model_env = os.getenv(\"OPENROUTER_MODEL\") or None\n",
    "temp_env = os.getenv(\"OPENROUTER_TEMPERATURE\")\n",
    "temp_arg = float(temp_env) if temp_env is not None else None\n",
    "\n",
    "out_path = etapa_13_avaliacao_llm(\n",
    "    run_dir=selected_run_dir,\n",
    "    model_override=model_env or DEFAULT_OPENROUTER_MODEL,\n",
    "    temperature=temp_arg\n",
    ")\n",
    "print(f\"Concluída. Arquivo final: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNncOrMgtxYjzuXQeV9Lubz",
   "collapsed_sections": [
    "GRNqsqxZWPzs",
    "clC8wQo4V2_j",
    "W-aIMDUNNhfi",
    "5KG4WKvaiBxh",
    "1BL2I4I9uZKo",
    "CVu46r3fvtxW"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
