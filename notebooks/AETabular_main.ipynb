{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r5OMfQfNaAm"
   },
   "source": [
    "#**Licença de Uso**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d22_lVKTNenJ"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "→ You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution–NonCommercial 4.0 International License.\n",
    "\n",
    "→ You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**© 2025 Leandro Bernardo Rodrigues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRNqsqxZWPzs"
   },
   "source": [
    "# **Gestão do Ambiente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clC8wQo4V2_j"
   },
   "source": [
    "##**Criar repositório .git no Colab**\n",
    "---\n",
    "Google Drive é considerado o ponto de verdade\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1760321535671,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "wQi7bUIQUmAr",
    "outputId": "10878a69-6039-4686-e83c-beead081416d"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# parágrafo git: inicialização do repositório no drive e push inicial para o github\n",
    "\n",
    "# imports\n",
    "from pathlib import Path\n",
    "import subprocess, os, sys, getpass, textwrap\n",
    "\n",
    "# util de shell\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout, r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "# garantir que o diretório do projeto exista\n",
    "repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# montar drive no colab se necessário\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive  # type: ignore\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        print(\"montando google drive…\")\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# configurar safe.directory para evitar avisos do git com caminhos de rede\n",
    "try:\n",
    "    sh([\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", str(repo_dir)])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# inicializar repositório se ainda não existir\n",
    "if not (repo_dir / \".git\").exists():\n",
    "    print(\"inicializando repositório git…\")\n",
    "    sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "    # garantir branch principal como main (compatível com versões antigas)\n",
    "    try:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"branch\", \"-M\", default_branch], cwd=repo_dir)\n",
    "else:\n",
    "    print(\".git já existe; seguindo\")\n",
    "\n",
    "# configurar identidade local\n",
    "sh([\"git\", \"config\", \"user.name\", author_name], cwd=repo_dir)\n",
    "sh([\"git\", \"config\", \"user.email\", author_email], cwd=repo_dir)\n",
    "\n",
    "# criar .gitignore básico e readme se estiverem ausentes\n",
    "gitignore_path = repo_dir / \".gitignore\"\n",
    "if not gitignore_path.exists():\n",
    "    gitignore_path.write_text(textwrap.dedent(\"\"\"\n",
    "      # python\n",
    "      __pycache__/\n",
    "      *.py[cod]\n",
    "      *.egg-info/\n",
    "      .venv*/\n",
    "      venv/\n",
    "\n",
    "      # segredos\n",
    "      .env\n",
    "      *.key\n",
    "      *.pem\n",
    "      *.tok\n",
    "\n",
    "      # jupyter/colab\n",
    "      .ipynb_checkpoints/\n",
    "\n",
    "      # artefatos e dados locais (não versionar)\n",
    "      data/\n",
    "      input/                 # inclui input.csv sensível\n",
    "      output/\n",
    "      runs/\n",
    "      logs/\n",
    "      figures/\n",
    "      *.log\n",
    "      *.tmp\n",
    "      *.bak\n",
    "      *.png\n",
    "      *.jpg\n",
    "      *.pdf\n",
    "      *.html\n",
    "\n",
    "      # allowlist para a pasta de referências\n",
    "      !references/\n",
    "      !references/**\n",
    "    \"\"\").strip() + \"\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado .gitignore\")\n",
    "\n",
    "readme_path = repo_dir / \"README.md\"\n",
    "if not readme_path.exists():\n",
    "    readme_path.write_text(f\"# {repo_name}\\n\\nprojeto de autoencoder tabular para journal entries.\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado README.md\")\n",
    "\n",
    "# configurar remoto origin\n",
    "remote_base = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "existing_remotes = sh([\"git\", \"remote\"], cwd=repo_dir)\n",
    "if \"origin\" not in existing_remotes.split():\n",
    "    sh([\"git\", \"remote\", \"add\", \"origin\", remote_base], cwd=repo_dir)\n",
    "    print(f\"remoto origin adicionado: {remote_base}\")\n",
    "else:\n",
    "    # se já existe, garantir que aponta para o repo correto\n",
    "    current_url = sh([\"git\", \"remote\", \"get-url\", \"origin\"], cwd=repo_dir)\n",
    "    if current_url != remote_base:\n",
    "        sh([\"git\", \"remote\", \"set-url\", \"origin\", remote_base], cwd=repo_dir)\n",
    "        print(f\"remoto origin atualizado para: {remote_base}\")\n",
    "    else:\n",
    "        print(\"remoto origin já configurado corretamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-aIMDUNNhfi"
   },
   "source": [
    "##**Utilitário:** verificação da formatação de código\n",
    "\n",
    "Black [88] + Isort, desconsiderando células mágicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Jfa9X-d-Kfdn"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0001\n",
    "#pré-visualizar/aplicar (pula magics) — isort(profile=black)+black(88) { display-mode: \"form\" }\n",
    "import sys, subprocess, os, re, difflib, textwrap, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===== CONFIG =====\n",
    "NOTEBOOK = \"/content/drive/MyDrive/Notebooks/data-analysis/notebooks/AETabular_main.ipynb\"  # <- ajuste\n",
    "LINE_LENGTH = 88\n",
    "# ==================\n",
    "\n",
    "# 1) Instalar libs no MESMO Python do kernel\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"black\", \"isort\", \"nbformat\"])\n",
    "\n",
    "import nbformat\n",
    "import black\n",
    "import isort\n",
    "\n",
    "BLACK_MODE = black.Mode(line_length=LINE_LENGTH)\n",
    "ISORT_CFG  = isort.Config(profile=\"black\", line_length=LINE_LENGTH)\n",
    "\n",
    "# 2) Regras para pular células com magics/shell\n",
    "#   - linhas começando com %, %%, !\n",
    "#   - chamadas a get_ipython(\n",
    "MAGIC_LINE = re.compile(r\"^\\s*(%{1,2}|!)\", re.M)\n",
    "GET_IPY    = re.compile(r\"get_ipython\\s*\\(\")\n",
    "\n",
    "def has_magics(code: str) -> bool:\n",
    "    return bool(MAGIC_LINE.search(code) or GET_IPY.search(code))\n",
    "\n",
    "def format_code(code: str) -> str:\n",
    "    # isort primeiro, depois black\n",
    "    sorted_code = isort.api.sort_code_string(code, config=ISORT_CFG)\n",
    "    return black.format_str(sorted_code, mode=BLACK_MODE)\n",
    "\n",
    "def summarize_diff(diff_lines: List[str]) -> Tuple[int, int]:\n",
    "    added = removed = 0\n",
    "    for ln in diff_lines:\n",
    "        # ignorar cabeçalhos do diff\n",
    "        if ln.startswith((\"---\", \"+++\", \"@@\")):\n",
    "            continue\n",
    "        if ln.startswith(\"+\"):\n",
    "            added += 1\n",
    "        elif ln.startswith(\"-\"):\n",
    "            removed += 1\n",
    "    return added, removed\n",
    "\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(title)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "if not os.path.exists(NOTEBOOK):\n",
    "    raise FileNotFoundError(f\"Notebook não encontrado:\\n{NOTEBOOK}\")\n",
    "\n",
    "# 3) Leitura do .ipynb\n",
    "with open(NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "changed_cells = []  # (idx, added, removed, diff_text, preview_snippet, new_code)\n",
    "\n",
    "# 4) Pré-visualização célula a célula\n",
    "header(\"Pré-visualização (NÃO grava) — somente células com mudanças\")\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "\n",
    "    original = cell.get(\"source\", \"\")\n",
    "    if not original.strip():\n",
    "        continue\n",
    "\n",
    "    # Pular células com magics/shell\n",
    "    if has_magics(original):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formatted = format_code(original)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] célula {i}: erro no formatador — pulando ({e})\")\n",
    "        continue\n",
    "\n",
    "    if original.strip() != formatted.strip():\n",
    "        # Gerar diff unificado legível\n",
    "        diff = list(difflib.unified_diff(\n",
    "            original.splitlines(), formatted.splitlines(),\n",
    "            fromfile=f\"cell_{i}:before\", tofile=f\"cell_{i}:after\", lineterm=\"\"\n",
    "        ))\n",
    "        add, rem = summarize_diff(diff)\n",
    "        snippet = original.strip().splitlines()[0][:120] if original.strip().splitlines() else \"<célula vazia>\"\n",
    "        changed_cells.append((i, add, rem, \"\\n\".join(diff), snippet, formatted))\n",
    "\n",
    "# 5) Exibição dos diffs por célula (se houver)\n",
    "if not changed_cells:\n",
    "    print(\"✔ Nada a alterar: todas as células (não mágicas) já estão conforme isort/black.\")\n",
    "else:\n",
    "    total_add = total_rem = 0\n",
    "    for (idx, add, rem, diff_text, snippet, _new) in changed_cells:\n",
    "        total_add += add\n",
    "        total_rem += rem\n",
    "        header(f\"Diff — Célula #{idx}  (+{add}/-{rem})\")\n",
    "        print(f\"Primeira linha da célula: {snippet!r}\\n\")\n",
    "        print(diff_text)\n",
    "\n",
    "    header(\"Resumo\")\n",
    "    print(f\"Células com mudanças: {len(changed_cells)}\")\n",
    "    print(f\"Linhas adicionadas:   {total_add}\")\n",
    "    print(f\"Linhas removidas:     {total_rem}\")\n",
    "\n",
    "# 6) Perguntar se aplica\n",
    "if changed_cells:\n",
    "    print(\"\\nDigite 'p' para **Proceder** e gravar as mudanças nessas células, ou 'c' para **Cancelar**.\")\n",
    "    try:\n",
    "        choice = input(\"Proceder (p) / Cancelar (c): \").strip().lower()\n",
    "    except Exception:\n",
    "        choice = \"c\"\n",
    "\n",
    "    if choice == \"p\":\n",
    "        # Backup antes de escrever\n",
    "        backup = NOTEBOOK + \".bak\"\n",
    "        if not os.path.exists(backup):\n",
    "            with open(backup, \"w\", encoding=\"utf-8\") as bf:\n",
    "                nbformat.write(nb, bf)\n",
    "\n",
    "        # Aplicar somente nas células com mudanças\n",
    "        idx_to_new = {idx: new for (idx, _a, _r, _d, _s, new) in changed_cells}\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if i in idx_to_new and cell.get(\"cell_type\") == \"code\":\n",
    "                cell[\"source\"] = idx_to_new[i]\n",
    "\n",
    "        # Escrever no .ipynb\n",
    "        with open(NOTEBOOK, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "\n",
    "        # Sync delay (Drive)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        header(\"Concluído\")\n",
    "        print(f\"✔ Mudanças aplicadas em {len(changed_cells)} célula(s).\")\n",
    "        print(f\"Backup criado em: {backup}\")\n",
    "        print(\"Dica: recarregue o notebook no Colab para ver a formatação atualizada.\")\n",
    "    else:\n",
    "        print(\"\\nOperação cancelada. Nada foi gravado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwzVcwk9Ol0K"
   },
   "source": [
    "##**Sincronizar alterações no código do projeto**\n",
    "Comandos para sincronizar código (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive é considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16371,
     "status": "ok",
     "timestamp": 1760321482679,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "2hJZaAa2OqEp",
    "outputId": "dced9304-82d5-41e0-baf7-8fde04ec8676"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive é a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de versão (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; só 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_AETABULAR (fallback: env; último caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#utilitários silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heurística curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permissões/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local inválido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#configurações do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do repositório no GitHub\n",
    "repo_name      = \"ae-tabular\"    # nome do repositório\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive não montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se é um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta operações pendentes (não apaga histórico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #força branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diretório como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necessário)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normalização\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos já rastreados que hoje são ignorados pelo .gitignore,\n",
    "    limpa o índice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contrário.\n",
    "    \"\"\"\n",
    "    #remove lock de índice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o índice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que estão rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detecção? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de versão\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato legível + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autenticação (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_AETABULAR')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - variável de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_AETABULAR\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#listas de força\n",
    "FORCE_UNTRACK = [\"input/\", \"output/\", \"data/\", \"runs/\", \"logs/\", \"figures/\"]\n",
    "FORCE_TRACK   = [\"references/\"]  #versionar tudo dentro (PDFs inclusive)\n",
    "\n",
    "def force_index_rules():\n",
    "    #garante que pastas sensíveis NUNCA fiquem rastreadas\n",
    "    for p in FORCE_UNTRACK:\n",
    "        try:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "    #garante que references/ SEMPRE entre (útil se ainda há *.pdf globais)\n",
    "    for p in FORCE_TRACK:\n",
    "        try:\n",
    "            git(\"add\", \"-f\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de versão (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudança: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #versão\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma única vez, se necessário)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #aplica regras de força\n",
    "        force_index_rules()\n",
    "\n",
    "        #stage de tudo (Drive é a verdade; remoções entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se só a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se já existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive é a verdade): validação + push forçado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # você é o owner; não perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se não existir (repo vazio), faz um probe genérico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #repositório pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push forçado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. Versão atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem única, curta, sem detalhes sensíveis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt9CwUvIWJ4-"
   },
   "source": [
    "#**Projeto**\n",
    "Comandos para sincronizar código (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive é considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOA1JgyIPBYj"
   },
   "source": [
    "## **Etapa 1:** Ativação do ambiente virtual\n",
    "---\n",
    "Monta o Google Drive, define a BASE e REPO do projeto Git, cria/ativa o ambiente virtual.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "6rreQXmSPFlE"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0003\n",
    "#inicialização robusta: Drive + venv fora do Drive + Git checks (com patch de venv/ensurepip) { display-mode: \"form\" }\n",
    "#força clear do kernel/variáveis desta sessão\n",
    "%reset -f\n",
    "\n",
    "#imports básicos -----\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML\n",
    "import json, os, sys, time, shutil, pathlib, subprocess\n",
    "\n",
    "#helper de subprocess -----\n",
    "def run(cmd, check=True, cwd=None):\n",
    "    r = subprocess.run(cmd, text=True, capture_output=True, cwd=cwd)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout + r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "#funções utilitárias de Drive/FS -----\n",
    "def _is_mount_active(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"verifica em /proc/mounts se o mountpoint está realmente montado\"\"\"\n",
    "    try:\n",
    "        with open(\"/proc/mounts\", \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if len(parts) > 1 and parts[1] == mountpoint:\n",
    "                    return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def _cleanup_local_mountpoint(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"limpa conteúdo local do mountpoint quando NÃO está montado\"\"\"\n",
    "    if os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
    "        print(f\"[info] mountpoint '{mountpoint}' contém arquivos locais. limpando...\")\n",
    "        for name in os.listdir(mountpoint):\n",
    "            p = os.path.join(mountpoint, name)\n",
    "            try:\n",
    "                if os.path.isfile(p) or os.path.islink(p):\n",
    "                    os.remove(p)\n",
    "                else:\n",
    "                    shutil.rmtree(p)\n",
    "            except Exception as e:\n",
    "                print(f\"[aviso] não foi possível remover {p}: {e}\")\n",
    "        print(\"[ok] limpeza concluída.\")\n",
    "\n",
    "def safe_mount_google_drive(preferred_mountpoint: str = \"/content/drive\", readonly: bool = False, timeout_ms: int = 120000):\n",
    "    \"\"\"desmonta se preciso, limpa o mountpoint local e monta o drive\"\"\"\n",
    "    try:\n",
    "        if _is_mount_active(preferred_mountpoint):\n",
    "          # print(\"[info] drive montado. tentando desmontar...\")\n",
    "          drive.flush_and_unmount()\n",
    "          for _ in range(50):\n",
    "              if not _is_mount_active(preferred_mountpoint):\n",
    "                  break\n",
    "              time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not _is_mount_active(preferred_mountpoint):\n",
    "        _cleanup_local_mountpoint(preferred_mountpoint)\n",
    "\n",
    "    os.makedirs(preferred_mountpoint, exist_ok=True)\n",
    "    if os.listdir(preferred_mountpoint):\n",
    "        alt = \"/mnt/drive\"\n",
    "        print(f\"[aviso] '{preferred_mountpoint}' ainda não está vazio. usando alternativo '{alt}'.\")\n",
    "        os.makedirs(alt, exist_ok=True)\n",
    "        mountpoint = alt\n",
    "    else:\n",
    "        mountpoint = preferred_mountpoint\n",
    "\n",
    "    print(f\"[info] montando o google drive em '{mountpoint}'...\")\n",
    "    drive.mount(mountpoint, force_remount=True, timeout_ms=timeout_ms, readonly=readonly)\n",
    "    print(\"[ok]   drive montado com sucesso.\")\n",
    "    return mountpoint\n",
    "\n",
    "def safe_chdir(path):\n",
    "    \"\"\"usa os.chdir com validações, evitando %cd\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"caminho não existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"[ok]   diretório atual:\", os.getcwd())\n",
    "\n",
    "#parâmetros do projeto -----\n",
    "GITHUB_OWNER = \"LeoBR84p\"\n",
    "GITHUB_REPO  = \"ae-tabular\"\n",
    "CLEAN_URL    = f\"https://github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "#montar/remontar o google drive (robusto)\n",
    "MOUNTPOINT = safe_mount_google_drive(\"/content/drive\")\n",
    "BASE = f\"{MOUNTPOINT}/MyDrive/Notebooks\"  #ajuste se quiser\n",
    "REPO = \"ae-tabular\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "#venv fora do drive (mais rápido e evita sync)\n",
    "VENV_PATH = \"/content/.venv_data\"\n",
    "VENV_BIN  = f\"{VENV_PATH}/bin\"\n",
    "VENV_PY   = f\"{VENV_BIN}/python\"\n",
    "VENV_PIP  = f\"{VENV_BIN}/pip\"   #pode não existir ainda se o venv foi criado sem pip\n",
    "\n",
    "#criação do venv com fallback para 'virtualenv'\n",
    "def create_or_repair_venv(venv_path: str, venv_python: str):\n",
    "    if not os.path.exists(VENV_BIN):\n",
    "        #print(f\"[info] criando venv (stdlib) em {venv_path} --without-pip ...\")\n",
    "        try:\n",
    "            run([sys.executable, \"-m\", \"venv\", \"--without-pip\", venv_path], check=True)\n",
    "            print(\"[ok]   venv criado (sem pip).\")\n",
    "        except Exception as e:\n",
    "            print(f\"[aviso] venv(stdlib) falhou: {e}\")\n",
    "            #print(\"[info] instalando 'virtualenv' e criando venv alternativo com pip embutido...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", sys.executable, venv_path], check=True)\n",
    "            print(\"[ok]   venv criado via virtualenv.\")\n",
    "    else:\n",
    "        print(f\"[ok]   venv já existe em {venv_path}\")\n",
    "\n",
    "create_or_repair_venv(VENV_PATH, VENV_PY)\n",
    "\n",
    "#ajusta PATH antes de qualquer instalação\n",
    "os.environ[\"PATH\"] = f\"{VENV_BIN}{os.pathsep}{os.environ['PATH']}\"\n",
    "os.environ[\"VIRTUAL_ENV\"] = VENV_PATH\n",
    "print(\"[ok]   venv adicionado ao PATH\")\n",
    "\n",
    "#garante pip dentro do venv (ensurepip -> fallback virtualenv)\n",
    "def _ensure_pip_in_venv(vpy: str):\n",
    "    try:\n",
    "        run([vpy, \"-m\", \"pip\", \"--version\"], check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        #print(\"[info] pip ausente no venv. tentando ensurepip dentro do venv...\")\n",
    "        try:\n",
    "            run([vpy, \"-m\", \"ensurepip\", \"--upgrade\", \"--default-pip\"], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            #print(f\"[aviso] ensurepip no venv falhou: {e}\")\n",
    "            #print(\"[info] fallback: usando virtualenv para semear o pip dentro do venv existente...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", vpy, VENV_PATH], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "\n",
    "if not _ensure_pip_in_venv(VENV_PY):\n",
    "    raise RuntimeError(\"não foi possível provisionar o pip dentro do venv\")\n",
    "\n",
    "# garante que os pacotes instalados no venv sejam visíveis para este kernel\n",
    "_ver = subprocess.check_output([VENV_PY, \"-c\", \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\"], text=True).strip()\n",
    "_site_dir = f\"{VENV_PATH}/lib/python{_ver}/site-packages\"\n",
    "if _site_dir not in sys.path:\n",
    "    sys.path.insert(0, _site_dir)\n",
    "print(\"[ok]   site-packages do venv adicionado ao sys.path:\", _site_dir)\n",
    "\n",
    "#instala dependências de sessão DENTRO do venv\n",
    "print(\"[info] instalando pacotes no venv...\")\n",
    "run([VENV_PY, \"-m\", \"pip\", \"install\", \"-q\", \"jupytext\", \"nbdime\", \"nbstripout\"])\n",
    "\n",
    "#habilita integração do nbdime com git (global)\n",
    "print(\"[info] habilitando nbdime em git config --global ...\")\n",
    "run([VENV_PY, \"-m\", \"nbdime\", \"config-git\", \"--enable\", \"--global\"])\n",
    "\n",
    "#checks do repositório git + navegação até a pasta do projeto\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"[aviso] pasta do projeto não encontrada em {PROJ}.\")\n",
    "else:\n",
    "    print(\"[ok]   pasta do projeto encontrada.\")\n",
    "    safe_chdir(PROJ)\n",
    "    if not os.path.isdir(\".git\"):\n",
    "        print(\"[aviso] esta pasta não parece ser um repositório Git (.git ausente).\")\n",
    "    else:\n",
    "        print(\"[ok]   repositório Git detectado.\")\n",
    "\n",
    "# resumo do ambiente (confirmação objetiva e detalhada)\n",
    "kernel_py = sys.executable\n",
    "venv_py = VENV_PY\n",
    "site_dir = _site_dir\n",
    "\n",
    "# verifica se o site-packages do venv está no sys.path\n",
    "site_ok = site_dir in sys.path\n",
    "\n",
    "# obtém versões e caminhos\n",
    "try:\n",
    "    py_ver = subprocess.check_output([venv_py, \"-V\"], text=True).strip()\n",
    "    pip_ver = subprocess.check_output([venv_py, \"-m\", \"pip\", \"--version\"], text=True).strip()\n",
    "    pip_path = subprocess.check_output(\n",
    "        [venv_py, \"-m\", \"pip\", \"show\", \"pip\"], text=True, stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    pip_path_line = next((l for l in pip_path.splitlines() if l.startswith(\"Location:\")), \"\")\n",
    "except subprocess.CalledProcessError:\n",
    "    py_ver, pip_ver, pip_path_line = \"erro\", \"erro\", \"\"\n",
    "\n",
    "# imprime status linha a linha\n",
    "print(f\"[ok]   venv habilitado\" if venv_py else \"[erro] venv não encontrado\")\n",
    "print(f\"[info] python em uso: {kernel_py}\")\n",
    "print(f\"[info] versão do python: {py_ver}\")\n",
    "print(f\"[ok]   pip do venv ativo\" if \"pip\" in pip_ver.lower() else \"[erro] pip do venv não detectado\")\n",
    "print(f\"[info] caminho do pip: {venv_py.replace('python','pip')}\")\n",
    "print(f\"[ok]   site-packages no sys.path: {site_dir}\" if site_ok else f\"[erro] site-packages ausente no sys.path: {site_dir}\")\n",
    "print(f\"[info] versão do pip: {pip_ver}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>🤖 Skynet</b>: T-800 ativado. Diagnóstico do ambiente concluído. '\n",
    "             '🎯 Alvo principal: organização do notebook e venv fora do drive.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXFHi64oRHjs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1760320462073,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "a0dv1bqORH_w",
    "outputId": "3077366d-5003-4032-d579-a755b99957e3"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID003\n",
    "# imports principais\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import json, os, getpass, platform, subprocess, sys\n",
    "\n",
    "# configurações do projeto (já fornecidas)\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do repositório no GitHub\n",
    "repo_name      = \"ae-tabular\"       # nome do repositório\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "# zona de tempo brasilia\n",
    "TZ_BR = ZoneInfo(\"America/Sao_Paulo\")\n",
    "\n",
    "def skynet(msg: str):\n",
    "    \"\"\"log simples padronizado\"\"\"\n",
    "    ts = datetime.now(TZ_BR).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[skynet {ts}] {msg}\")\n",
    "\n",
    "# 1) montar o google drive se necessário\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive  # type: ignore\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        skynet(\"montando google drive…\")\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "    else:\n",
    "        skynet(\"google drive já montado\")\n",
    "except Exception:\n",
    "    skynet(\"ambiente não é colab ou google drive indisponível, prosseguindo assim mesmo\")\n",
    "\n",
    "# 2) criar estrutura de pastas do projeto\n",
    "INPUT_DIR  = repo_dir / \"input\"\n",
    "OUTPUT_DIR = repo_dir / \"output\"\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 3) criar subpasta de execução em output com carimbo de data e hora de brasília\n",
    "run_stamp = datetime.now(TZ_BR).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "RUN_DIR = OUTPUT_DIR / run_stamp\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "# 4) opcional: escrever metadados mínimos da execução\n",
    "run_meta = {\n",
    "    \"author_name\": author_name,\n",
    "    \"author_email\": author_email,\n",
    "    \"project\": repo_name,\n",
    "    \"run_stamp_br\": run_stamp,\n",
    "    \"timezone\": \"America/Sao_Paulo\",\n",
    "    \"python_version\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"uid\": getpass.getuser(),\n",
    "}\n",
    "with open(RUN_DIR / \"run.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 5) git opcional: configurar identidade local se este for um repositório git\n",
    "def _git(cmd, cwd=None):\n",
    "    return subprocess.run([\"git\", *cmd], cwd=cwd, text=True, capture_output=True)\n",
    "\n",
    "if (repo_dir / \".git\").exists():\n",
    "    _git([\"config\", \"user.name\", author_name], cwd=repo_dir)\n",
    "    _git([\"config\", \"user.email\", author_email], cwd=repo_dir)\n",
    "    skynet(\"git detectado e identidade configurada no repositório\")\n",
    "else:\n",
    "    skynet(\"repositório git não detectado em repo_dir, etapa git será ignorada por enquanto\")\n",
    "\n",
    "# 6) impressão de caminhos úteis\n",
    "skynet(\"ambiente preparado com sucesso\")\n",
    "print(\"repo_dir:   \", repo_dir)\n",
    "print(\"input_dir:  \", INPUT_DIR)\n",
    "print(\"output_dir: \", OUTPUT_DIR)\n",
    "print(\"run_dir:    \", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1yqyFuEYPbL"
   },
   "source": [
    "## **Etapa 2:** Ingestão de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32304,
     "status": "ok",
     "timestamp": 1760322681145,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "lLmTZYkJZtEu",
    "outputId": "87a358d7-6613-4014-cfa3-7a4dcf93f838"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# parágrafo 2: ingestão de journal entries (csv utf-8 bom ;), seleção por índice e simulação 50k\n",
    "\n",
    "# imports\n",
    "import os, sys, json, math, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dependências do parágrafo 1\n",
    "assert 'RUN_DIR' in globals() and 'repo_dir' in globals(), \"execute o parágrafo 1 antes.\"\n",
    "assert 'INPUT_DIR' in globals() and 'OUTPUT_DIR' in globals(), \"execute o parágrafo 1 antes.\"\n",
    "assert 'TZ_BR' in globals() and callable(skynet), \"execute o parágrafo 1 antes.\"\n",
    "\n",
    "# parâmetros de leitura obrigatórios\n",
    "CSV_ENCODING = \"utf-8-sig\"   # utf-8 com bom\n",
    "CSV_SEP      = \";\"           # separador ponto e vírgula\n",
    "\n",
    "# colunas esperadas\n",
    "REQUIRED_COLS = [\n",
    "    \"username\",        # nome do usuário\n",
    "    \"lotacao\",         # unidade/lotação\n",
    "    \"tipoconta\",       # Ativo, Passivo, Resultado, Receita, Despesa, Outras\n",
    "    \"valormi\",         # valor em reais com 2 casas decimais\n",
    "    \"dc\",              # 'd' para débito, 'c' para crédito\n",
    "    \"contacontabil\",   # conta contábil COSIF (número)\n",
    "]\n",
    "\n",
    "# categorias válidas\n",
    "TIPOCONTA_VALIDAS = {\"Ativo\",\"Passivo\",\"Resultado\",\"Receita\",\"Despesa\",\"Outras\"}\n",
    "DC_VALIDOS        = {\"d\",\"c\"}\n",
    "\n",
    "# utilitário: listar csvs no google drive do usuário (mydrive), ordenados por data desc\n",
    "def listar_csvs_mydrive(max_files=2000):\n",
    "    raiz = Path(\"/content/drive/MyDrive\")\n",
    "    arquivos = []\n",
    "    for p in raiz.rglob(\"*.csv\"):\n",
    "        try:\n",
    "            stat = p.stat()\n",
    "            arquivos.append((p, stat.st_mtime, stat.st_size))\n",
    "            if len(arquivos) >= max_files:\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    # ordena por mtime desc\n",
    "    arquivos.sort(key=lambda x: x[1], reverse=True)\n",
    "    return arquivos\n",
    "\n",
    "# utilitário: impressão amigável de bytes\n",
    "def _fmt_size(n):\n",
    "    for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if n < 1024:\n",
    "            return f\"{n:.1f}{u}\"\n",
    "        n /= 1024\n",
    "    return f\"{n:.1f}PB\"\n",
    "\n",
    "# geração de dataset simulado (50k linhas) balanceado por partidas dobradas\n",
    "def gerar_csv_simulado_50k(dest_dir: Path) -> Path:\n",
    "    # mapeia tipoconta -> prefixo cosif plausível\n",
    "    cosif_prefix = {\n",
    "        \"Ativo\": \"1\",\n",
    "        \"Passivo\": \"2\",\n",
    "        \"Resultado\": \"3\",\n",
    "        \"Receita\": \"7\",\n",
    "        \"Despesa\": \"8\",\n",
    "        \"Outras\": \"9\",\n",
    "    }\n",
    "    # listas de apoio\n",
    "    lotacoes = [f\"UNID-{i:03d}\" for i in range(1, 121)]\n",
    "    usuarios = [f\"user{i:04d}\" for i in range(1, 3001)]\n",
    "    tipos    = list(cosif_prefix.keys())\n",
    "    # número de pares (d,c)\n",
    "    n_pairs = 25_000  # 50k linhas\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    rows = []\n",
    "    for _ in range(n_pairs):\n",
    "        # valor do par, duas casas\n",
    "        base_val = float(np.round(rng.uniform(10.0, 50_000.0), 2))\n",
    "        # pequena variação entre d e c para simular centavos e depois ajustar\n",
    "        d_val = float(np.round(base_val * rng.uniform(0.5, 1.5), 2))\n",
    "        c_val = d_val  # assegura partida dobrada perfeita\n",
    "\n",
    "        # escolhe atributos débito\n",
    "        t_d  = random.choice(tipos)\n",
    "        lot_d = random.choice(lotacoes)\n",
    "        usr_d = random.choice(usuarios)\n",
    "        cc_d  = f\"{cosif_prefix[t_d]}{rng.integers(0, 10_000_000):07d}\"\n",
    "\n",
    "        # escolhe atributos crédito\n",
    "        t_c  = random.choice(tipos)\n",
    "        lot_c = random.choice(lotacoes)\n",
    "        usr_c = random.choice(usuarios)\n",
    "        cc_c  = f\"{cosif_prefix[t_c]}{rng.integers(0, 10_000_000):07d}\"\n",
    "\n",
    "        rows.append((usr_d, lot_d, t_d, d_val, \"d\", cc_d))\n",
    "        rows.append((usr_c, lot_c, t_c, c_val, \"c\", cc_c))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=REQUIRED_COLS)\n",
    "\n",
    "    # garante duas casas decimais no export; manteremos float em memória\n",
    "    stamp = datetime.now(TZ_BR).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = dest_dir / f\"journal_simulado_{stamp}.csv\"\n",
    "\n",
    "    # exporta com utf-8-sig e separador ';'\n",
    "    # formata valormi com duas casas decimais\n",
    "    df_fmt = df.copy()\n",
    "    df_fmt[\"valormi\"] = df_fmt[\"valormi\"].map(lambda x: f\"{x:.2f}\")\n",
    "    df_fmt.to_csv(out_path, index=False, sep=CSV_SEP, encoding=CSV_ENCODING)\n",
    "\n",
    "    return out_path\n",
    "\n",
    "# etapa a) gerar um csv simulado em INPUT_DIR para protótipo\n",
    "sim_path = gerar_csv_simulado_50k(INPUT_DIR)\n",
    "skynet(f\"csv simulado gerado: {sim_path}\")\n",
    "\n",
    "# etapa b) listar csvs do mydrive para seleção por índice\n",
    "skynet(\"varrendo csvs em /content/drive/MyDrive (isso pode levar algum tempo em drives grandes)\")\n",
    "csvs = listar_csvs_mydrive(max_files=2000)\n",
    "\n",
    "# inclui o simulado recém-gerado no topo da lista para facilitar\n",
    "csvs = [(sim_path, sim_path.stat().st_mtime, sim_path.stat().st_size)] + csvs\n",
    "\n",
    "# imprime lista\n",
    "print(\"\\narquivos csv encontrados (índice, tamanho, caminho):\")\n",
    "for i, (p, mtime, sz) in enumerate(csvs):\n",
    "    print(f\"[{i:03d}] {_fmt_size(sz):>8}  {str(p)}\")\n",
    "\n",
    "# seleção por índice\n",
    "while True:\n",
    "    try:\n",
    "        sel = input(\"\\ndigite o índice do csv desejado e pressione enter: \").strip()\n",
    "        idx = int(sel)\n",
    "        assert 0 <= idx < len(csvs)\n",
    "        SELECTED_CSV = Path(csvs[idx][0])\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"índice inválido. tente novamente. erro: {e}\")\n",
    "\n",
    "skynet(f\"arquivo selecionado: {SELECTED_CSV}\")\n",
    "\n",
    "# etapa c) validar formato (encoding/delimitador/colunas) e carregar dataframe\n",
    "def carregar_validar_csv(path: Path) -> pd.DataFrame:\n",
    "    # leitura com requisitos obrigatórios\n",
    "    df = pd.read_csv(path, sep=CSV_SEP, encoding=CSV_ENCODING, dtype=str)\n",
    "    # remove espaços nas colunas e valores\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # checa colunas requeridas\n",
    "    faltantes = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if faltantes:\n",
    "        raise ValueError(f\"colunas ausentes: {faltantes}. esperado: {REQUIRED_COLS}\")\n",
    "\n",
    "    # tipagem: valormi -> float com duas casas (ponto decimal)\n",
    "    # se vier com vírgula decimal por engano, converte\n",
    "    def _parse_val(v):\n",
    "        v = (v or \"\").replace(\".\", \"\").replace(\",\", \".\") if isinstance(v, str) and v.count(\",\")==1 and v.count(\".\")>1 else v\n",
    "        v = (v or \"\").replace(\",\", \".\") if isinstance(v, str) else v\n",
    "        try:\n",
    "            return round(float(v), 2)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"valormi\"] = df[\"valormi\"].apply(_parse_val).astype(float)\n",
    "\n",
    "    # normaliza dc\n",
    "    df[\"dc\"] = df[\"dc\"].str.lower()\n",
    "    # normaliza tipoconta (capitalização 1ª maiúscula)\n",
    "    df[\"tipoconta\"] = df[\"tipoconta\"].str.capitalize()\n",
    "\n",
    "    # validações de conteúdo\n",
    "    problemas = {}\n",
    "    if not set(REQUIRED_COLS).issubset(df.columns):\n",
    "        problemas[\"colunas_invalidas\"] = list(set(REQUIRED_COLS) - set(df.columns))\n",
    "    if (~df[\"dc\"].isin(DC_VALIDOS)).any():\n",
    "        problemas[\"dc_invalido\"] = int((~df[\"dc\"].isin(DC_VALIDOS)).sum())\n",
    "    if (~df[\"tipoconta\"].isin(TIPOCONTA_VALIDAS)).any():\n",
    "        problemas[\"tipoconta_invalida\"] = int((~df[\"tipoconta\"].isin(TIPOCONTA_VALIDAS)).sum())\n",
    "    if df[\"valormi\"].isna().any():\n",
    "        problemas[\"valormi_na\"] = int(df[\"valormi\"].isna().sum())\n",
    "    if (df[\"valormi\"] < 0).any():\n",
    "        problemas[\"valormi_negativo\"] = int((df[\"valormi\"] < 0).sum())\n",
    "\n",
    "    # contacontabil: manter como string numérica\n",
    "    df[\"contacontabil\"] = df[\"contacontabil\"].astype(str)\n",
    "    if (~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).any():\n",
    "        problemas[\"contacontabil_nao_numerica\"] = int((~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).sum())\n",
    "\n",
    "    if problemas:\n",
    "        report_path = RUN_DIR / \"validacao_ingestao.json\"\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(problemas, f, ensure_ascii=False, indent=2)\n",
    "        raise ValueError(f\"falhas de validação encontradas. veja {report_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "DF_RAW = carregar_validar_csv(SELECTED_CSV)\n",
    "skynet(f\"csv carregado com sucesso: {len(DF_RAW):,} linhas\")\n",
    "\n",
    "# etapa d) persistir cópia da fonte e snapshot parquet no run_dir\n",
    "# cópia padronizada do csv selecionado para rastreabilidade\n",
    "src_copy = RUN_DIR / f\"selected_source.csv\"\n",
    "if SELECTED_CSV.resolve() != src_copy.resolve():\n",
    "    try:\n",
    "        # reexporta com formatação padronizada garantida (utf-8-sig ; e valormi com 2 casas)\n",
    "        df_exp = DF_RAW.copy()\n",
    "        df_exp[\"valormi\"] = df_exp[\"valormi\"].map(lambda x: f\"{x:.2f}\")\n",
    "        df_exp.to_csv(src_copy, index=False, sep=CSV_SEP, encoding=CSV_ENCODING)\n",
    "        skynet(f\"fonte padronizada salva em {src_copy}\")\n",
    "    except Exception as e:\n",
    "        skynet(f\"aviso: não foi possível salvar cópia padronizada do csv ({e})\")\n",
    "\n",
    "# salva snapshot parquet para processamento rápido nas próximas fases\n",
    "snap_path = RUN_DIR / \"journal_entries.parquet\"\n",
    "DF_RAW.to_parquet(snap_path, index=False)\n",
    "skynet(f\"snapshot parquet salvo: {snap_path}\")\n",
    "\n",
    "# imprime um resumo inicial\n",
    "print(\"\\nvisão geral:\")\n",
    "print(DF_RAW.head(5))\n",
    "print(\"\\ncontagem por dc:\\n\", DF_RAW[\"dc\"].value_counts(dropna=False))\n",
    "print(\"\\ncontagem por tipoconta:\\n\", DF_RAW[\"tipoconta\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHnnmL-ebMGV"
   },
   "source": [
    "## **Etapa 3:** Limpeza, configurações e split (train/val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3474,
     "status": "ok",
     "timestamp": 1760323381443,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "olv2piVPbMRF",
    "outputId": "e1b18211-4caf-4feb-a1b5-25b1493e7ec1"
   },
   "outputs": [],
   "source": [
    "# parágrafo 3: limpeza, engenharia de features e split train/val\n",
    "\n",
    "import os, json, math, re, pickle, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dependências do parágrafo 1 e 2\n",
    "assert 'RUN_DIR' in globals() and 'snap_path' in globals() and 'skynet' in globals(), \"execute os parágrafos 1 e 2 antes.\"\n",
    "assert Path(snap_path).exists(), \"snapshot parquet ausente. finalize o parágrafo 2.\"\n",
    "\n",
    "# importa dados brutos do snapshot\n",
    "DF = pd.read_parquet(snap_path)\n",
    "\n",
    "# 1) limpeza básica\n",
    "# remove espaços extras novamente e normaliza casos (segurança)\n",
    "for c in DF.columns:\n",
    "    if DF[c].dtype == object:\n",
    "        DF[c] = DF[c].astype(str).str.strip()\n",
    "\n",
    "DF[\"dc\"] = DF[\"dc\"].str.lower().map({\"d\": \"d\", \"c\": \"c\"})  # garante domínio\n",
    "DF[\"tipoconta\"] = DF[\"tipoconta\"].str.capitalize()\n",
    "\n",
    "# garante numérico valormi (já veio como float do parágrafo 2)\n",
    "DF[\"valormi\"] = DF[\"valormi\"].astype(float)\n",
    "\n",
    "# remove linhas com falhas críticas\n",
    "mask_ok = (\n",
    "    DF[\"username\"].notna() &\n",
    "    DF[\"lotacao\"].notna() &\n",
    "    DF[\"tipoconta\"].notna() &\n",
    "    DF[\"dc\"].isin({\"d\",\"c\"}) &\n",
    "    DF[\"contacontabil\"].str.fullmatch(r\"\\d+\") &\n",
    "    DF[\"valormi\"].notna() & (DF[\"valormi\"] >= 0)\n",
    ")\n",
    "removed = (~mask_ok).sum()\n",
    "DF = DF.loc[mask_ok].reset_index(drop=True)\n",
    "skynet(f\"linhas removidas por validação adicional: {removed}\")\n",
    "\n",
    "# 2) features auxiliares\n",
    "# 2.1) decomposições COSIF (prefixos para hierarquia)\n",
    "DF[\"cosif_len\"] = DF[\"contacontabil\"].str.len().clip(upper=12).astype(int)\n",
    "DF[\"cosif_p1\"]  = DF[\"contacontabil\"].str[0]          # 1 dígito\n",
    "DF[\"cosif_p2\"]  = DF[\"contacontabil\"].str[:2]         # 2 dígitos\n",
    "DF[\"cosif_p3\"]  = DF[\"contacontabil\"].str[:3]         # 3 dígitos\n",
    "\n",
    "# 2.2) transformações em valor\n",
    "DF[\"valormi_log1p\"] = np.log1p(DF[\"valormi\"])\n",
    "# (opcional) normalizações por grupo virão como z-scores abaixo\n",
    "\n",
    "# 2.3) frequency encoding para categorias grandes\n",
    "def freq_encode(series: pd.Series) -> pd.Series:\n",
    "    freq = series.value_counts(dropna=False)\n",
    "    return series.map(freq).astype(float)\n",
    "\n",
    "DF[\"freq_username\"]      = freq_encode(DF[\"username\"])\n",
    "DF[\"freq_lotacao\"]       = freq_encode(DF[\"lotacao\"])\n",
    "DF[\"freq_contacontabil\"] = freq_encode(DF[\"contacontabil\"])\n",
    "DF[\"freq_cosif_p2\"]      = freq_encode(DF[\"cosif_p2\"])\n",
    "DF[\"freq_cosif_p3\"]      = freq_encode(DF[\"cosif_p3\"])\n",
    "\n",
    "# 2.4) estatísticas por grupo (média, desvio e z-score de valormi)\n",
    "def add_group_stats(df: pd.DataFrame, key: str, val_col: str = \"valormi\"):\n",
    "    g = df.groupby(key)[val_col].agg([\"mean\",\"std\",\"median\"]).rename(\n",
    "        columns={\"mean\":f\"{key}_mean\", \"std\":f\"{key}_std\", \"median\":f\"{key}_median\"}\n",
    "    )\n",
    "    df = df.join(g, on=key)\n",
    "    # evita divisão por zero\n",
    "    df[f\"{key}_std\"] = df[f\"{key}_std\"].replace(0, np.nan)\n",
    "    df[f\"{key}_z\"]   = (df[val_col] - df[f\"{key}_mean\"]) / df[f\"{key}_std\"]\n",
    "    df[f\"{key}_mad\"] = (df[val_col] - df[f\"{key}_median\"]).abs()\n",
    "    return df\n",
    "\n",
    "for k in [\"username\", \"lotacao\", \"contacontabil\", \"cosif_p2\", \"cosif_p3\"]:\n",
    "    DF = add_group_stats(DF, k, \"valormi\")\n",
    "\n",
    "# 2.5) codificação de baixo cardinalidade\n",
    "# dc: binária (d=1, c=0); tipoconta: one-hot\n",
    "DF[\"dc_bin\"] = DF[\"dc\"].map({\"d\":1, \"c\":0}).astype(int)\n",
    "tipos = sorted(DF[\"tipoconta\"].dropna().unique().tolist())\n",
    "tipodummies = pd.get_dummies(DF[\"tipoconta\"], prefix=\"tipo\", dtype=int)\n",
    "DF = pd.concat([DF, tipodummies], axis=1)\n",
    "\n",
    "# 3) matriz de features final\n",
    "num_cols = [\n",
    "    \"valormi\", \"valormi_log1p\",\n",
    "    \"freq_username\", \"freq_lotacao\", \"freq_contacontabil\", \"freq_cosif_p2\", \"freq_cosif_p3\",\n",
    "    \"cosif_len\",\n",
    "    \"username_mean\",\"username_std\",\"username_z\",\"username_median\",\"username_mad\",\n",
    "    \"lotacao_mean\",\"lotacao_std\",\"lotacao_z\",\"lotacao_median\",\"lotacao_mad\",\n",
    "    \"contacontabil_mean\",\"contacontabil_std\",\"contacontabil_z\",\"contacontabil_median\",\"contacontabil_mad\",\n",
    "    \"cosif_p2_mean\",\"cosif_p2_std\",\"cosif_p2_z\",\"cosif_p2_median\",\"cosif_p2_mad\",\n",
    "    \"cosif_p3_mean\",\"cosif_p3_std\",\"cosif_p3_z\",\"cosif_p3_median\",\"cosif_p3_mad\",\n",
    "    \"dc_bin\",\n",
    "]\n",
    "oh_cols = tipodummies.columns.tolist()\n",
    "\n",
    "# garante existência de todas as colunas numericas (caso algum grupo não crie std etc.)\n",
    "for c in num_cols:\n",
    "    if c not in DF.columns:\n",
    "        DF[c] = np.nan\n",
    "\n",
    "FEATURE_COLS = num_cols + oh_cols\n",
    "DF_FE = DF[FEATURE_COLS].copy()\n",
    "\n",
    "# 4) imputação simples e padronização\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler  = StandardScaler()\n",
    "\n",
    "X_imp = imputer.fit_transform(DF_FE.values)\n",
    "X_std = scaler.fit_transform(X_imp)\n",
    "\n",
    "# 5) split não supervisionado (aleatório reprodutível)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val = train_test_split(X_std, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "skynet(f\"features geradas: {DF_FE.shape[1]} colunas; amostras: {DF_FE.shape[0]:,}\")\n",
    "skynet(f\"split realizado: train={X_train.shape[0]:,} | val={X_val.shape[0]:,}\")\n",
    "\n",
    "# 6) persistência de artefatos para as próximas fases\n",
    "artifacts = {\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"tipos\": tipos,\n",
    "    \"imputer\": imputer,\n",
    "    \"scaler\": scaler,\n",
    "}\n",
    "with open(RUN_DIR / \"features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "\n",
    "np.savez(RUN_DIR / \"dataset_npz.npz\", X_train=X_train, X_val=X_val)\n",
    "\n",
    "# 7) diagnóstico rápido\n",
    "summary = {\n",
    "    \"n_rows\": int(DF_FE.shape[0]),\n",
    "    \"n_features\": int(DF_FE.shape[1]),\n",
    "    \"train_rows\": int(X_train.shape[0]),\n",
    "    \"val_rows\": int(X_val.shape[0]),\n",
    "    \"one_hot_tipoconta_cols\": oh_cols,\n",
    "}\n",
    "with open(RUN_DIR / \"features_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"amostra das primeiras 5 linhas de features padronizadas (numpy):\")\n",
    "print(X_std[:5])\n",
    "\n",
    "skynet(f\"artefatos salvos em {RUN_DIR}: features.pkl e dataset_npz.npz\")\n",
    "\n",
    "# complemento do parágrafo 3: estatísticas por grupo e total de linhas removidas\n",
    "\n",
    "# imprime quantas linhas foram removidas por falhas críticas na validação\n",
    "print(f\"\\nlinhas removidas por falhas críticas (parágrafo 3): {removed}\")\n",
    "\n",
    "# utilitário para exibir dataframes no colab/notebook\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = print  # fallback simples\n",
    "\n",
    "# função de resumo por grupo e identificação de outliers via |z|\n",
    "def resumo_por_grupo(df: pd.DataFrame, key: str, top_n: int = 10):\n",
    "    # resumo estatístico por grupo (valormi)\n",
    "    agg = (\n",
    "        df.groupby(key)[\"valormi\"]\n",
    "          .agg(n=\"size\", media=\"mean\", desvio=\"std\", mediana=\"median\", minimo=\"min\", maximo=\"max\")\n",
    "          .sort_values(\"n\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== resumo por grupo: {key} (top {top_n} por contagem) ===\")\n",
    "    display(agg.head(top_n))\n",
    "\n",
    "    # salva o resumo completo em csv (utf-8-sig ; )\n",
    "    resumo_path = RUN_DIR / f\"resumo_{key}.csv\"\n",
    "    agg.to_csv(resumo_path, sep=\";\", encoding=\"utf-8-sig\")\n",
    "    skynet(f\"resumo por grupo '{key}' salvo em {resumo_path}\")\n",
    "\n",
    "    # top outliers por |z| no grupo, usando as colunas já criadas no parágrafo 3\n",
    "    zcol = f\"{key}_z\"\n",
    "    mean_col = f\"{key}_mean\"\n",
    "    std_col  = f\"{key}_std\"\n",
    "\n",
    "    if zcol in df.columns and mean_col in df.columns and std_col in df.columns:\n",
    "        out = (\n",
    "            df[[key, \"valormi\", zcol, mean_col, std_col]]\n",
    "            .dropna(subset=[zcol])\n",
    "            .assign(abs_z=lambda x: x[zcol].abs())\n",
    "            .sort_values(\"abs_z\", ascending=False)\n",
    "            .head(top_n)\n",
    "        )\n",
    "        print(f\"\\n--- top {top_n} potenciais outliers por |z| em {key} ---\")\n",
    "        display(out)\n",
    "\n",
    "        out_path = RUN_DIR / f\"outliers_{key}.csv\"\n",
    "        out.to_csv(out_path, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "        skynet(f\"outliers por |z| '{key}' salvos em {out_path}\")\n",
    "    else:\n",
    "        print(f\"(aviso) colunas de z-score não encontradas para '{key}' — verifique a etapa de features.\")\n",
    "\n",
    "# executa para os principais agrupamentos contábeis\n",
    "for k in [\"username\", \"lotacao\", \"contacontabil\", \"cosif_p2\", \"cosif_p3\"]:\n",
    "    resumo_por_grupo(DF, k, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1noFN5QdMLP"
   },
   "source": [
    "# **Etapa 4:** Autoencoder utilizando treino com early-stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213805,
     "status": "ok",
     "timestamp": 1760323743280,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "A9lcTuoIdVcG",
    "outputId": "c35e22b1-9b3e-47c6-f019-7fadf6905033"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# parágrafo 4: autoencoder denso (pytorch) com early-stopping e artefatos\n",
    "\n",
    "# imports\n",
    "import os, json, math, time, random, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dependências anteriores\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os parágrafos anteriores.\"\n",
    "ds_path = RUN_DIR / \"dataset_npz.npz\"\n",
    "assert ds_path.exists(), \"dataset_npz.npz ausente — finalize o parágrafo 3.\"\n",
    "with np.load(ds_path) as npz:\n",
    "    X_train = npz[\"X_train\"].astype(np.float32)\n",
    "    X_val   = npz[\"X_val\"].astype(np.float32)\n",
    "\n",
    "# tenta importar torch\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"pytorch não disponível. instale no colab com: pip install torch --quiet\") from e\n",
    "\n",
    "# dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "skynet(f\"treino em dispositivo: {device}\")\n",
    "\n",
    "# reprodutibilidade\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# dataset e dataloaders\n",
    "BATCH_SIZE = 1024\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(X_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(X_val))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# configuração do modelo\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [256, 128, 64]  # você pode ajustar\n",
    "dropout_p = 0.05\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, h_dims, dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc = []\n",
    "        last = in_dim\n",
    "        for h in h_dims:\n",
    "            enc += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "\n",
    "        dec = []\n",
    "        rev = list(reversed(h_dims))\n",
    "        last = rev[0]\n",
    "        for h in rev[1:]:\n",
    "            dec += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        dec += [nn.Linear(last, in_dim)]  # camada de saída linear\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z)\n",
    "        return y\n",
    "\n",
    "model = AutoEncoder(input_dim, hidden_dims, dropout=dropout_p).to(device)\n",
    "skynet(f\"modelo criado: input_dim={input_dim}, hidden={hidden_dims}, dropout={dropout_p}\")\n",
    "\n",
    "# otimizador e perda\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# early-stopping\n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE   = 15\n",
    "MIN_DELTA  = 1e-5\n",
    "\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "best_val = float(\"inf\")\n",
    "best_epoch = -1\n",
    "no_improve = 0\n",
    "best_path = RUN_DIR / \"ae.pt\"\n",
    "\n",
    "def epoch_pass(dataloader, train: bool):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            yhat = model(xb)\n",
    "            loss = criterion(yhat, yb)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            count += xb.size(0)\n",
    "    return total_loss / max(count, 1)\n",
    "\n",
    "skynet(\"iniciando treino do autoencoder\")\n",
    "t0 = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    tr_loss = epoch_pass(train_dl, train=True)\n",
    "    vl_loss = epoch_pass(val_dl, train=False)\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"val_loss\"].append(vl_loss)\n",
    "\n",
    "    # logs ocasionais\n",
    "    if epoch == 1 or epoch % 5 == 0:\n",
    "        skynet(f\"epoch {epoch:03d}  train={tr_loss:.6f}  val={vl_loss:.6f}\")\n",
    "\n",
    "    # early-stopping\n",
    "    if vl_loss + MIN_DELTA < best_val:\n",
    "        best_val = vl_loss\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "        torch.save({\"model_state\": model.state_dict(),\n",
    "                    \"input_dim\": input_dim,\n",
    "                    \"hidden_dims\": hidden_dims,\n",
    "                    \"dropout_p\": dropout_p}, best_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            skynet(f\"early-stopping em epoch {epoch} (sem melhoria por {PATIENCE} épocas)\")\n",
    "            break\n",
    "\n",
    "t1 = time.time()\n",
    "skynet(f\"treino finalizado em {(t1 - t0):.1f}s; melhor época={best_epoch} val_loss={best_val:.6f}\")\n",
    "assert best_path.exists(), \"modelo não foi salvo — verifique o treino.\"\n",
    "\n",
    "# salva histórico\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_csv = RUN_DIR / \"training_history.csv\"\n",
    "hist_df.to_csv(hist_csv, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "skynet(f\"histórico de treino salvo em {hist_csv}\")\n",
    "\n",
    "# gráfico das perdas\n",
    "plt.figure(figsize=(7,4.2))\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"train\")\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"],   label=\"val\")\n",
    "plt.xlabel(\"época\")\n",
    "plt.ylabel(\"mse\")\n",
    "plt.title(\"autoencoder: curva de perdas\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plot_path = RUN_DIR / \"loss_plot.png\"\n",
    "plt.savefig(plot_path, dpi=140, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "skynet(f\"gráfico de perdas salvo em {plot_path}\")\n",
    "\n",
    "# carrega melhor estado e calcula erros de reconstrução no conjunto de validação\n",
    "chk = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(chk[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_tensor = torch.from_numpy(X_val).to(device)\n",
    "    recon_val  = model(val_tensor).cpu().numpy()\n",
    "val_err = np.mean((recon_val - X_val) ** 2, axis=1).astype(np.float32)\n",
    "\n",
    "# salva distribuição de erros e percentis para apoio a threshold\n",
    "err_path = RUN_DIR / \"reconstruction_errors_val.npy\"\n",
    "np.save(err_path, val_err)\n",
    "\n",
    "percentis = [50, 75, 90, 95, 97, 99, 99.5, 99.9]\n",
    "thr = {f\"p{p}\": float(np.percentile(val_err, p)) for p in percentis}\n",
    "thr[\"mean\"] = float(np.mean(val_err))\n",
    "thr[\"std\"]  = float(np.std(val_err))\n",
    "thr[\"suggested_threshold\"] = thr[\"p99.5\"]  # sugestão inicial (ajuste conforme sua capacidade operacional)\n",
    "\n",
    "thr_path = RUN_DIR / \"thresholds.json\"\n",
    "with open(thr_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(thr, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "skynet(f\"erros de validação salvos em {err_path}\")\n",
    "skynet(f\"thresholds salvos em {thr_path}\")\n",
    "print(\"\\nresumo thresholds (val):\")\n",
    "print(pd.Series(thr))\n",
    "\n",
    "# salva configuração do modelo\n",
    "cfg = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dims\": hidden_dims,\n",
    "    \"dropout_p\": dropout_p,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LR,\n",
    "    \"max_epochs\": MAX_EPOCHS,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"min_delta\": MIN_DELTA,\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_val_loss\": best_val,\n",
    "    \"device\": str(device),\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "with open(RUN_DIR / \"model_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "skynet(\"artefatos do modelo escritos em disco: ae.pt, training_history.csv, loss_plot.png, thresholds.json, model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEdTKvXDf-PP"
   },
   "source": [
    "## **Etapa 6:** Calibração e metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1709,
     "status": "ok",
     "timestamp": 1760324524434,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "BqM0y_P2gE8F",
    "outputId": "9c671c93-41a0-4ae6-a85f-3f453187aba8"
   },
   "outputs": [],
   "source": [
    "# parágrafo 6: calibração do limiar (threshold) e relatórios de custo / alertas\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, json, math\n",
    "from pathlib import Path\n",
    "\n",
    "# dependências\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os parágrafos anteriores.\"\n",
    "thr_path = RUN_DIR / \"thresholds.json\"\n",
    "err_path = RUN_DIR / \"reconstruction_errors_val.npy\"\n",
    "assert thr_path.exists() and err_path.exists(), \"execute o parágrafo 4 antes.\"\n",
    "\n",
    "# carrega erros de reconstrução do conjunto de validação\n",
    "val_err = np.load(err_path)\n",
    "with open(thr_path, encoding=\"utf-8\") as f:\n",
    "    thr_info = json.load(f)\n",
    "\n",
    "# parâmetros operacionais\n",
    "CUSTO_FP = 1.0    # custo unitário de um falso positivo (alerta desnecessário)\n",
    "CUSTO_FN = 25.0   # custo unitário de um falso negativo (fraude/erro não detectado)\n",
    "ALERTAS_META = 200  # meta operacional de alertas/dia\n",
    "\n",
    "# função: calcular custo e quantidade de alertas para cada limiar\n",
    "def calcular_metricas(errs: np.ndarray, limiares: np.ndarray):\n",
    "    n = len(errs)\n",
    "    # ordena erros\n",
    "    errs_sorted = np.sort(errs)\n",
    "    metrics = []\n",
    "    for thr in limiares:\n",
    "        alertas = np.count_nonzero(errs > thr)\n",
    "        fp_rate = alertas / n\n",
    "        fn_rate = 1 - fp_rate\n",
    "        custo = fp_rate * CUSTO_FP + fn_rate * CUSTO_FN\n",
    "        metrics.append((thr, alertas, fp_rate, fn_rate, custo))\n",
    "    df = pd.DataFrame(metrics, columns=[\"threshold\", \"alertas\", \"fp_rate\", \"fn_rate\", \"custo\"])\n",
    "    return df\n",
    "\n",
    "# grid de limiares entre p50 e p99.9\n",
    "low, high = np.percentile(val_err, [50, 99.9])\n",
    "limiares = np.linspace(low, high, 200)\n",
    "df_calib = calcular_metricas(val_err, limiares)\n",
    "\n",
    "# acha o limiar que gera ~ALERTAS_META alertas\n",
    "target_idx = (df_calib[\"alertas\"] - ALERTAS_META).abs().idxmin()\n",
    "limiar_meta = float(df_calib.loc[target_idx, \"threshold\"])\n",
    "\n",
    "# curva de custo\n",
    "plt.figure(figsize=(6.8, 4))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"custo\"], label=\"custo total\")\n",
    "plt.axvline(limiar_meta, color=\"r\", ls=\"--\", label=f\"meta {ALERTAS_META} alertas\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"custo relativo\")\n",
    "plt.title(\"Curva de custo vs. threshold\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "cost_path = RUN_DIR / \"cost_curve.png\"\n",
    "plt.savefig(cost_path, dpi=140)\n",
    "plt.close()\n",
    "\n",
    "# curva de alertas\n",
    "plt.figure(figsize=(6.8, 4))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"alertas\"])\n",
    "plt.axhline(ALERTAS_META, color=\"r\", ls=\"--\", label=\"meta operacional\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"quantidade de alertas\")\n",
    "plt.title(\"Alertas vs. threshold\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "alerts_path = RUN_DIR / \"alerts_vs_threshold.png\"\n",
    "plt.savefig(alerts_path, dpi=140)\n",
    "plt.close()\n",
    "\n",
    "# salva relatório completo\n",
    "calib_csv = RUN_DIR / \"calibration_report.csv\"\n",
    "df_calib.to_csv(calib_csv, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "# resumo das métricas principais\n",
    "summary = {\n",
    "    \"threshold_meta\": limiar_meta,\n",
    "    \"alertas_meta\": int(df_calib.loc[target_idx, \"alertas\"]),\n",
    "    \"custo_meta\": float(df_calib.loc[target_idx, \"custo\"]),\n",
    "    \"custo_minimo\": float(df_calib[\"custo\"].min()),\n",
    "    \"threshold_custo_minimo\": float(df_calib.loc[df_calib[\"custo\"].idxmin(), \"threshold\"]),\n",
    "}\n",
    "with open(RUN_DIR / \"calibration_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "skynet(f\"calibração concluída. limiar_meta={limiar_meta:.6f} → {summary['alertas_meta']} alertas estimados\")\n",
    "skynet(f\"relatórios salvos em: {calib_csv}, {cost_path}, {alerts_path}\")\n",
    "print(pd.Series(summary))\n",
    "\n",
    "# complemento do parágrafo 6: marcar pontos escolhidos e exibir/salvar figuras\n",
    "\n",
    "# recupera valores nos pontos de interesse\n",
    "idx_meta = target_idx\n",
    "thr_meta = float(df_calib.loc[idx_meta, \"threshold\"])\n",
    "custo_meta = float(df_calib.loc[idx_meta, \"custo\"])\n",
    "alertas_meta_calc = int(df_calib.loc[idx_meta, \"alertas\"])\n",
    "\n",
    "idx_cmin = int(df_calib[\"custo\"].idxmin())\n",
    "thr_cmin = float(df_calib.loc[idx_cmin, \"threshold\"])\n",
    "custo_min = float(df_calib.loc[idx_cmin, \"custo\"])\n",
    "alertas_cmin = int(df_calib.loc[idx_cmin, \"alertas\"])\n",
    "\n",
    "# 1) curva de custo com marcações\n",
    "plt.figure(figsize=(7.2, 4.5))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"custo\"], label=\"custo total\")\n",
    "plt.axvline(thr_meta, linestyle=\"--\", label=f\"threshold_meta = {thr_meta:.6f}\")\n",
    "plt.scatter([thr_meta, thr_cmin], [custo_meta, custo_min], s=60, label=\"pontos escolhidos\")\n",
    "plt.annotate(f\"meta\\nc={custo_meta:.2f}\", xy=(thr_meta, custo_meta),\n",
    "             xytext=(10, 10), textcoords=\"offset points\")\n",
    "plt.annotate(f\"mínimo\\nc={custo_min:.2f}\", xy=(thr_cmin, custo_min),\n",
    "             xytext=(10, -15), textcoords=\"offset points\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"custo relativo\")\n",
    "plt.title(\"Curva de custo vs. threshold (com marcações)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "cost_marked_path = RUN_DIR / \"cost_curve_marked.png\"\n",
    "plt.savefig(cost_marked_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"curva de custo (marcada) salva em {cost_marked_path}\")\n",
    "\n",
    "# 2) curva de alertas com marcações\n",
    "plt.figure(figsize=(7.2, 4.5))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"alertas\"], label=\"alertas\")\n",
    "plt.axhline(ALERTAS_META, linestyle=\"--\", label=f\"meta operacional = {ALERTAS_META}\")\n",
    "plt.scatter([thr_meta, thr_cmin], [alertas_meta_calc, alertas_cmin], s=60, label=\"pontos escolhidos\")\n",
    "plt.annotate(f\"meta\\nA={alertas_meta_calc}\", xy=(thr_meta, alertas_meta_calc),\n",
    "             xytext=(10, 10), textcoords=\"offset points\")\n",
    "plt.annotate(f\"mínimo(custo)\\nA={alertas_cmin}\", xy=(thr_cmin, alertas_cmin),\n",
    "             xytext=(10, -15), textcoords=\"offset points\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"quantidade de alertas\")\n",
    "plt.title(\"Alertas vs. threshold (com marcações)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "alerts_marked_path = RUN_DIR / \"alerts_vs_threshold_marked.png\"\n",
    "plt.savefig(alerts_marked_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"curva de alertas (marcada) salva em {alerts_marked_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXb4j3zdhS2V"
   },
   "source": [
    "## **Etapa 7:** Pontuação em lote e scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3931,
     "status": "ok",
     "timestamp": 1760324847866,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "dPz-pBI2iWnE",
    "outputId": "95f50720-71f6-497b-e57b-693d55c460d9"
   },
   "outputs": [],
   "source": [
    "# parágrafo 7: pontuação em lote com autoencoder treinado e export de scores (versão corrigida)\n",
    "\n",
    "import os, json, math, pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dependências anteriores\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os parágrafos anteriores.\"\n",
    "assert 'REQUIRED_COLS' in globals() and 'CSV_SEP' in globals() and 'CSV_ENCODING' in globals(), \"execute o parágrafo 2 antes.\"\n",
    "assert 'carregar_validar_csv' in globals(), \"a função de carga/validação do parágrafo 2 é necessária.\"\n",
    "artifacts_path = RUN_DIR / \"features.pkl\"\n",
    "model_path     = RUN_DIR / \"ae.pt\"\n",
    "assert artifacts_path.exists() and model_path.exists(), \"artefatos ausentes: finalize os parágrafos 3 e 4.\"\n",
    "\n",
    "# carrega artefatos de features e modelo\n",
    "with open(artifacts_path, \"rb\") as f:\n",
    "    feats = pickle.load(f)\n",
    "FEATURE_COLS = feats[\"feature_cols\"]\n",
    "imputer      = feats[\"imputer\"]\n",
    "scaler       = feats[\"scaler\"]\n",
    "\n",
    "# threshold: usa o da calibração se existir; senão p99.5 do parágrafo 4\n",
    "thr_use = None\n",
    "calib_summary = RUN_DIR / \"calibration_summary.json\"\n",
    "thr_json      = RUN_DIR / \"thresholds.json\"\n",
    "if calib_summary.exists():\n",
    "    with open(calib_summary, encoding=\"utf-8\") as f:\n",
    "        thr_use = json.load(f).get(\"threshold_meta\", None)\n",
    "if thr_use is None and thr_json.exists():\n",
    "    with open(thr_json, encoding=\"utf-8\") as f:\n",
    "        tj = json.load(f)\n",
    "        thr_use = tj.get(\"suggested_threshold\", None)\n",
    "assert thr_use is not None, \"threshold não encontrado; execute o parágrafo 6 (ou use thresholds.json do parágrafo 4).\"\n",
    "thr_use = float(thr_use)\n",
    "\n",
    "# função: frequency encoding consistente\n",
    "def _freq_encode(series: pd.Series) -> pd.Series:\n",
    "    freq = series.value_counts(dropna=False)\n",
    "    return series.map(freq).astype(float)\n",
    "\n",
    "# função: estatísticas por grupo (mean/std/median, z, mad)\n",
    "def _add_group_stats(df: pd.DataFrame, key: str, val_col: str = \"valormi\"):\n",
    "    g = df.groupby(key, dropna=False)[val_col].agg([\"mean\",\"std\",\"median\"]).rename(\n",
    "        columns={\"mean\":f\"{key}_mean\", \"std\":f\"{key}_std\", \"median\":f\"{key}_median\"}\n",
    "    )\n",
    "    df = df.join(g, on=key)\n",
    "    df[f\"{key}_std\"] = df[f\"{key}_std\"].replace(0, np.nan)\n",
    "    df[f\"{key}_z\"]   = (df[val_col] - df[f\"{key}_mean\"]) / df[f\"{key}_std\"]\n",
    "    df[f\"{key}_mad\"] = (df[val_col] - df[f\"{key}_median\"]).abs()\n",
    "    return df\n",
    "\n",
    "# função: construir features exatamente como no parágrafo 3 (com correção .str.strip())\n",
    "def construir_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # normalização básica de strings (uso correto de .str.strip())\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    # domínios padronizados\n",
    "    df[\"dc\"] = df[\"dc\"].astype(str).str.lower().map({\"d\":\"d\",\"c\":\"c\"})\n",
    "    df[\"tipoconta\"] = df[\"tipoconta\"].astype(str).str.capitalize()\n",
    "    # garante tipo numérico para valormi\n",
    "    df[\"valormi\"] = pd.to_numeric(df[\"valormi\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "    # decomposições cosif\n",
    "    df[\"contacontabil\"] = df[\"contacontabil\"].astype(str).str.strip()\n",
    "    df[\"cosif_len\"] = df[\"contacontabil\"].str.len().clip(upper=12).astype(int)\n",
    "    df[\"cosif_p1\"]  = df[\"contacontabil\"].str[0]\n",
    "    df[\"cosif_p2\"]  = df[\"contacontabil\"].str[:2]\n",
    "    df[\"cosif_p3\"]  = df[\"contacontabil\"].str[:3]\n",
    "\n",
    "    # transformações de valor\n",
    "    df[\"valormi_log1p\"] = np.log1p(df[\"valormi\"].clip(lower=0))\n",
    "\n",
    "    # frequency encoding\n",
    "    df[\"freq_username\"]      = _freq_encode(df[\"username\"])\n",
    "    df[\"freq_lotacao\"]       = _freq_encode(df[\"lotacao\"])\n",
    "    df[\"freq_contacontabil\"] = _freq_encode(df[\"contacontabil\"])\n",
    "    df[\"freq_cosif_p2\"]      = _freq_encode(df[\"cosif_p2\"])\n",
    "    df[\"freq_cosif_p3\"]      = _freq_encode(df[\"cosif_p3\"])\n",
    "\n",
    "    # estatísticas por grupo\n",
    "    for k in [\"username\",\"lotacao\",\"contacontabil\",\"cosif_p2\",\"cosif_p3\"]:\n",
    "        df = _add_group_stats(df, k, \"valormi\")\n",
    "\n",
    "    # codificações de baixa cardinalidade\n",
    "    df[\"dc_bin\"] = df[\"dc\"].map({\"d\":1,\"c\":0}).astype(\"Int64\").astype(int)\n",
    "    tipodummies = pd.get_dummies(df[\"tipoconta\"], prefix=\"tipo\", dtype=int)\n",
    "\n",
    "    df = pd.concat([df, tipodummies], axis=1)\n",
    "\n",
    "    # garante presença de todas as colunas esperadas e na mesma ordem do treino\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    df_out = df[FEATURE_COLS].copy()\n",
    "    return df_out\n",
    "\n",
    "# carrega modelo pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "chk = torch.load(model_path, map_location=device)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, h_dims, dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc, last = [], in_dim\n",
    "        for h in h_dims:\n",
    "            enc += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "        dec, rev = [], list(reversed(h_dims))\n",
    "        last = rev[0]\n",
    "        for h in rev[1:]:\n",
    "            dec += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        dec += [nn.Linear(last, in_dim)]\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z)\n",
    "        return y\n",
    "\n",
    "model = AutoEncoder(chk[\"input_dim\"], chk[\"hidden_dims\"], dropout=chk[\"dropout_p\"]).to(device)\n",
    "model.load_state_dict(chk[\"model_state\"])\n",
    "model.eval()\n",
    "skynet(f\"modelo carregado para pontuação (device={device})\")\n",
    "\n",
    "# seleção do arquivo a pontuar: reusar o selecionado ou escolher outro\n",
    "use_same = input(\"pressione enter para usar o CSV já selecionado; ou digite 'novo' para escolher outro: \").strip().lower()\n",
    "if use_same == \"novo\":\n",
    "    assert 'listar_csvs_mydrive' in globals(), \"a função listar_csvs_mydrive do parágrafo 2 é necessária.\"\n",
    "    csvs = listar_csvs_mydrive(max_files=2000)\n",
    "    print(\"\\narquivos csv encontrados (índice, tamanho, caminho):\")\n",
    "    for i, (p, mtime, sz) in enumerate(csvs):\n",
    "        print(f\"[{i:03d}] {sz/1024/1024:6.2f}MB  {str(p)}\")\n",
    "    sel = int(input(\"\\ndigite o índice do csv desejado: \").strip())\n",
    "    CSV_TO_SCORE = Path(csvs[sel][0])\n",
    "else:\n",
    "    assert 'SELECTED_CSV' in globals(), \"não há CSV selecionado anteriormente; escolha 'novo'.\"\n",
    "    CSV_TO_SCORE = Path(SELECTED_CSV)\n",
    "\n",
    "skynet(f\"pontuando arquivo: {CSV_TO_SCORE}\")\n",
    "\n",
    "# carrega e valida o csv\n",
    "DF_SCORE_SRC = carregar_validar_csv(CSV_TO_SCORE)\n",
    "\n",
    "# remove linhas com falhas críticas como no parágrafo 3\n",
    "mask_ok_score = (\n",
    "    DF_SCORE_SRC[\"username\"].notna() &\n",
    "    DF_SCORE_SRC[\"lotacao\"].notna() &\n",
    "    DF_SCORE_SRC[\"tipoconta\"].notna() &\n",
    "    DF_SCORE_SRC[\"dc\"].isin({\"d\",\"c\"}) &\n",
    "    DF_SCORE_SRC[\"contacontabil\"].astype(str).str.fullmatch(r\"\\d+\") &\n",
    "    DF_SCORE_SRC[\"valormi\"].notna() & (DF_SCORE_SRC[\"valormi\"] >= 0)\n",
    ")\n",
    "removed_score = int((~mask_ok_score).sum())\n",
    "DF_SCORE = DF_SCORE_SRC.loc[mask_ok_score].reset_index(drop=True)\n",
    "skynet(f\"linhas removidas nesta pontuação: {removed_score}\")\n",
    "\n",
    "# constroi features e aplica imputer/scaler treinados\n",
    "X_feat = construir_features(DF_SCORE)\n",
    "X_imp  = imputer.transform(X_feat.values)\n",
    "X_std  = scaler.transform(X_imp)\n",
    "\n",
    "# inferência\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.from_numpy(X_std.astype(np.float32)).to(device)\n",
    "    X_recon  = model(X_tensor).cpu().numpy()\n",
    "\n",
    "# erro de reconstrução (mse por amostra) e contribuições por feature (erro quadrático)\n",
    "err_vec = np.mean((X_recon - X_std) ** 2, axis=1).astype(np.float32)\n",
    "contrib_mat = (X_recon - X_std) ** 2  # mesma escala das features padronizadas\n",
    "contrib_cols = FEATURE_COLS\n",
    "\n",
    "# top-k contribuições por amostra (nomes das variáveis)\n",
    "TOPK = 5\n",
    "topk_idx = np.argsort(contrib_mat, axis=1)[:, ::-1][:, :TOPK]\n",
    "topk_names = [[contrib_cols[j] for j in row] for row in topk_idx]\n",
    "\n",
    "# flag por limiar\n",
    "flags = (err_vec > thr_use).astype(int)\n",
    "\n",
    "# compõe dataframe de saída\n",
    "OUT = DF_SCORE[[\"username\",\"lotacao\",\"tipoconta\",\"valormi\",\"dc\",\"contacontabil\"]].copy()\n",
    "OUT[\"recon_error\"] = err_vec\n",
    "OUT[\"flag_threshold\"] = flags\n",
    "OUT[\"threshold_used\"] = thr_use\n",
    "for k in range(TOPK):\n",
    "    OUT[f\"top{k+1}\"] = [names[k] for names in topk_names]\n",
    "\n",
    "# salva csv de scores\n",
    "scores_path = RUN_DIR / \"scores.csv\"\n",
    "OUT.to_csv(scores_path, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "skynet(f\"scores salvos em {scores_path}\")\n",
    "\n",
    "# salva contribuições médias por feature (para visão global)\n",
    "mean_contrib = contrib_mat.mean(axis=0)\n",
    "CONTRIB_DF = pd.DataFrame({\"feature\": contrib_cols, \"mean_contrib\": mean_contrib})\n",
    "CONTRIB_DF.sort_values(\"mean_contrib\", ascending=False, inplace=True)\n",
    "contrib_path = RUN_DIR / \"feature_contributions_mean.csv\"\n",
    "CONTRIB_DF.to_csv(contrib_path, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "skynet(f\"contribuições médias por feature salvas em {contrib_path}\")\n",
    "\n",
    "# histograma de erros com linha de threshold\n",
    "plt.figure(figsize=(7.0, 4.4))\n",
    "plt.hist(err_vec, bins=60, alpha=0.85)\n",
    "plt.axvline(thr_use, linestyle=\"--\", label=f\"threshold={thr_use:.6f}\")\n",
    "plt.xlabel(\"erro de reconstrução\")\n",
    "plt.ylabel(\"frequência\")\n",
    "plt.title(\"distribuição do erro de reconstrução (batch scoring)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "hist_path = RUN_DIR / \"scoring_error_hist.png\"\n",
    "plt.savefig(hist_path, dpi=140, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "skynet(f\"histograma de erros salvo em {hist_path}\")\n",
    "\n",
    "# imprime amostra dos top-n suspeitos\n",
    "TOPN_PRINT = 15\n",
    "rank = OUT.sort_values(\"recon_error\", ascending=False).head(TOPN_PRINT)\n",
    "print(\"\\nTop suspeitos por erro de reconstrução:\")\n",
    "print(rank[[\"username\",\"lotacao\",\"tipoconta\",\"valormi\",\"dc\",\"contacontabil\",\"recon_error\",\"flag_threshold\",\"top1\",\"top2\",\"top3\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMskiJYXiikw"
   },
   "source": [
    "## **Etapa 8:** Monitoramento de drift e drift do erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1760324959801,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "PpFJcJPBiuCx",
    "outputId": "1b39448b-cb27-4df0-cc4a-0017e98e30bc"
   },
   "outputs": [],
   "source": [
    "# parágrafo 8: monitoramento de drift (PSI por feature) e drift do erro (PSI + KS)\n",
    "\n",
    "import os, json, math, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dependências dos parágrafos 3, 4 e 7\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os parágrafos anteriores.\"\n",
    "artifacts_path = RUN_DIR / \"features.pkl\"\n",
    "npz_path       = RUN_DIR / \"dataset_npz.npz\"            # contém X_train/X_val (padronizados)\n",
    "val_err_path   = RUN_DIR / \"reconstruction_errors_val.npy\"  # erros da validação (par. 4)\n",
    "scores_path    = RUN_DIR / \"scores.csv\"                 # resultado da pontuação (par. 7)\n",
    "\n",
    "assert artifacts_path.exists() and npz_path.exists() and val_err_path.exists(), \\\n",
    "    \"artefatos ausentes: finalize os parágrafos 3 e 4.\"\n",
    "assert scores_path.exists(), \"scores.csv ausente. finalize o parágrafo 7.\"\n",
    "\n",
    "with open(artifacts_path, \"rb\") as f:\n",
    "    feats = pickle.load(f)\n",
    "FEATURE_COLS = feats[\"feature_cols\"]\n",
    "imputer      = feats[\"imputer\"]\n",
    "scaler       = feats[\"scaler\"]\n",
    "\n",
    "# baseline (treino) para PSI de features\n",
    "with np.load(npz_path) as npz:\n",
    "    X_train_base = npz[\"X_train\"]  # já padronizado\n",
    "\n",
    "# erros baseline (validação do treino) para drift de erro\n",
    "err_val_base = np.load(val_err_path)\n",
    "\n",
    "# ----- recuperar features padronizadas do LOTE ATUAL -----\n",
    "# Se X_std e DF_SCORE ainda estiverem em memória (par.7), ótimo; caso contrário, reconstruir\n",
    "def _reconst_features_from_source():\n",
    "    # precisamos do mesmo CSV do par.7; como fallback, usamos o DF reconstruído de scores.csv\n",
    "    # (scores.csv não contém features; então pedimos para escolher um CSV novamente, garantindo consistência)\n",
    "    assert 'carregar_validar_csv' in globals() and 'construir_features' in globals(), \\\n",
    "        \"funções do parágrafo 2 e 7 são necessárias.\"\n",
    "    print(\"\\nreconstruindo features: selecione novamente o CSV a monitorar.\")\n",
    "    # reutiliza a listagem do par.2\n",
    "    assert 'listar_csvs_mydrive' in globals(), \"a função listar_csvs_mydrive do par.2 é necessária.\"\n",
    "    csvs = listar_csvs_mydrive(max_files=2000)\n",
    "    for i, (p, mtime, sz) in enumerate(csvs):\n",
    "        print(f\"[{i:03d}] {sz/1024/1024:6.2f}MB  {str(p)}\")\n",
    "    sel = int(input(\"\\ndigite o índice do csv desejado: \").strip())\n",
    "    csv_path = Path(csvs[sel][0])\n",
    "    df_src = carregar_validar_csv(csv_path)\n",
    "    # filtra linhas críticas como no par.3/7\n",
    "    mask_ok = (\n",
    "        df_src[\"username\"].notna() &\n",
    "        df_src[\"lotacao\"].notna() &\n",
    "        df_src[\"tipoconta\"].notna() &\n",
    "        df_src[\"dc\"].isin({\"d\",\"c\"}) &\n",
    "        df_src[\"contacontabil\"].astype(str).str.fullmatch(r\"\\d+\") &\n",
    "        df_src[\"valormi\"].notna() & (df_src[\"valormi\"] >= 0)\n",
    "    )\n",
    "    df_use = df_src.loc[mask_ok].reset_index(drop=True)\n",
    "    X_feat = construir_features(df_use)\n",
    "    X_imp  = imputer.transform(X_feat.values)\n",
    "    X_std  = scaler.transform(X_imp)\n",
    "    return X_std\n",
    "\n",
    "if 'X_std' in globals():\n",
    "    X_curr = X_std\n",
    "else:\n",
    "    X_curr = _reconst_features_from_source()\n",
    "\n",
    "# ----- utilitários de PSI/KS -----\n",
    "def _bin_edges_from_base(base_vals: np.ndarray, n_bins: int = 10):\n",
    "    # cria bins por quantis no baseline; garante bordas únicas (jitter se necessário)\n",
    "    qs = np.linspace(0, 1, n_bins + 1)\n",
    "    edges = np.unique(np.quantile(base_vals, qs))\n",
    "    # se todas iguais (variância zero), retorna None\n",
    "    if len(edges) <= 2:\n",
    "        return None\n",
    "    return edges\n",
    "\n",
    "def _hist_proportions(vals: np.ndarray, edges: np.ndarray):\n",
    "    hist, _ = np.histogram(vals, bins=edges)\n",
    "    props = hist.astype(float) / max(1, vals.shape[0])\n",
    "    # suavização mínima para evitar log(0) no PSI\n",
    "    eps = 1e-6\n",
    "    props = np.clip(props, eps, 1.0)\n",
    "    return props\n",
    "\n",
    "def psi(base: np.ndarray, curr: np.ndarray, n_bins: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    PSI padrão por bins definidos a partir do baseline.\n",
    "    \"\"\"\n",
    "    edges = _bin_edges_from_base(base, n_bins)\n",
    "    if edges is None:\n",
    "        return 0.0\n",
    "    p = _hist_proportions(base, edges)\n",
    "    q = _hist_proportions(curr, edges)\n",
    "    return float(np.sum((p - q) * np.log(p / q)))\n",
    "\n",
    "def ks_2sample(a: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    KS 2-amostras sem SciPy (ECDF discreta).\n",
    "    Retorna (D, p_value_approx) com aproximação de p.\n",
    "    \"\"\"\n",
    "    a = np.sort(a)\n",
    "    b = np.sort(b)\n",
    "    n, m = len(a), len(b)\n",
    "    i = j = 0\n",
    "    d = 0.0\n",
    "    while i < n and j < m:\n",
    "        if a[i] <= b[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        fa = i / n\n",
    "        fb = j / m\n",
    "        d = max(d, abs(fa - fb))\n",
    "    # aproximação de p-value (Massey, 1951)\n",
    "    en = np.sqrt(n * m / (n + m))\n",
    "    p = 2.0 * np.exp(-2.0 * (d * en) ** 2)\n",
    "    p = float(np.clip(p, 0.0, 1.0))\n",
    "    return float(d), p\n",
    "\n",
    "# ----- PSI por feature -----\n",
    "skynet(\"calculando PSI por feature (baseline = X_train, atual = lote pontuado)\")\n",
    "psi_list = []\n",
    "# limita a quantidade de colunas para gráficos (mas calcula PSI para todas)\n",
    "for j, col in enumerate(FEATURE_COLS):\n",
    "    base_col = X_train_base[:, j]\n",
    "    curr_col = X_curr[:, j]\n",
    "    try:\n",
    "        score = psi(base_col, curr_col, n_bins=10)\n",
    "    except Exception:\n",
    "        score = np.nan\n",
    "    psi_list.append((col, float(score)))\n",
    "\n",
    "PSI_DF = pd.DataFrame(psi_list, columns=[\"feature\", \"psi\"]).sort_values(\"psi\", ascending=False)\n",
    "psi_csv = RUN_DIR / \"monitor_psi_features.csv\"\n",
    "PSI_DF.to_csv(psi_csv, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "skynet(f\"PSI por feature salvo em {psi_csv}\")\n",
    "\n",
    "# ----- Drift do erro de reconstrução -----\n",
    "# Reusa erro do lote atual se disponível do par.7; senão, estima novamente a partir do scores.csv (não contém erro por si só).\n",
    "if 'err_vec' in globals():\n",
    "    err_curr = err_vec\n",
    "else:\n",
    "    # tentar recuperar do histograma salvo não é possível; então avisar\n",
    "    warnings.warn(\"err_vec não encontrado em memória; reexecute o parágrafo 7 para popular err_vec para análise completa.\")\n",
    "    # fallback: abortar seção de erro se não houver err_vec\n",
    "    err_curr = None\n",
    "\n",
    "drift_report = {}\n",
    "if err_curr is not None and len(err_curr) > 0:\n",
    "    psi_err = psi(err_val_base, err_curr, n_bins=20)\n",
    "    ks_D, ks_p = ks_2sample(err_val_base, err_curr)\n",
    "    drift_report = {\"psi_error\": float(psi_err), \"ks_D\": float(ks_D), \"ks_p_approx\": float(ks_p)}\n",
    "\n",
    "    # gráficos: hist overlay de erros\n",
    "    plt.figure(figsize=(7.2, 4.6))\n",
    "    plt.hist(err_val_base, bins=60, alpha=0.55, label=\"val (baseline)\")\n",
    "    plt.hist(err_curr,     bins=60, alpha=0.55, label=\"lote atual\")\n",
    "    plt.xlabel(\"erro de reconstrução\")\n",
    "    plt.ylabel(\"frequência\")\n",
    "    plt.title(\"Distribuição do erro de reconstrução: baseline vs lote\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    err_hist_path = RUN_DIR / \"error_hist_overlay.png\"\n",
    "    plt.savefig(err_hist_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    skynet(f\"histograma de erro (overlay) salvo em {err_hist_path}\")\n",
    "else:\n",
    "    skynet(\"aviso: err_vec indisponível; pulei gráficos e métricas de drift do erro.\")\n",
    "\n",
    "# ----- Gráfico: Top-k features com maior PSI -----\n",
    "TOPK = 20\n",
    "top_df = PSI_DF.head(TOPK)\n",
    "plt.figure(figsize=(8.8, max(4.0, 0.3 * TOPK)))\n",
    "plt.barh(top_df[\"feature\"][::-1], top_df[\"psi\"][::-1])\n",
    "plt.xlabel(\"PSI\")\n",
    "plt.title(f\"Top {TOPK} features com maior PSI (baseline X_train vs lote)\")\n",
    "plt.grid(axis=\"x\", alpha=0.3)\n",
    "psi_bar_path = RUN_DIR / \"psi_bar_top.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(psi_bar_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"gráfico de barras do PSI salvo em {psi_bar_path}\")\n",
    "\n",
    "# ----- Salvamento do relatório de drift do erro -----\n",
    "if drift_report:\n",
    "    drift_json = RUN_DIR / \"monitor_error_drift.json\"\n",
    "    with open(drift_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(drift_report, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nresumo drift do erro (baseline val vs lote):\")\n",
    "    print(pd.Series(drift_report))\n",
    "    skynet(f\"relatório de drift do erro salvo em {drift_json}\")\n",
    "\n",
    "# ----- Sinalizadores práticos -----\n",
    "# heurísticas comuns de PSI:\n",
    "#   < 0.1: estável; 0.1–0.25: atenção; > 0.25: shift relevante\n",
    "flags = {\n",
    "    \"num_features_psi_gt_0_25\": int((PSI_DF[\"psi\"] > 0.25).sum()),\n",
    "    \"num_features_psi_gt_0_10\": int((PSI_DF[\"psi\"] > 0.10).sum()),\n",
    "    \"max_feature_psi\": float(PSI_DF[\"psi\"].max() if len(PSI_DF) else np.nan),\n",
    "    \"max_feature_name\": str(PSI_DF.iloc[0][\"feature\"] if len(PSI_DF) else \"\"),\n",
    "}\n",
    "with open(RUN_DIR / \"monitor_flags.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(flags, f, ensure_ascii=False, indent=2)\n",
    "print(\"\\nresumo de flags de estabilidade (PSI):\")\n",
    "print(pd.Series(flags))\n",
    "skynet(\"monitoramento concluído: PSI por feature, drift do erro e flags salvos no RUN_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FND4f7mZjuny"
   },
   "source": [
    "### **Complemento** Análise gráfica do drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4602,
     "status": "ok",
     "timestamp": 1760325245411,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "yfxGTpkGjth_",
    "outputId": "0ceea81a-e272-4918-82c9-509678e53744"
   },
   "outputs": [],
   "source": [
    "# complemento interpretativo do parágrafo 8: histogramas e ECDF com marcação do KS\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# pré-requisitos: err_val_base (do par.8) e err_curr (do par.7) em memória\n",
    "if 'err_val_base' not in globals():\n",
    "    err_val_base = np.load(RUN_DIR / \"reconstruction_errors_val.npy\")\n",
    "if 'err_curr' not in globals():\n",
    "    raise RuntimeError(\"err_curr indisponível. execute o parágrafo 7 para calcular os erros do lote atual (err_vec).\")\n",
    "\n",
    "# função utilitária para ECDF (degrau)\n",
    "def _ecdf(x: np.ndarray):\n",
    "    x = np.sort(np.asarray(x))\n",
    "    y = np.arange(1, len(x) + 1) / len(x)\n",
    "    return x, y\n",
    "\n",
    "# calcula ECDFs\n",
    "x1, F1 = _ecdf(err_val_base)\n",
    "x2, F2 = _ecdf(err_curr)\n",
    "\n",
    "# constrói grade comum de x (todos os pontos observados)\n",
    "x_all = np.sort(np.unique(np.concatenate([x1, x2])))\n",
    "\n",
    "# valores de F1 e F2 sobre a grade comum (passo à esquerda)\n",
    "def _step_at(x_grid, x_vals, F_vals):\n",
    "    # para cada x_grid, pega F(x) = proporção de pontos <= x_grid\n",
    "    idx = np.searchsorted(x_vals, x_grid, side=\"right\") - 1\n",
    "    idx = np.clip(idx, -1, len(F_vals) - 1)\n",
    "    out = np.where(idx >= 0, F_vals[idx], 0.0)\n",
    "    return out\n",
    "\n",
    "F1g = _step_at(x_all, x1, F1)\n",
    "F2g = _step_at(x_all, x2, F2)\n",
    "\n",
    "# estatística KS e ponto de maior divergência\n",
    "diff = np.abs(F1g - F2g)\n",
    "ks_D = float(diff.max())\n",
    "argmax = int(diff.argmax())\n",
    "x_star = float(x_all[argmax])\n",
    "\n",
    "# 1) histograma comparativo (normalizado) com anotação KS\n",
    "plt.figure(figsize=(7.8, 4.8))\n",
    "plt.hist(err_val_base, bins=60, alpha=0.55, density=True, label=\"validação (baseline)\")\n",
    "plt.hist(err_curr,     bins=60, alpha=0.55, density=True, label=\"lote atual\")\n",
    "plt.axvline(x_star, linestyle=\"--\", label=f\"x* (KS) ≈ {x_star:.6f}\")\n",
    "plt.title(\"erro de reconstrução — distribuição (baseline vs. lote)\")\n",
    "plt.xlabel(\"erro de reconstrução\")\n",
    "plt.ylabel(\"densidade\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "hist_interp_path = RUN_DIR / \"error_drift_interpret_hist.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_interp_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"gráfico interpretativo (hist): {hist_interp_path}\")\n",
    "\n",
    "# 2) ECDFs com marcação visual do KS (distância máxima vertical)\n",
    "plt.figure(figsize=(7.8, 4.8))\n",
    "plt.step(x_all, F1g, where=\"post\", label=\"ECDF val (baseline)\")\n",
    "plt.step(x_all, F2g, where=\"post\", label=\"ECDF lote atual\")\n",
    "\n",
    "# marca o ponto de maior divergência\n",
    "plt.axvline(x_star, linestyle=\"--\", alpha=0.8)\n",
    "# desenha o segmento vertical da distância KS\n",
    "y1_star = F1g[argmax]\n",
    "y2_star = F2g[argmax]\n",
    "y_low, y_high = sorted([y1_star, y2_star])\n",
    "plt.vlines(x_star, y_low, y_high, linewidth=3, alpha=0.9, label=f\"KS = {ks_D:.3f}\")\n",
    "\n",
    "plt.title(\"erro de reconstrução — ECDFs e distância KS\")\n",
    "plt.xlabel(\"erro de reconstrução\")\n",
    "plt.ylabel(\"F(x)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "ecdf_interp_path = RUN_DIR / \"error_drift_interpret_ecdf.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(ecdf_interp_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"gráfico interpretativo (ecdf): {ecdf_interp_path}\")\n",
    "\n",
    "# explicação rápida impressa\n",
    "print(\n",
    "    f\"\\ninterpretação rápida:\\n\"\n",
    "    f\"- KS = {ks_D:.3f} é a maior distância vertical entre as ECDFs no ponto x* ≈ {x_star:.6f}.\\n\"\n",
    "    f\"- quanto maior o KS, maior a evidência de que as distribuições de erro mudaram.\\n\"\n",
    "    f\"- se KS estiver próximo de 1.0, as curvas quase não se sobrepõem; se perto de 0, são muito similares.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgOMyD9yS/uWLXnDBP6caW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
