{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r5OMfQfNaAm"
   },
   "source": [
    "#**Licença de Uso**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d22_lVKTNenJ"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "→ You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution–NonCommercial 4.0 International License.\n",
    "\n",
    "→ You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**© 2025 Leandro Bernardo Rodrigues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRNqsqxZWPzs"
   },
   "source": [
    "# **Gestão do Ambiente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clC8wQo4V2_j"
   },
   "source": [
    "##**Criar repositório .git no Colab**\n",
    "---\n",
    "Google Drive é considerado o ponto de verdade\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQi7bUIQUmAr"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# parágrafo git: inicialização do repositório no drive e push inicial para o github\n",
    "\n",
    "# imports\n",
    "from pathlib import Path\n",
    "import subprocess, os, sys, getpass, textwrap\n",
    "\n",
    "# util de shell\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout, r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "# garantir que o diretório do projeto exista\n",
    "repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# montar drive no colab se necessário\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive  # type: ignore\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        print(\"montando google drive…\")\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# configurar safe.directory para evitar avisos do git com caminhos de rede\n",
    "try:\n",
    "    sh([\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", str(repo_dir)])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# inicializar repositório se ainda não existir\n",
    "if not (repo_dir / \".git\").exists():\n",
    "    print(\"inicializando repositório git…\")\n",
    "    sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "    # garantir branch principal como main (compatível com versões antigas)\n",
    "    try:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"branch\", \"-M\", default_branch], cwd=repo_dir)\n",
    "else:\n",
    "    print(\".git já existe; seguindo\")\n",
    "\n",
    "# configurar identidade local\n",
    "sh([\"git\", \"config\", \"user.name\", author_name], cwd=repo_dir)\n",
    "sh([\"git\", \"config\", \"user.email\", author_email], cwd=repo_dir)\n",
    "\n",
    "# criar .gitignore básico e readme se estiverem ausentes\n",
    "gitignore_path = repo_dir / \".gitignore\"\n",
    "if not gitignore_path.exists():\n",
    "    gitignore_path.write_text(textwrap.dedent(\"\"\"\n",
    "      # python\n",
    "      __pycache__/\n",
    "      *.py[cod]\n",
    "      *.egg-info/\n",
    "      .venv*/\n",
    "      venv/\n",
    "\n",
    "      # segredos\n",
    "      .env\n",
    "      *.key\n",
    "      *.pem\n",
    "      *.tok\n",
    "\n",
    "      # jupyter/colab\n",
    "      .ipynb_checkpoints/\n",
    "\n",
    "      # artefatos e dados locais (não versionar)\n",
    "      data/\n",
    "      input/                 # inclui input.csv sensível\n",
    "      output/\n",
    "      runs/\n",
    "      logs/\n",
    "      figures/\n",
    "      *.log\n",
    "      *.tmp\n",
    "      *.bak\n",
    "      *.png\n",
    "      *.jpg\n",
    "      *.pdf\n",
    "      *.html\n",
    "\n",
    "      # allowlist para a pasta de referências\n",
    "      !references/\n",
    "      !references/**\n",
    "    \"\"\").strip() + \"\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado .gitignore\")\n",
    "\n",
    "readme_path = repo_dir / \"README.md\"\n",
    "if not readme_path.exists():\n",
    "    readme_path.write_text(f\"# {repo_name}\\n\\nprojeto de autoencoder tabular para journal entries.\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado README.md\")\n",
    "\n",
    "# configurar remoto origin\n",
    "remote_base = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "existing_remotes = sh([\"git\", \"remote\"], cwd=repo_dir)\n",
    "if \"origin\" not in existing_remotes.split():\n",
    "    sh([\"git\", \"remote\", \"add\", \"origin\", remote_base], cwd=repo_dir)\n",
    "    print(f\"remoto origin adicionado: {remote_base}\")\n",
    "else:\n",
    "    # se já existe, garantir que aponta para o repo correto\n",
    "    current_url = sh([\"git\", \"remote\", \"get-url\", \"origin\"], cwd=repo_dir)\n",
    "    if current_url != remote_base:\n",
    "        sh([\"git\", \"remote\", \"set-url\", \"origin\", remote_base], cwd=repo_dir)\n",
    "        print(f\"remoto origin atualizado para: {remote_base}\")\n",
    "    else:\n",
    "        print(\"remoto origin já configurado corretamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-aIMDUNNhfi"
   },
   "source": [
    "##**Utilitário:** verificação da formatação de código\n",
    "\n",
    "Black [88] + Isort, desconsiderando células mágicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jfa9X-d-Kfdn"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0001\n",
    "#pré-visualizar/aplicar (pula magics) — isort(profile=black)+black(88) { display-mode: \"form\" }\n",
    "import sys, subprocess, os, re, difflib, textwrap, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===== CONFIG =====\n",
    "NOTEBOOK = \"/content/drive/MyDrive/Notebooks/data-analysis/notebooks/AETabular_main.ipynb\"  # <- ajuste\n",
    "LINE_LENGTH = 88\n",
    "# ==================\n",
    "\n",
    "# 1) Instalar libs no MESMO Python do kernel\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"black\", \"isort\", \"nbformat\"])\n",
    "\n",
    "import nbformat\n",
    "import black\n",
    "import isort\n",
    "\n",
    "BLACK_MODE = black.Mode(line_length=LINE_LENGTH)\n",
    "ISORT_CFG  = isort.Config(profile=\"black\", line_length=LINE_LENGTH)\n",
    "\n",
    "# 2) Regras para pular células com magics/shell\n",
    "#   - linhas começando com %, %%, !\n",
    "#   - chamadas a get_ipython(\n",
    "MAGIC_LINE = re.compile(r\"^\\s*(%{1,2}|!)\", re.M)\n",
    "GET_IPY    = re.compile(r\"get_ipython\\s*\\(\")\n",
    "\n",
    "def has_magics(code: str) -> bool:\n",
    "    return bool(MAGIC_LINE.search(code) or GET_IPY.search(code))\n",
    "\n",
    "def format_code(code: str) -> str:\n",
    "    # isort primeiro, depois black\n",
    "    sorted_code = isort.api.sort_code_string(code, config=ISORT_CFG)\n",
    "    return black.format_str(sorted_code, mode=BLACK_MODE)\n",
    "\n",
    "def summarize_diff(diff_lines: List[str]) -> Tuple[int, int]:\n",
    "    added = removed = 0\n",
    "    for ln in diff_lines:\n",
    "        # ignorar cabeçalhos do diff\n",
    "        if ln.startswith((\"---\", \"+++\", \"@@\")):\n",
    "            continue\n",
    "        if ln.startswith(\"+\"):\n",
    "            added += 1\n",
    "        elif ln.startswith(\"-\"):\n",
    "            removed += 1\n",
    "    return added, removed\n",
    "\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(title)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "if not os.path.exists(NOTEBOOK):\n",
    "    raise FileNotFoundError(f\"Notebook não encontrado:\\n{NOTEBOOK}\")\n",
    "\n",
    "# 3) Leitura do .ipynb\n",
    "with open(NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "changed_cells = []  # (idx, added, removed, diff_text, preview_snippet, new_code)\n",
    "\n",
    "# 4) Pré-visualização célula a célula\n",
    "header(\"Pré-visualização (NÃO grava) — somente células com mudanças\")\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "\n",
    "    original = cell.get(\"source\", \"\")\n",
    "    if not original.strip():\n",
    "        continue\n",
    "\n",
    "    # Pular células com magics/shell\n",
    "    if has_magics(original):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formatted = format_code(original)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] célula {i}: erro no formatador — pulando ({e})\")\n",
    "        continue\n",
    "\n",
    "    if original.strip() != formatted.strip():\n",
    "        # Gerar diff unificado legível\n",
    "        diff = list(difflib.unified_diff(\n",
    "            original.splitlines(), formatted.splitlines(),\n",
    "            fromfile=f\"cell_{i}:before\", tofile=f\"cell_{i}:after\", lineterm=\"\"\n",
    "        ))\n",
    "        add, rem = summarize_diff(diff)\n",
    "        snippet = original.strip().splitlines()[0][:120] if original.strip().splitlines() else \"<célula vazia>\"\n",
    "        changed_cells.append((i, add, rem, \"\\n\".join(diff), snippet, formatted))\n",
    "\n",
    "# 5) Exibição dos diffs por célula (se houver)\n",
    "if not changed_cells:\n",
    "    print(\"✔ Nada a alterar: todas as células (não mágicas) já estão conforme isort/black.\")\n",
    "else:\n",
    "    total_add = total_rem = 0\n",
    "    for (idx, add, rem, diff_text, snippet, _new) in changed_cells:\n",
    "        total_add += add\n",
    "        total_rem += rem\n",
    "        header(f\"Diff — Célula #{idx}  (+{add}/-{rem})\")\n",
    "        print(f\"Primeira linha da célula: {snippet!r}\\n\")\n",
    "        print(diff_text)\n",
    "\n",
    "    header(\"Resumo\")\n",
    "    print(f\"Células com mudanças: {len(changed_cells)}\")\n",
    "    print(f\"Linhas adicionadas:   {total_add}\")\n",
    "    print(f\"Linhas removidas:     {total_rem}\")\n",
    "\n",
    "# 6) Perguntar se aplica\n",
    "if changed_cells:\n",
    "    print(\"\\nDigite 'p' para **Proceder** e gravar as mudanças nessas células, ou 'c' para **Cancelar**.\")\n",
    "    try:\n",
    "        choice = input(\"Proceder (p) / Cancelar (c): \").strip().lower()\n",
    "    except Exception:\n",
    "        choice = \"c\"\n",
    "\n",
    "    if choice == \"p\":\n",
    "        # Backup antes de escrever\n",
    "        backup = NOTEBOOK + \".bak\"\n",
    "        if not os.path.exists(backup):\n",
    "            with open(backup, \"w\", encoding=\"utf-8\") as bf:\n",
    "                nbformat.write(nb, bf)\n",
    "\n",
    "        # Aplicar somente nas células com mudanças\n",
    "        idx_to_new = {idx: new for (idx, _a, _r, _d, _s, new) in changed_cells}\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if i in idx_to_new and cell.get(\"cell_type\") == \"code\":\n",
    "                cell[\"source\"] = idx_to_new[i]\n",
    "\n",
    "        # Escrever no .ipynb\n",
    "        with open(NOTEBOOK, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "\n",
    "        # Sync delay (Drive)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        header(\"Concluído\")\n",
    "        print(f\"✔ Mudanças aplicadas em {len(changed_cells)} célula(s).\")\n",
    "        print(f\"Backup criado em: {backup}\")\n",
    "        print(\"Dica: recarregue o notebook no Colab para ver a formatação atualizada.\")\n",
    "    else:\n",
    "        print(\"\\nOperação cancelada. Nada foi gravado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwzVcwk9Ol0K"
   },
   "source": [
    "##**Sincronizar alterações no código do projeto**\n",
    "Comandos para sincronizar código (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive é considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2hJZaAa2OqEp"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive é a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de versão (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; só 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_AETABULAR (fallback: env; último caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#utilitários silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heurística curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permissões/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local inválido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#configurações do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do repositório no GitHub\n",
    "repo_name      = \"ae-tabular\"    # nome do repositório\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive não montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se é um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta operações pendentes (não apaga histórico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #força branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diretório como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necessário)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normalização\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos já rastreados que hoje são ignorados pelo .gitignore,\n",
    "    limpa o índice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contrário.\n",
    "    \"\"\"\n",
    "    #remove lock de índice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o índice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que estão rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detecção? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de versão\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato legível + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autenticação (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_AETABULAR')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - variável de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_AETABULAR\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#listas de força\n",
    "FORCE_UNTRACK = [\"input/\", \"output/\", \"data/\", \"runs/\", \"logs/\", \"figures/\"]\n",
    "FORCE_TRACK   = [\"references/\"]  #versionar tudo dentro (PDFs inclusive)\n",
    "\n",
    "def force_index_rules():\n",
    "    #garante que pastas sensíveis NUNCA fiquem rastreadas\n",
    "    for p in FORCE_UNTRACK:\n",
    "        try:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "    #garante que references/ SEMPRE entre (útil se ainda há *.pdf globais)\n",
    "    for p in FORCE_TRACK:\n",
    "        try:\n",
    "            git(\"add\", \"-f\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de versão (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudança: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #versão\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma única vez, se necessário)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #aplica regras de força\n",
    "        force_index_rules()\n",
    "\n",
    "        #stage de tudo (Drive é a verdade; remoções entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se só a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se já existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} — {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive é a verdade): validação + push forçado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # você é o owner; não perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se não existir (repo vazio), faz um probe genérico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #repositório pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push forçado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. Versão atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem única, curta, sem detalhes sensíveis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt9CwUvIWJ4-"
   },
   "source": [
    "#**Projeto**\n",
    "Comandos para sincronizar código (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive é considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vda5PsLZrSp"
   },
   "source": [
    "## **Etapa 1:** Setup do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33624,
     "status": "ok",
     "timestamp": 1760742346381,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "eYlX1wdJZvEh",
    "outputId": "46e3ed20-9957-4e59-d21b-5779755ba9b0"
   },
   "outputs": [],
   "source": [
    "# @title §1 — Ambiente: venv/dirs, dependências, imports e config global\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Preparar o ambiente da execução:\n",
    "- (opcional) Ativar venv se já estiver selecionada no kernel.\n",
    "- Montar Google Drive (se em Colab) e garantir a estrutura de diretórios do projeto.\n",
    "- Centralizar instalação de dependências e imports.\n",
    "- Fixar SEED e timezone; abrir um RUN_DIR com carimbo temporal e salvar metadados.\n",
    "\n",
    "Pontos FIXOS (recomendado manter):\n",
    "- TIMEZONE = \"America/Sao_Paulo\"\n",
    "- Estrutura de diretórios: input/, prerun/, output/, artifacts/, runs/, reports/\n",
    "- Arquivo de metadados: runs/<RUN_ID>/run.json\n",
    "\n",
    "Pontos CALIBRÁVEIS (ajuste conforme sua governança):\n",
    "- PROJ_ROOT (raiz do projeto)\n",
    "- Lista de pacotes mínimos em NEED_PIP\n",
    "- Política de versões (pinning) se exigir reprodutibilidade estrita\n",
    "- SEED\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# §1.0 — Utilidades básicas\n",
    "# =========================\n",
    "import os, sys, json, platform, subprocess, textwrap, random, hashlib, socket, getpass, re, io, base64, time, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Helpers de log \"Skynet\" ----------\n",
    "def _sk(msg: str) -> None:\n",
    "    \"\"\"Logger curto e padronizado para a execução.\"\"\"\n",
    "    print(f\"[skynet {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "def _warn(msg: str) -> None:\n",
    "    warnings.warn(msg, RuntimeWarning, stacklevel=2)\n",
    "# --------------------------------------------\n",
    "\n",
    "# ===================================================\n",
    "# §1.1 — (opcional) venv do kernel / detecção de venv\n",
    "# ===================================================\n",
    "# Observação:\n",
    "# - Em Google Colab, a \"venv\" é o próprio kernel do ambiente (não há ativação de .venv local).\n",
    "# - Em Jupyter local, a venv já vem \"ativada\" quando você escolhe o kernel correspondente.\n",
    "# - Aqui apenas detectamos e registramos a info para auditoria.\n",
    "VENV_INFO = {\n",
    "    \"python_exe\": sys.executable,\n",
    "    \"base_prefix\": sys.base_prefix,\n",
    "    \"prefix\": sys.prefix,\n",
    "    \"venv_active\": (sys.prefix != sys.base_prefix)  # heurística simples\n",
    "}\n",
    "_sk(f\"python: {VENV_INFO['python_exe']}\")\n",
    "_sk(f\"venv ativa? {VENV_INFO['venv_active']}\")\n",
    "\n",
    "# =====================================================\n",
    "# §1.2 — Montagem do Google Drive (se rodando em Colab)\n",
    "# =====================================================\n",
    "IN_COLAB = \"google.colab\" in sys.modules or \"COLAB_GPU\" in os.environ\n",
    "DRIVE_MOUNT = Path(\"/content/drive\")\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive as _colab_drive  # type: ignore\n",
    "        if not DRIVE_MOUNT.exists() or not os.path.ismount(str(DRIVE_MOUNT)):\n",
    "            _sk(\"montando Google Drive…\")\n",
    "            _colab_drive.mount(str(DRIVE_MOUNT))\n",
    "        else:\n",
    "            _sk(\"Google Drive já montado.\")\n",
    "    except Exception as e:\n",
    "        _warn(f\"Não foi possível montar o Google Drive automaticamente: {e}\")\n",
    "\n",
    "# ==================================================\n",
    "# §1.3 — Raiz do projeto e estrutura de diretórios\n",
    "# ==================================================\n",
    "# CALIBRÁVEL: escolha da pasta raiz do projeto (PROJ_ROOT).\n",
    "# Estratégia:\n",
    "#   - Se em Colab com Drive montado, usar uma pasta padrão no MyDrive.\n",
    "#   - Caso contrário, usar a pasta atual + \"ae-tabular\".\n",
    "DEFAULT_DRIVE_ROOT = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "DEFAULT_LOCAL_ROOT = Path.cwd() / \"ae-tabular\"\n",
    "\n",
    "PROJ_ROOT = (\n",
    "    DEFAULT_DRIVE_ROOT if (IN_COLAB and DEFAULT_DRIVE_ROOT.parent.exists())\n",
    "    else DEFAULT_LOCAL_ROOT\n",
    ")\n",
    "os.makedirs(PROJ_ROOT, exist_ok=True)\n",
    "\n",
    "# FIXO: subdiretórios que o pipeline utiliza\n",
    "INPUT_DIR    = PROJ_ROOT / \"input\"      # entrada bruta (não-preprocessado)\n",
    "PRERUN_DIR   = PROJ_ROOT / \"prerun\"     # CSVs tratados (saída da Etapa 2)\n",
    "OUTPUT_DIR   = PROJ_ROOT / \"output\"     # saídas finais (scores, tabelas auxiliares)\n",
    "ARTIF_DIR    = PROJ_ROOT / \"artifacts\"  # artefatos de treino (modelos, features, etc.)\n",
    "RUNS_DIR     = PROJ_ROOT / \"runs\"       # pastas por execução\n",
    "REPORTS_DIR  = PROJ_ROOT / \"reports\"    # relatórios (HTML/PDF/imagens)\n",
    "\n",
    "for d in (INPUT_DIR, PRERUN_DIR, OUTPUT_DIR, ARTIF_DIR, RUNS_DIR, REPORTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_sk(f\"PROJ_ROOT = {PROJ_ROOT}\")\n",
    "_sk(\"estrutura ok (input/, prerun/, output/, artifacts/, runs/, reports/)\")\n",
    "\n",
    "# ================================================\n",
    "# §1.4 — Carimbo, timezone e metadados da execução\n",
    "# ================================================\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo  # py>=3.9\n",
    "    TIMEZONE = \"America/Sao_Paulo\"  # FIXO (use a zona do BNDES quando aplicável)\n",
    "    _now = datetime.now(tz=ZoneInfo(TIMEZONE))\n",
    "except Exception:\n",
    "    TIMEZONE = \"America/Sao_Paulo\"\n",
    "    _now = datetime.now()\n",
    "\n",
    "RUN_ID   = _now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_DIR  = RUNS_DIR / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_sk(f\"RUN_ID = {RUN_ID}\")\n",
    "_sk(f\"RUN_DIR = {RUN_DIR}\")\n",
    "\n",
    "# ==================================\n",
    "# §1.5 — Dependências (pip) + imports\n",
    "# ==================================\n",
    "# CALIBRÁVEL: ajuste a lista se precisar \"pinning\" de versões (ex.: 'pandas==2.2.2')\n",
    "NEED_PIP = [\n",
    "    \"numpy\", \"pandas\", \"pyarrow\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    \"torch\",        # se o ambiente já tiver torch, a instalação será ignorada\n",
    "    \"reportlab\"     # geração de PDF (Etapa 13)\n",
    "]\n",
    "\n",
    "def _pip_install_missing(pkgs):\n",
    "    \"\"\"Tenta importar; se falhar, instala via pip no MESMO Python do kernel.\"\"\"\n",
    "    to_install = []\n",
    "    for spec in pkgs:\n",
    "        name = spec.split(\"==\")[0].split(\">=\")[0].split(\"<=\")[0]\n",
    "        try:\n",
    "            __import__(name)\n",
    "        except Exception:\n",
    "            to_install.append(spec)\n",
    "    if to_install:\n",
    "        _sk(f\"instalando pacotes: {to_install}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *to_install])\n",
    "    else:\n",
    "        _sk(\"todas dependências já presentes.\")\n",
    "\n",
    "_pip_install_missing(NEED_PIP)\n",
    "\n",
    "# Imports centralizados (garante a mesma versão carregada para todo o notebook)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# torch é opcional para CPU/GPU; se não estiver disponível, Etapa 7 falhará cedo com assert\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except Exception:\n",
    "    TORCH_OK = False\n",
    "    _warn(\"PyTorch não disponível — o treino do autoencoder (Etapa 7) exigirá Torch.\")\n",
    "\n",
    "# =========================================\n",
    "# §1.6 — SEED / determinismo (quando viável)\n",
    "# =========================================\n",
    "# CALIBRÁVEL: SEED da execução\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "if TORCH_OK:\n",
    "    try:\n",
    "        torch.manual_seed(SEED)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(SEED)\n",
    "        # Determinismo (atenção: pode impactar performance)\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "        _sk(f\"torch determinístico habilitado (cuda={torch.cuda.is_available()})\")\n",
    "    except Exception as e:\n",
    "        _warn(f\"Determinismo Torch parcial: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# §1.7 — Persistir metadados mínimos (run.json)\n",
    "# ==========================================\n",
    "RUN_META = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"created_at\": _now.isoformat(),\n",
    "    \"timezone\": TIMEZONE,\n",
    "    \"host\": socket.gethostname(),\n",
    "    \"user\": getpass.getuser() if hasattr(getpass, \"getuser\") else None,\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"paths\": {\n",
    "        \"proj_root\": str(PROJ_ROOT),\n",
    "        \"input_dir\": str(INPUT_DIR),\n",
    "        \"prerun_dir\": str(PRERUN_DIR),\n",
    "        \"output_dir\": str(OUTPUT_DIR),\n",
    "        \"artifacts_dir\": str(ARTIF_DIR),\n",
    "        \"runs_dir\": str(RUNS_DIR),\n",
    "        \"reports_dir\": str(REPORTS_DIR),\n",
    "        \"run_dir\": str(RUN_DIR),\n",
    "    },\n",
    "    \"venv\": VENV_INFO,\n",
    "    \"seed\": SEED,\n",
    "    \"deps\": NEED_PIP,\n",
    "}\n",
    "(RUN_DIR / \"run.json\").write_text(\n",
    "    json.dumps(RUN_META, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "_sk(\"metadados salvos em runs/<RUN_ID>/run.json\")\n",
    "\n",
    "# ==========================================\n",
    "# §1.8 — Exibir “resumo” do ambiente preparado\n",
    "# ==========================================\n",
    "print(\"\\n==== §1 — AMBIENTE PRONTO ====\")\n",
    "print(f\"PROJ_ROOT : {PROJ_ROOT}\")\n",
    "print(f\"INPUT_DIR : {INPUT_DIR}\")\n",
    "print(f\"PRERUN_DIR: {PRERUN_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"ARTIF_DIR : {ARTIF_DIR}\")\n",
    "print(f\"RUNS_DIR  : {RUNS_DIR}\")\n",
    "print(f\"REPORTS_DIR: {REPORTS_DIR}\")\n",
    "print(f\"RUN_DIR   : {RUN_DIR}\")\n",
    "print(f\"SEED      : {SEED}\")\n",
    "print(f\"TIMEZONE  : {TIMEZONE}\")\n",
    "print(f\"TORCH_OK  : {TORCH_OK}\")\n",
    "print(\"================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV2gzWLQaIF5"
   },
   "source": [
    "## **Etapa 2:** Utilitário de pré-processamento de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "executionInfo": {
     "elapsed": 23593,
     "status": "ok",
     "timestamp": 1760378869105,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "rZ3mndfXaPqp",
    "outputId": "159134bf-1bda-420b-8655-6084a906d2a8"
   },
   "outputs": [],
   "source": [
    "# @title §2 — Pré-processamento de CSVs (Google Drive → prerun/)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Permitir ao usuário escolher um CSV (UTF-8 com BOM, separador ';') a partir de\n",
    "PROJ_ROOT/input, aplicar tratamentos mínimos padronizados e salvar o resultado\n",
    "em PROJ_ROOT/prerun, junto com um relatório JSON de evidências.\n",
    "\n",
    "Pontos FIXOS:\n",
    "- Diretórios: INPUT_DIR (origem), PRERUN_DIR (destino), RUN_DIR (logs)\n",
    "- Formato exigido: encoding = \"utf-8-sig\", sep = \";\"\n",
    "- Logger: _sk(...)\n",
    "\n",
    "Pontos CALIBRÁVEIS:\n",
    "- REQUIRED_COLS: colunas mínimas esperadas para o domínio contábil (ajuste conforme seu dicionário)\n",
    "- RENAME_MAP: padronização de nomes de colunas\n",
    "- NUMERIC_COLS: colunas a serem convertidas para numérico (com vírgula -> ponto)\n",
    "- DATE_COLS: colunas de data que serão parseadas para ISO (YYYY-MM-DD)\n",
    "- NORMALIZADORES: padronização de valores categóricos (ex.: 'd'/'c', capitalização)\n",
    "\n",
    "Saídas:\n",
    "- CSV tratado em PRERUN_DIR com sufixo \"-clean.csv\"\n",
    "- Snapshot em Parquet (.parquet) para carga mais rápida\n",
    "- Relatório JSON com estatísticas e contagens de ajustes\n",
    "\"\"\"\n",
    "\n",
    "import json, re, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------- Pré-checagens ----------\n",
    "assert 'INPUT_DIR' in globals() and 'PRERUN_DIR' in globals() and 'RUN_DIR' in globals(), \\\n",
    "    \"Execute a Etapa 1 antes (dirs e run.json).\"\n",
    "assert isinstance(INPUT_DIR, Path) and isinstance(PRERUN_DIR, Path), \"INPUT_DIR/PRERUN_DIR inválidos.\"\n",
    "\n",
    "CSV_ENCODING_REQ = \"utf-8-sig\"  # FIXO: exigência com BOM\n",
    "CSV_SEP_REQ      = \";\"          # FIXO: separador exigido\n",
    "\n",
    "# --------- Domínio contábil (CALIBRÁVEL) ----------\n",
    "# Se seu dicionário for outro, ajuste aqui:\n",
    "REQUIRED_COLS = [\n",
    "    \"username\",       # usuário que lançou\n",
    "    \"lotacao\",        # unidade funcional\n",
    "    \"data_lcto\",      # data do lançamento (formato livre; será parseada)\n",
    "    \"valormi\",        # valor monetário (texto com vírgula/ponto será normalizado)\n",
    "    \"dc\",             # débito/crédito ('d'/'c')\n",
    "    \"contacontabil\",  # código COSIF (apenas dígitos)\n",
    "    \"nome_conta\",     # descrição da conta\n",
    "    \"documento_num\"   # identificador do documento\n",
    "]\n",
    "\n",
    "# Muitos arquivos trazem nomes alternativos; normalize aqui:\n",
    "RENAME_MAP = {  # CALIBRÁVEL: mapeie variações comuns -> padrão desejado\n",
    "    \"usuario\": \"username\",\n",
    "    \"user\": \"username\",\n",
    "    \"lotação\": \"lotacao\",\n",
    "    \"data\": \"data_lcto\",\n",
    "    \"data_lancamento\": \"data_lcto\",\n",
    "    \"valor\": \"valormi\",\n",
    "    \"debito_credito\": \"dc\",\n",
    "    \"d_c\": \"dc\",\n",
    "    \"conta_contabil\": \"contacontabil\",\n",
    "    \"conta\": \"contacontabil\",\n",
    "    \"nome_da_conta\": \"nome_conta\",\n",
    "    \"documento\": \"documento_num\",\n",
    "    \"num_documento\": \"documento_num\",\n",
    "}\n",
    "\n",
    "# Colunas a tratar como numéricas (vírgula brasileira -> ponto) — CALIBRÁVEL\n",
    "NUMERIC_COLS = [\"valormi\"]\n",
    "\n",
    "# Colunas de data a parsear — CALIBRÁVEL\n",
    "DATE_COLS = [\"data_lcto\"]\n",
    "\n",
    "# Normalização de 'dc' — CALIBRÁVEL\n",
    "DC_MAP = {\n",
    "    \"d\": \"d\", \"deb\": \"d\", \"debito\": \"d\", \"débito\": \"d\",\n",
    "    \"c\": \"c\", \"cred\": \"c\", \"credito\": \"c\", \"crédito\": \"c\",\n",
    "}\n",
    "\n",
    "# ----------------- Funções utilitárias (com comentários) -----------------\n",
    "def _has_utf8_bom(path: Path) -> bool:\n",
    "    \"\"\"Checa os 3 primeiros bytes por BOM UTF-8.\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        head = f.read(3)\n",
    "    return head == b\"\\xef\\xbb\\xbf\"\n",
    "\n",
    "def _validate_csv_format(path: Path) -> dict:\n",
    "    \"\"\"Valida encoding (BOM) e separador ';' de forma barata (amostra inicial).\"\"\"\n",
    "    ok_bom = _has_utf8_bom(path)\n",
    "    # Teste de separador por amostragem rápida de linha 1\n",
    "    with open(path, \"rb\") as f:\n",
    "        first_line = f.readline().decode(\"utf-8\", errors=\"ignore\")\n",
    "    sep_count = first_line.count(\";\")\n",
    "    return {\"utf8_bom\": ok_bom, \"sep_semicolon\": (sep_count >= 1), \"sep_count_header\": sep_count}\n",
    "\n",
    "def _read_csv_strict(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lê CSV exigindo 'utf-8-sig' e sep=';'.\n",
    "    - dtype=str para preservar valores originais; conversões vêm depois.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path, sep=CSV_SEP_REQ, encoding=CSV_ENCODING_REQ, dtype=str)\n",
    "\n",
    "def _strip_all(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove espaços nas bordas de strings; colapsa espaços múltiplos internos.\n",
    "    \"\"\"\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            df[c] = df[c].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "            df[c] = df[c].replace({\"nan\": \"\", \"None\": \"\", \"NULL\": \"\"})\n",
    "    return df\n",
    "\n",
    "def _rename_columns(df: pd.DataFrame, mapping: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Renomeia colunas segundo RENAME_MAP; normaliza para minúsculas sem acentos.\n",
    "    \"\"\"\n",
    "    # título \"cru\" -> sem acento e minúsculo\n",
    "    def _norm(name: str) -> str:\n",
    "        s = name.strip()\n",
    "        s = re.sub(r\"[^\\w\\s]\", \"_\", s, flags=re.UNICODE)  # troca pontuação por \"_\"\n",
    "        s = re.sub(r\"\\s+\", \"_\", s)\n",
    "        s = s.lower()\n",
    "        # remoção simples de acentos\n",
    "        s = (s\n",
    "             .replace(\"á\",\"a\").replace(\"à\",\"a\").replace(\"ã\",\"a\").replace(\"â\",\"a\").replace(\"ä\",\"a\")\n",
    "             .replace(\"é\",\"e\").replace(\"ê\",\"e\").replace(\"è\",\"e\").replace(\"ë\",\"e\")\n",
    "             .replace(\"í\",\"i\").replace(\"ì\",\"i\").replace(\"î\",\"i\").replace(\"ï\",\"i\")\n",
    "             .replace(\"ó\",\"o\").replace(\"ô\",\"o\").replace(\"õ\",\"o\").replace(\"ò\",\"o\").replace(\"ö\",\"o\")\n",
    "             .replace(\"ú\",\"u\").replace(\"ù\",\"u\").replace(\"û\",\"u\").replace(\"ü\",\"u\")\n",
    "             .replace(\"ç\",\"c\")\n",
    "            )\n",
    "        return s\n",
    "\n",
    "    norm_map = {c: _norm(c) for c in df.columns}\n",
    "    df = df.rename(columns=norm_map)\n",
    "\n",
    "    # aplica RENAME_MAP pós-normalização\n",
    "    df = df.rename(columns={src: dst for src, dst in mapping.items() if src in df.columns})\n",
    "    return df\n",
    "\n",
    "from typing import Optional  # adicione perto dos outros imports da célula\n",
    "\n",
    "def _to_float_br(s: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Converte string monetária pt-BR para float.\n",
    "    Retorna None quando vazio/inválido (pandas converte para NaN depois).\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    t = str(s).strip()\n",
    "    if t == \"\":\n",
    "        return None\n",
    "    if \",\" in t:\n",
    "        t = t.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    t = t.replace(\" \", \"\")\n",
    "    try:\n",
    "        return float(t)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _normalize_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizações de valores:\n",
    "    - NUMERIC_COLS -> float (pt-BR).\n",
    "    - DATE_COLS -> ISO 'YYYY-MM-DD' (coerção 'coerce' para valores inválidos).\n",
    "    - 'dc' -> {'d','c'}\n",
    "    - 'contacontabil' -> apenas dígitos\n",
    "    \"\"\"\n",
    "    # Números\n",
    "    for c in NUMERIC_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].map(_to_float_br).astype(float)\n",
    "\n",
    "    # Datas\n",
    "    for c in DATE_COLS:\n",
    "        if c in df.columns:\n",
    "            # tenta múltiplos formatos comuns BR/ISO\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "            df[c] = df[c].dt.date.astype(\"string\")  # ISO simples 'YYYY-MM-DD' ou <NA>\n",
    "\n",
    "    # Débito/Crédito\n",
    "    if \"dc\" in df.columns:\n",
    "        df[\"dc\"] = df[\"dc\"].astype(str).str.lower().str.strip()\n",
    "        df[\"dc\"] = df[\"dc\"].map(lambda x: DC_MAP.get(x, x))\n",
    "        df.loc[~df[\"dc\"].isin([\"d\",\"c\"]), \"dc\"] = \"\"  # zera inválidos; serão reportados\n",
    "\n",
    "    # COSIF (somente dígitos)\n",
    "    if \"contacontabil\" in df.columns:\n",
    "        df[\"contacontabil\"] = df[\"contacontabil\"].astype(str).str.replace(r\"\\D+\", \"\", regex=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def _validate_required_cols(df: pd.DataFrame, required: list[str]) -> list[str]:\n",
    "    \"\"\"Retorna lista de colunas faltantes (se houver).\"\"\"\n",
    "    cols = list(df.columns)\n",
    "    return [c for c in required if c not in cols]\n",
    "\n",
    "def _report_stats(original_path: Path, df_raw: pd.DataFrame, df_clean: pd.DataFrame, fmt_check: dict) -> dict:\n",
    "    \"\"\"Gera um dicionário com métricas de evidência do pré-processamento.\"\"\"\n",
    "    stats = {\n",
    "        \"source_file\": str(original_path),\n",
    "        \"size_bytes\": original_path.stat().st_size,\n",
    "        \"format_check\": fmt_check,\n",
    "        \"rows_raw\": int(len(df_raw)),\n",
    "        \"cols_raw\": int(df_raw.shape[1]),\n",
    "        \"rows_clean\": int(len(df_clean)),\n",
    "        \"cols_clean\": int(df_clean.shape[1]),\n",
    "        \"na_counts\": {c: int(df_clean[c].isna().sum()) for c in df_clean.columns},\n",
    "        \"empty_counts\": {c: int((df_clean[c].astype(str)==\"\").sum()) for c in df_clean.columns},\n",
    "        \"dc_invalid_count\": int((\"dc\" in df_clean.columns) and (~df_clean[\"dc\"].isin([\"d\",\"c\"])).sum()),\n",
    "        \"valormi_na_count\": int((\"valormi\" in df_clean.columns) and df_clean[\"valormi\"].isna().sum()),\n",
    "        \"conta_non_digit_count\": int((\"contacontabil\" in df_clean.columns) and (~df_clean[\"contacontabil\"].str.fullmatch(r\"\\d+\")).sum())\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# ----------------- Fluxo principal da Etapa 2 -----------------\n",
    "# 1) Listar CSVs no INPUT_DIR\n",
    "csvs = sorted(INPUT_DIR.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not csvs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Não há CSVs em {INPUT_DIR}. Coloque um CSV (UTF-8 BOM, ';') no diretório e rode novamente.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n[§2] CSVs disponíveis em INPUT_DIR:\")\n",
    "for i, p in enumerate(csvs):\n",
    "    print(f\"[{i:03d}] {p.name}\")\n",
    "\n",
    "# 2) Escolher índice\n",
    "idx = None\n",
    "while idx is None:\n",
    "    try:\n",
    "        idx = int(input(\"\\nÍndice do CSV para pré-processar: \").strip())\n",
    "        assert 0 <= idx < len(csvs)\n",
    "    except Exception:\n",
    "        print(\"Índice inválido. Tente novamente.\")\n",
    "        idx = None\n",
    "\n",
    "SRC_PATH = csvs[idx]\n",
    "_sk(f\"arquivo selecionado: {SRC_PATH.name}\")\n",
    "\n",
    "# 3) Validação de formato (BOM + separador)\n",
    "fmt = _validate_csv_format(SRC_PATH)\n",
    "if not fmt[\"utf8_bom\"] or not fmt[\"sep_semicolon\"]:\n",
    "    raise ValueError(\n",
    "        f\"Formato inválido: utf8_bom={fmt['utf8_bom']}, sep_semicolon={fmt['sep_semicolon']} \"\n",
    "        f\"(esperado: BOM UTF-8 e ';').\"\n",
    "    )\n",
    "\n",
    "# 4) Leitura estrita, padronização de nomes e limpeza\n",
    "df0 = _read_csv_strict(SRC_PATH)\n",
    "df1 = _rename_columns(df0, RENAME_MAP)\n",
    "df1 = _strip_all(df1)\n",
    "\n",
    "# 5) Normalizações de valores (número, data, dc, cosif)\n",
    "df2 = _normalize_values(df1)\n",
    "\n",
    "# 6) Checagem de colunas obrigatórias (domínio) — gera aviso se faltar\n",
    "missing = _validate_required_cols(df2, REQUIRED_COLS)\n",
    "if missing:\n",
    "    _sk(f\"ATENÇÃO: colunas obrigatórias ausentes: {missing}. O arquivo será salvo, mas isso pode bloquear etapas posteriores.\")\n",
    "\n",
    "# 7) Persistência em prerun/ + relatório\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "base   = SRC_PATH.stem\n",
    "dst_csv = PRERUN_DIR / f\"{base}-clean-{stamp}.csv\"\n",
    "dst_parq= PRERUN_DIR / f\"{base}-clean-{stamp}.parquet\"\n",
    "dst_rep = RUN_DIR    / f\"preprocess_report_{base}-{stamp}.json\"\n",
    "\n",
    "# CSV: garantir formato exigido para as próximas etapas\n",
    "out = df2.copy()\n",
    "# (Opcional) formatar valormi com 2 casas ao salvar; mantém numérico no DataFrame\n",
    "if \"valormi\" in out.columns:\n",
    "    out[\"valormi\"] = out[\"valormi\"].map(lambda x: (\"\" if pd.isna(x) else f\"{x:.2f}\"))\n",
    "\n",
    "out.to_csv(dst_csv, index=False, sep=CSV_SEP_REQ, encoding=CSV_ENCODING_REQ)\n",
    "df2.to_parquet(dst_parq, index=False)\n",
    "\n",
    "report = _report_stats(SRC_PATH, df0, df2, fmt)\n",
    "report[\"required_cols_missing\"] = missing\n",
    "dst_rep.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "_sk(f\"pré-processamento concluído → {dst_csv.name}\")\n",
    "_sk(f\"snapshot parquet → {dst_parq.name}\")\n",
    "_sk(f\"relatório JSON → {dst_rep.name}\")\n",
    "\n",
    "# 8) Amostra para inspeção rápida\n",
    "print(\"\\n[§2] Pré-visualização (5 linhas):\")\n",
    "display(df2.head(5))\n",
    "\n",
    "# 9) Dica para próxima etapa\n",
    "print(\"\\nPróximo passo: Etapa 3 — selecionar um arquivo de PRERUN_DIR para TREINO/VAL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de2SoMqgdcRq"
   },
   "source": [
    "## **Etapa 3:** Ingestão de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1cl-D10gdwpU"
   },
   "outputs": [],
   "source": [
    "# @title §3 — Ingestão de treino (somente CSVs pré-processados de prerun/) — seleção explícita\n",
    "# -*- coding: utf-8 -*-\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "assert 'RUN_DIR' in globals() and 'PRERUN_DIR' in globals(), \"Execute as Etapas 1 e 2 antes.\"\n",
    "\n",
    "CSV_ENCODING = \"utf-8-sig\"   # FIXO\n",
    "CSV_SEP      = \";\"           # FIXO\n",
    "\n",
    "REQUIRED_COLS = [\"username\", \"lotacao\", \"data_lcto\", \"valormi\", \"dc\", \"contacontabil\"]  # CALIBRÁVEL\n",
    "DC_VALIDOS = {\"d\", \"c\"}  # FIXO\n",
    "\n",
    "def carregar_validar_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=CSV_SEP, encoding=CSV_ENCODING, dtype=str)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    df = df.apply(lambda col: col.str.strip() if col.dtype == object else col)\n",
    "\n",
    "    def _to_float(v):\n",
    "        if v is None: return np.nan\n",
    "        s = str(v).strip()\n",
    "        if s == \"\": return np.nan\n",
    "        if \",\" in s: s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        s = s.replace(\" \", \"\")\n",
    "        try: return float(s)\n",
    "        except Exception: return np.nan\n",
    "\n",
    "    if \"valormi\" in df.columns:\n",
    "        df[\"valormi\"] = df[\"valormi\"].map(_to_float).astype(float)\n",
    "\n",
    "    if \"dc\" in df.columns:\n",
    "        df[\"dc\"] = df[\"dc\"].astype(str).str.lower().str.strip()\n",
    "        df.loc[~df[\"dc\"].isin(DC_VALIDOS), \"dc\"] = \"\"\n",
    "\n",
    "    if \"contacontabil\" in df.columns:\n",
    "        df[\"contacontabil\"] = df[\"contacontabil\"].astype(str).str.replace(r\"\\D+\", \"\", regex=True)\n",
    "\n",
    "    faltantes = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if faltantes:\n",
    "        raise ValueError(f\"Colunas obrigatórias ausentes: {faltantes}\")\n",
    "\n",
    "    problemas = {}\n",
    "    if df[\"dc\"].eq(\"\").any(): problemas[\"dc_invalidos\"] = int(df[\"dc\"].eq(\"\").sum())\n",
    "    if df[\"valormi\"].isna().any(): problemas[\"valormi_na\"] = int(df[\"valormi\"].isna().sum())\n",
    "    if (~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).any():\n",
    "        problemas[\"contacontabil_nao_numerica\"] = int((~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).sum())\n",
    "    if problemas:\n",
    "        rep_path = RUN_DIR / f\"validacao_ingestao_{path.stem}.json\"\n",
    "        rep_path.write_text(json.dumps(problemas, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"[§3] Aviso: inconsistências registradas em {rep_path.name}\")\n",
    "    return df\n",
    "\n",
    "def _human_size(b: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if b < 1024: return f\"{b:.1f}{unit}\"\n",
    "        b /= 1024\n",
    "    return f\"{b:.1f}PB\"\n",
    "\n",
    "# ---------- listagem e filtro opcional ----------\n",
    "pr_list_all = sorted(PRERUN_DIR.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not pr_list_all:\n",
    "    raise FileNotFoundError(\"Nenhum CSV encontrado em prerun/. Rode a Etapa 2 primeiro.\")\n",
    "\n",
    "def _mostrar(lista):\n",
    "    print(\"\\n[§3] CSVs em prerun/:\")\n",
    "    for i, p in enumerate(lista):\n",
    "        ts = datetime.fromtimestamp(p.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{i:03d}] {p.name:60s}  { _human_size(p.stat().st_size):>8s}  mtime={ts}\")\n",
    "\n",
    "# loop de seleção sem default\n",
    "while True:\n",
    "    _mostrar(pr_list_all)\n",
    "    termo = input(\"\\n(Facultativo) Digite um termo para filtrar por nome, ou tecle Enter para listar todos: \").strip()\n",
    "    if termo:\n",
    "        pr_list = [p for p in pr_list_all if termo.lower() in p.name.lower()]\n",
    "        if not pr_list:\n",
    "            print(\"Nenhum arquivo corresponde ao filtro. Tente novamente.\")\n",
    "            continue\n",
    "    else:\n",
    "        pr_list = pr_list_all\n",
    "\n",
    "    _mostrar(pr_list)\n",
    "    raw = input(\"\\nDigite o ÍNDICE do CSV a usar para TREINO/VAL (ou 'x' para recomeçar o filtro): \").strip().lower()\n",
    "    if raw == \"x\":\n",
    "        continue\n",
    "    try:\n",
    "        idx = int(raw)\n",
    "        assert 0 <= idx < len(pr_list)\n",
    "    except Exception:\n",
    "        print(\"Índice inválido. Tente novamente.\")\n",
    "        continue\n",
    "\n",
    "    selecionado = pr_list[idx]\n",
    "    print(f\"\\nSelecionado: {selecionado.name}\")\n",
    "    ok = input(\"Confirma este arquivo? (s/n): \").strip().lower()\n",
    "    if ok == \"s\":\n",
    "        SELECTED_CSV = selecionado\n",
    "        break\n",
    "    else:\n",
    "        print(\"Seleção cancelada. Reiniciando…\")\n",
    "\n",
    "print(f\"[§3] Arquivo confirmado: {SELECTED_CSV.name}\")\n",
    "\n",
    "# ---------- ingestão e rastreabilidade ----------\n",
    "DF_RAW = carregar_validar_csv(SELECTED_CSV)\n",
    "print(f\"[§3] Linhas carregadas: {len(DF_RAW):,}\")\n",
    "\n",
    "pad_path = RUN_DIR / \"selected_source.csv\"\n",
    "DF_RAW.to_csv(pad_path, index=False, sep=CSV_SEP, encoding=CSV_ENCODING)\n",
    "snap_path = RUN_DIR / \"journal_entries.parquet\"\n",
    "DF_RAW.to_parquet(snap_path, index=False)\n",
    "\n",
    "print(f\"[§3] cópias salvas: {pad_path.name}, {snap_path.name}\")\n",
    "display(DF_RAW.head(5))\n",
    "\n",
    "print(\"\\nPróximo passo: Etapa 4 — criação do vocabulário de treino.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIhjPdBreCqJ"
   },
   "source": [
    "## **Etapa 4:** Vocabulário de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cfNCS5txeBo8"
   },
   "outputs": [],
   "source": [
    "# @title §4 — Vocabulário de treino (categoria → inteiro) — congelado\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Criar e congelar um vocabulário (mapeamento categoria → índice inteiro) para as\n",
    "colunas categóricas do dataset de TREINO (DF_RAW).\n",
    "\n",
    "Como funciona\n",
    "-------------\n",
    "1) Define a lista de colunas categóricas (CALIBRÁVEL em CAT_COLS).\n",
    "2) Para cada coluna:\n",
    "   - Calcula frequências no DF_RAW.\n",
    "   - Ordena por (freq desc, depois valor asc) para determinismo.\n",
    "   - Reserva índice 0 para OOV (\"out-of-vocabulary\").\n",
    "   - Atribui índices a partir de 1 às categorias vistas no treino.\n",
    "3) Salva artefatos:\n",
    "   - JSON com mapas por coluna: runs/<RUN_ID>/categorical_maps.json\n",
    "   - Resumo de cardinalidades: runs/<RUN_ID>/categorical_cardinality.json\n",
    "   - (opcional) cópia durável em artifacts/: artifacts/categorical_maps_latest.json\n",
    "4) Expõe helpers:\n",
    "   - encode_categoricals(df) → adiciona colunas *_int usando o vocabulário\n",
    "   - decode_category(col, idx) → retorna string a partir do índice\n",
    "\n",
    "Pontos FIXOS\n",
    "------------\n",
    "- OOV = 0\n",
    "- Ordem determinística: (-freq, categoria)\n",
    "\n",
    "Pontos CALIBRÁVEIS\n",
    "------------------\n",
    "- CAT_COLS: escolha de colunas categóricas\n",
    "- Se incluir 'documento_num' (alta cardinalidade) depende da estratégia de features.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Pré-checagens\n",
    "assert 'DF_RAW' in globals(), \"DF_RAW não encontrado. Execute a Etapa 3 antes.\"\n",
    "assert 'RUN_DIR' in globals() and 'ARTIF_DIR' in globals(), \"Execute a Etapa 1 antes.\"\n",
    "\n",
    "# ========= Configuração (CALIBRÁVEL) =========\n",
    "# Escolha das colunas categóricas.\n",
    "# Sugestão: incluir identificadores estáveis e sinais contábeis; evitar IDs\n",
    "# muito voláteis caso não sejam úteis ao modelo.\n",
    "CAT_COLS = [\n",
    "    \"username\",\n",
    "    \"lotacao\",\n",
    "    \"dc\",\n",
    "    \"contacontabil\",\n",
    "    # \"nome_conta\",      # ative se quiser usar descrição textual\n",
    "    # \"documento_num\",   # cuidado: alta cardinalidade; ative se fizer sentido\n",
    "]\n",
    "\n",
    "OOV_INDEX = 0   # FIXO: índice reservado para 'desconhecidos'\n",
    "OFFSET    = 1   # FIXO: categorias vistas começam em 1\n",
    "\n",
    "# ========= Construção do vocabulário =========\n",
    "categorical_maps: dict[str, dict[str, int]] = {}\n",
    "categorical_cardinality: dict[str, int] = {}\n",
    "\n",
    "def _build_map(series: pd.Series) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Constrói o mapa categoria->índice:\n",
    "    - Conta frequências (com NaNs tratados como string vazia para consistência).\n",
    "    - Ordena por frequência desc, depois valor asc.\n",
    "    - Reserva 0 para OOV, categorias começam em 1.\n",
    "    \"\"\"\n",
    "    s = series.fillna(\"\").astype(str)\n",
    "    freq = Counter(s.tolist())\n",
    "\n",
    "    # Remove a categoria vazia \"\" do ranking se quiser (opcional):\n",
    "    # freq.pop(\"\", None)\n",
    "\n",
    "    # Ordenação determinística: mais frequentes primeiro; empate por ordem alfabética\n",
    "    ordered = sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "\n",
    "    mapping = {\"__oov__\": OOV_INDEX, \"__offset__\": OFFSET}\n",
    "    idx = OFFSET\n",
    "    for cat, _count in ordered:\n",
    "        mapping[cat] = idx\n",
    "        idx += 1\n",
    "    return mapping\n",
    "\n",
    "# Constrói mapas por coluna\n",
    "for col in CAT_COLS:\n",
    "    if col not in DF_RAW.columns:\n",
    "        raise ValueError(f\"Coluna categórica '{col}' não existe no DF_RAW.\")\n",
    "    cmap = _build_map(DF_RAW[col])\n",
    "    categorical_maps[col] = cmap\n",
    "    # cardinalidade = total de índices \"válidos\" (exclui OOV)\n",
    "    categorical_cardinality[col] = len(cmap) - 2  # remove chaves meta (__oov__, __offset__)\n",
    "\n",
    "# ========= Persistência dos artefatos =========\n",
    "maps_path_run = RUN_DIR / \"categorical_maps.json\"\n",
    "card_path_run = RUN_DIR / \"categorical_cardinality.json\"\n",
    "maps_path_art = ARTIF_DIR / \"categorical_maps_latest.json\"\n",
    "\n",
    "maps_path_run.write_text(json.dumps(categorical_maps, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "card_path_run.write_text(json.dumps(categorical_cardinality, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "maps_path_art.write_text(json.dumps(categorical_maps, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[§4] vocabulário salvo em: {maps_path_run.name}  (cópia: artifacts/{maps_path_art.name})\")\n",
    "print(f\"[§4] cardinalidades salvas em: {card_path_run.name}\")\n",
    "\n",
    "# ========= Helpers de codificação/decodificação =========\n",
    "def encode_categoricals(df: pd.DataFrame,\n",
    "                        cat_cols: list[str] = None,\n",
    "                        maps: dict[str, dict[str, int]] = None,\n",
    "                        suffix: str = \"_int\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Projeta colunas categóricas para índices inteiros usando os mapas congelados.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame de entrada\n",
    "    - cat_cols: lista de colunas categóricas (default = CAT_COLS)\n",
    "    - maps: dicionário de mapas (default = categorical_maps deste run)\n",
    "    - suffix: sufixo para colunas codificadas (default: '_int')\n",
    "\n",
    "    Saída:\n",
    "    - DataFrame com novas colunas <col><suffix> (inteiros).\n",
    "    \"\"\"\n",
    "    if cat_cols is None:\n",
    "        cat_cols = CAT_COLS\n",
    "    if maps is None:\n",
    "        maps = categorical_maps\n",
    "\n",
    "    out = df.copy()\n",
    "    for col in cat_cols:\n",
    "        if col not in out.columns:\n",
    "            raise ValueError(f\"Coluna '{col}' não encontrada no DF para codificar.\")\n",
    "        cmap = maps.get(col)\n",
    "        if cmap is None:\n",
    "            raise ValueError(f\"Mapa não encontrado para coluna '{col}'.\")\n",
    "        oov = cmap.get(\"__oov__\", 0)\n",
    "        series = out[col].fillna(\"\").astype(str)\n",
    "        out[col + suffix] = series.map(lambda x: cmap.get(x, oov)).astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def decode_category(col: str, idx: int,\n",
    "                    maps: dict[str, dict[str, int]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Decodifica um índice inteiro de volta à categoria (quando possível).\n",
    "    - Retorna '<OOV>' para índices não mapeados ou OOV=0.\n",
    "    \"\"\"\n",
    "    if maps is None:\n",
    "        maps = categorical_maps\n",
    "    cmap = maps.get(col)\n",
    "    if not cmap:\n",
    "        return \"<OOV>\"\n",
    "    if idx == cmap.get(\"__oov__\", 0):\n",
    "        return \"<OOV>\"\n",
    "    # constrói cache reverso simples\n",
    "    rev = {v: k for k, v in cmap.items() if not k.startswith(\"__\")}\n",
    "    return rev.get(idx, \"<OOV>\")\n",
    "\n",
    "# ========= Demonstração rápida (opcional) =========\n",
    "demo_cols = \", \".join(CAT_COLS)\n",
    "print(f\"[§4] colunas categóricas configuradas: {demo_cols}\")\n",
    "for col in CAT_COLS:\n",
    "    print(f\"[§4] {col}: cardinalidade = {categorical_cardinality[col]} (OOV=0, offset={OFFSET})\")\n",
    "\n",
    "# Mantém DF_RAW na memória para próxima etapa\n",
    "print(\"\\nPróximo passo: Etapa 5 — Engenharia de Features (usando os *_int gerados aqui).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBReg_7YeSVj"
   },
   "source": [
    "## **Etapa 5:** Engenharia de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LYBTlu2IeSdB"
   },
   "outputs": [],
   "source": [
    "# @title §5 — Engenharia de Features (transformação tabular para treino)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Transformar o DF_RAW em uma tabela de treino numérica, aplicando:\n",
    "1. Codificação das variáveis categóricas (via vocabulário da Etapa 4)\n",
    "2. Criação de variáveis numéricas derivadas\n",
    "3. Combinação final das features numéricas e categóricas\n",
    "4. Persistência no RUN_DIR\n",
    "\n",
    "Pontos FIXOS:\n",
    "- Carrega DF_RAW e categorical_maps da Etapa 4.\n",
    "- Todas as novas colunas criadas recebem prefixos claros (feat_*).\n",
    "- Cria arquivo 'features_preview.csv' para inspeção.\n",
    "\n",
    "Pontos CALIBRÁVEIS:\n",
    "- NUMERIC_BASE_COLS: colunas base a tratar como numéricas (antes de engenharia)\n",
    "- FEATURE_DERIVATIONS: funções/transformações adicionais (log, z-score, etc.)\n",
    "- CAT_SUFFIX: sufixo para colunas categóricas inteiras\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "assert 'DF_RAW' in globals(), \"DF_RAW ausente. Execute a Etapa 3.\"\n",
    "assert 'categorical_maps' in globals(), \"Vocabulário não encontrado. Execute a Etapa 4.\"\n",
    "assert 'RUN_DIR' in globals(), \"RUN_DIR ausente. Execute a Etapa 1.\"\n",
    "\n",
    "# ========= Configurações básicas =========\n",
    "NUMERIC_BASE_COLS = [\"valormi\"]  # FIXO\n",
    "CAT_SUFFIX = \"_int\"              # FIXO (compatível com Etapa 4)\n",
    "FEATURE_DERIVATIONS = True       # CALIBRÁVEL: ativa derivadas padrão\n",
    "\n",
    "# Copia para evitar mutação acidental\n",
    "df = DF_RAW.copy()\n",
    "\n",
    "# ========= 1) Codificação das categóricas =========\n",
    "df = encode_categoricals(df, cat_cols=list(categorical_maps.keys()), maps=categorical_maps, suffix=CAT_SUFFIX)\n",
    "\n",
    "# ========= 2) Criação de derivadas numéricas =========\n",
    "if FEATURE_DERIVATIONS:\n",
    "    # log1p do valor (mitiga escala e skew)\n",
    "    df[\"feat_log_valormi\"] = np.log1p(df[\"valormi\"].abs())\n",
    "\n",
    "    # sinal de débito/crédito (1 = débito, -1 = crédito)\n",
    "    if \"dc\" in df.columns:\n",
    "        df[\"feat_sign_dc\"] = df[\"dc\"].map({\"d\": 1, \"c\": -1}).fillna(0).astype(int)\n",
    "\n",
    "    # comprimento da conta COSIF (proxy de granularidade)\n",
    "    if \"contacontabil\" in df.columns:\n",
    "        df[\"feat_len_conta\"] = df[\"contacontabil\"].astype(str).str.len().astype(int)\n",
    "\n",
    "    # dia da semana e mês (sazonalidade)\n",
    "    if \"data_lcto\" in df.columns:\n",
    "        _dates = pd.to_datetime(df[\"data_lcto\"], errors=\"coerce\")\n",
    "        df[\"feat_weekday\"] = _dates.dt.weekday.fillna(-1).astype(int)\n",
    "        df[\"feat_month\"]   = _dates.dt.month.fillna(0).astype(int)\n",
    "\n",
    "    # estatísticas locais por usuário (proxy de comportamento)\n",
    "    if \"username\" in df.columns:\n",
    "        user_group = df.groupby(\"username\")[\"valormi\"]\n",
    "        df[\"feat_user_mean\"] = df[\"username\"].map(user_group.mean())\n",
    "        df[\"feat_user_std\"]  = df[\"username\"].map(user_group.std().fillna(0))\n",
    "        df[\"feat_user_cnt\"]  = df[\"username\"].map(user_group.count())\n",
    "else:\n",
    "    print(\"[§5] Derivadas numéricas desativadas por configuração.\")\n",
    "\n",
    "# ========= 3) Seleção final das colunas de treino =========\n",
    "# Inclui colunas *_int e todas feat_*\n",
    "feature_cols = [c for c in df.columns if c.endswith(CAT_SUFFIX) or c.startswith(\"feat_\")]\n",
    "df_features = df[feature_cols].copy()\n",
    "\n",
    "# ========= 4) Persistência e logs =========\n",
    "feat_preview_path = RUN_DIR / \"features_preview.csv\"\n",
    "df_features.head(100).to_csv(feat_preview_path, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# salva configuração de features\n",
    "cfg = {\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"n_features\": len(feature_cols),\n",
    "    \"source_rows\": len(df),\n",
    "    \"derivations_active\": FEATURE_DERIVATIONS,\n",
    "}\n",
    "cfg_path = RUN_DIR / \"features_config.json\"\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[§5] features derivadas e categóricas criadas ({len(feature_cols)} colunas).\")\n",
    "print(f\"[§5] prévia salva em: {feat_preview_path.name}\")\n",
    "print(f\"[§5] config salva em: {cfg_path.name}\")\n",
    "\n",
    "# Mantém em memória para próxima etapa\n",
    "FEATURE_COLS = feature_cols\n",
    "DF_FEATURES = df_features\n",
    "\n",
    "print(\"\\nPróximo passo: Etapa 6 — limpeza, normalização e split train/val.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kUde_I8eik4"
   },
   "source": [
    "## **Etapa 6:** Limpeza, normalização e split treino/validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OZr4KLO_emhO"
   },
   "outputs": [],
   "source": [
    "# @title §6 — Limpeza, imputação, normalização e split (train/val)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Preparar a matriz numérica para o Autoencoder:\n",
    "1) Montar X a partir de DF_FEATURES/FEATURE_COLS\n",
    "2) Split train/val com semente fixa\n",
    "3) Imputação (mediana) treinada somente no treino\n",
    "4) Normalização (StandardScaler) treinada somente no treino\n",
    "5) Persistir artefatos e conjuntos prontos para a Etapa 7\n",
    "\n",
    "Pontos FIXOS:\n",
    "- Imputador = SimpleImputer(strategy=\"median\")\n",
    "- Scaler = StandardScaler()\n",
    "- Artefatos em RUN_DIR (com cópias úteis no ARTIF_DIR)\n",
    "\n",
    "Pontos CALIBRÁVEIS:\n",
    "- TEST_SIZE (fração para validação)\n",
    "- SHUFFLE (embaralhamento antes do split)\n",
    "- RANDOM_STATE (usa SEED global por padrão)\n",
    "- dtype das matrizes (float32 vs float64)\n",
    "\"\"\"\n",
    "\n",
    "import json, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# --------------------------------\n",
    "# Pré-requisitos e configurações\n",
    "# --------------------------------\n",
    "assert 'DF_FEATURES' in globals() and 'FEATURE_COLS' in globals(), \"Execute a Etapa 5 antes.\"\n",
    "assert 'RUN_DIR' in globals() and 'ARTIF_DIR' in globals(), \"Execute a Etapa 1 antes.\"\n",
    "assert isinstance(FEATURE_COLS, (list, tuple)) and len(FEATURE_COLS) > 0, \"FEATURE_COLS inválido.\"\n",
    "\n",
    "# CALIBRÁVEIS\n",
    "TEST_SIZE     = 0.2     # fração para validação\n",
    "SHUFFLE       = True\n",
    "RANDOM_STATE  = SEED if 'SEED' in globals() else 42\n",
    "OUTPUT_DTYPE  = np.float32  # reduzir memória e acelerar treino\n",
    "\n",
    "# FIXOS\n",
    "CSV_SEP       = \";\"\n",
    "CSV_ENCODING  = \"utf-8-sig\"\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Montagem da matriz de features\n",
    "# --------------------------------\n",
    "df_feats = DF_FEATURES.copy()\n",
    "\n",
    "# Garante ordem e existência das colunas\n",
    "missing = [c for c in FEATURE_COLS if c not in df_feats.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[§6] Colunas de feature ausentes em DF_FEATURES: {missing}\")\n",
    "\n",
    "# Reordena/seleciona\n",
    "df_feats = df_feats[FEATURE_COLS]\n",
    "\n",
    "# Força numérico onde couber (apenas segurança, já devem ser numéricas)\n",
    "for c in df_feats.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(df_feats[c]):\n",
    "        df_feats[c] = pd.to_numeric(df_feats[c], errors=\"coerce\")\n",
    "\n",
    "# Matriz numpy\n",
    "X_all = df_feats.to_numpy(dtype=np.float64, copy=True)  # usa float64 aqui para cálculos estáveis\n",
    "n_rows, n_cols = X_all.shape\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Split train/val\n",
    "# --------------------------------\n",
    "idx_all = np.arange(n_rows)\n",
    "X_train, X_val, idx_train, idx_val = train_test_split(\n",
    "    X_all, idx_all,\n",
    "    test_size=TEST_SIZE, shuffle=SHUFFLE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Imputação (treino) e aplicação\n",
    "# --------------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)  # apenas no treino\n",
    "X_train_imp = imputer.transform(X_train)\n",
    "X_val_imp   = imputer.transform(X_val)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Normalização (treino) e aplicação\n",
    "# --------------------------------\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(X_train_imp)         # apenas no treino\n",
    "X_train_std = scaler.transform(X_train_imp)\n",
    "X_val_std   = scaler.transform(X_val_imp)\n",
    "\n",
    "# Cast para dtype final (economia de RAM/tempo de treino)\n",
    "X_train_final = X_train_std.astype(OUTPUT_DTYPE, copy=False)\n",
    "X_val_final   = X_val_std.astype(OUTPUT_DTYPE, copy=False)\n",
    "\n",
    "# --------------------------------\n",
    "# 5) Persistência de artefatos/conjuntos\n",
    "# --------------------------------\n",
    "# 5.1. hashes e metadados das features (rastreabilidade)\n",
    "def _hash_list_str(items) -> str:\n",
    "    m = hashlib.sha256()\n",
    "    for it in items:\n",
    "        m.update(str(it).encode(\"utf-8\"))\n",
    "        m.update(b\"|\")\n",
    "    return m.hexdigest()\n",
    "\n",
    "features_hash = _hash_list_str(FEATURE_COLS)\n",
    "\n",
    "meta = {\n",
    "    \"n_rows\": int(n_rows),\n",
    "    \"n_cols\": int(n_cols),\n",
    "    \"test_size\": TEST_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"dtype\": str(OUTPUT_DTYPE),\n",
    "    \"features_hash\": features_hash,\n",
    "    \"n_train\": int(X_train_final.shape[0]),\n",
    "    \"n_val\": int(X_val_final.shape[0]),\n",
    "}\n",
    "\n",
    "# 5.2. salvar conjuntos\n",
    "npz_path = RUN_DIR / \"dataset_npz.npz\"\n",
    "np.savez_compressed(\n",
    "    npz_path,\n",
    "    X_train=X_train_final,\n",
    "    X_val=X_val_final,\n",
    "    idx_train=idx_train.astype(np.int64),\n",
    "    idx_val=idx_val.astype(np.int64),\n",
    ")\n",
    "\n",
    "# 5.3. salvar artefatos (imputer/scaler/feature_cols)\n",
    "feat_pkl_path = RUN_DIR / \"features.pkl\"\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"features_hash\": features_hash,\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"dtype\": np.dtype(OUTPUT_DTYPE).name,\n",
    "    },\n",
    "    feat_pkl_path\n",
    ")\n",
    "\n",
    "# cópias úteis no artifacts/\n",
    "feat_latest = ARTIF_DIR / \"features_latest.pkl\"\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "        \"features_hash\": features_hash,\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"dtype\": str(OUTPUT_DTYPE),\n",
    "    },\n",
    "    feat_latest\n",
    ")\n",
    "\n",
    "# 5.4. estatísticas descritivas (para auditoria/inspeção)\n",
    "def _describe_np(X: np.ndarray) -> dict:\n",
    "    X64 = X.astype(np.float64, copy=False)\n",
    "    return {\n",
    "        \"shape\": list(X.shape),\n",
    "        \"mean\": np.nanmean(X64, axis=0).tolist(),\n",
    "        \"std\":  np.nanstd(X64,  axis=0, ddof=0).tolist(),\n",
    "        \"min\":  np.nanmin(X64,  axis=0).tolist(),\n",
    "        \"p25\":  np.nanpercentile(X64, 25, axis=0).tolist(),\n",
    "        \"p50\":  np.nanpercentile(X64, 50, axis=0).tolist(),\n",
    "        \"p75\":  np.nanpercentile(X64, 75, axis=0).tolist(),\n",
    "        \"max\":  np.nanmax(X64,  axis=0).tolist(),\n",
    "    }\n",
    "\n",
    "desc = {\n",
    "    \"train\": _describe_np(X_train_final),\n",
    "    \"val\":   _describe_np(X_val_final),\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "}\n",
    "(RUN_DIR / \"features_desc.json\").write_text(json.dumps(desc, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# --------------------------------\n",
    "# 6) Relatos no console e variáveis em memória\n",
    "# --------------------------------\n",
    "print(f\"[§6] X_all       : {X_all.shape} (antes de imputar/normalizar)\")\n",
    "print(f\"[§6] X_train/val : {X_train_final.shape} / {X_val_final.shape}\")\n",
    "print(f\"[§6] dtype final : {X_train_final.dtype}\")\n",
    "print(f\"[§6] artefatos   : {feat_pkl_path.name}, {npz_path.name}, features_desc.json\")\n",
    "print(f\"[§6] cópia útil  : artifacts/{feat_latest.name}\")\n",
    "print(f\"[§6] features_hash = {features_hash[:16]}…\")\n",
    "\n",
    "# Mantém em memória para a Etapa 7\n",
    "X_TRAIN = X_train_final\n",
    "X_VAL   = X_val_final\n",
    "FEATURES_HASH = features_hash\n",
    "\n",
    "print(\"\\nPróximo passo: Etapa 7 — Autoencoder com early stopping usando estes conjuntos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duK-Dm3-e29D"
   },
   "source": [
    "## **Etapa 7:** Autoencoder com early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uI8f2HMjvo1W"
   },
   "outputs": [],
   "source": [
    "# @title §7 — Treinamento do Autoencoder (salva histórico, config, pesos, erros e figura)\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "import os, json, math, time, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Pré-condições\n",
    "# ---------------------------------------------------------------------\n",
    "assert 'RUN_DIR' in globals(), \"Defina RUN_DIR no §1.\"\n",
    "RUN_DIR = Path(RUN_DIR)\n",
    "ART_DIR = RUN_DIR / \"figures\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_NPZ = RUN_DIR / \"dataset_npz.npz\"\n",
    "assert DATASET_NPZ.exists(), \"dataset_npz.npz não encontrado. Execute o §6.\"\n",
    "\n",
    "# (Opcional) configuração existente para reaproveitar algo (não obrigatório)\n",
    "MODEL_CFG_PATH = RUN_DIR / \"model_config.json\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Hiperparâmetros (padrões seguros; pode ajustar livremente)\n",
    "# ---------------------------------------------------------------------\n",
    "SEED            = 42\n",
    "BATCH_SIZE      = 512\n",
    "EPOCHS          = 100\n",
    "PATIENCE        = 10                   # early stopping\n",
    "LR              = 1e-3\n",
    "WEIGHT_DECAY    = 0.0\n",
    "HIDDEN_SIZES    = [128, 64]            # encoder; decoder espelha\n",
    "LATENT_DIM      = 16                   # “gargalo”\n",
    "LOSS_FN         = \"mse\"                # ou \"mae\"\n",
    "USE_BN          = True                 # batch norm\n",
    "DROPOUT         = 0.0                  # dropout\n",
    "NUM_WORKERS     = 0\n",
    "\n",
    "# Permite sobrescrever facilmente via dicionário externo (opcional)\n",
    "if 'MODEL_CFG_OVERRIDE' in globals() and isinstance(MODEL_CFG_OVERRIDE, dict):\n",
    "    locals().update({k:v for k,v in MODEL_CFG_OVERRIDE.items() if k.isupper()})\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Reprodutibilidade\n",
    "# ---------------------------------------------------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # cudnn determinístico (pode reduzir performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[§7] Dispositivo: {device}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Carregar dados (§6 gerou dataset_npz.npz)\n",
    "#   Esperado: X_train e X_val já pré-processados/imputados/normalizados\n",
    "# ---------------------------------------------------------------------\n",
    "npz = np.load(DATASET_NPZ)\n",
    "X_train = npz[\"X_train\"].astype(np.float32)\n",
    "X_val   = npz[\"X_val\"].astype(np.float32)\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"[§7] Formas: X_train={X_train.shape}, X_val={X_val.shape}\")\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Definição do modelo (MLP simétrico)\n",
    "# ---------------------------------------------------------------------\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: list[int], latent: int,\n",
    "                 use_bn: bool = True, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        layers_enc = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers_enc += [nn.Linear(last, h)]\n",
    "            if use_bn: layers_enc += [nn.BatchNorm1d(h)]\n",
    "            layers_enc += [nn.ReLU(inplace=True)]\n",
    "            if dropout > 0: layers_enc += [nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        layers_enc += [nn.Linear(last, latent)]\n",
    "        self.encoder = nn.Sequential(*layers_enc)\n",
    "\n",
    "        # decoder espelha\n",
    "        layers_dec = []\n",
    "        last = latent\n",
    "        for h in reversed(hidden):\n",
    "            layers_dec += [nn.Linear(last, h)]\n",
    "            if use_bn: layers_dec += [nn.BatchNorm1d(h)]\n",
    "            layers_dec += [nn.ReLU(inplace=True)]\n",
    "            if dropout > 0: layers_dec += [nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        layers_dec += [nn.Linear(last, in_dim)]\n",
    "        self.decoder = nn.Sequential(*layers_dec)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        xr = self.decoder(z)\n",
    "        return xr\n",
    "\n",
    "model = AE(input_dim, HIDDEN_SIZES, LATENT_DIM, USE_BN, DROPOUT).to(device)\n",
    "\n",
    "# Perda e otimizador\n",
    "criterion = nn.MSELoss(reduction=\"mean\") if LOSS_FN.lower() == \"mse\" else nn.L1Loss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Loop de treinamento com Early Stopping\n",
    "# ---------------------------------------------------------------------\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "best_val = float(\"inf\")\n",
    "best_epoch = -1\n",
    "pat_left = PATIENCE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # treino\n",
    "    model.train()\n",
    "    tr_loss_sum, tr_count = 0.0, 0\n",
    "    for (xb,) in train_dl:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        xr = model(xb)\n",
    "        loss = criterion(xr, xb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bs = xb.size(0)\n",
    "        tr_loss_sum += loss.item() * bs\n",
    "        tr_count += bs\n",
    "    train_loss = tr_loss_sum / max(tr_count, 1)\n",
    "\n",
    "    # validação\n",
    "    model.eval()\n",
    "    va_loss_sum, va_count = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for (xb,) in val_dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            xr = model(xb)\n",
    "            loss = criterion(xr, xb)\n",
    "            bs = xb.size(0)\n",
    "            va_loss_sum += loss.item() * bs\n",
    "            va_count += bs\n",
    "    val_loss = va_loss_sum / max(va_count, 1)\n",
    "\n",
    "    # log\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    print(f\"[§7] Época {epoch:03d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss + 1e-12 < best_val:\n",
    "        best_val = val_loss\n",
    "        best_epoch = epoch\n",
    "        pat_left = PATIENCE\n",
    "        # checkpoint de pesos\n",
    "        torch.save(model.state_dict(), RUN_DIR / \"ae.pt\")\n",
    "    else:\n",
    "        pat_left -= 1\n",
    "        if pat_left <= 0:\n",
    "            print(f\"[§7] Early stopping em epoch {epoch} (best_epoch={best_epoch}, best_val={best_val:.6f})\")\n",
    "            break\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Salvar histórico\n",
    "# ---------------------------------------------------------------------\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_csv = RUN_DIR / \"training_history.csv\"\n",
    "hist_df.to_csv(hist_csv, index=False)\n",
    "print(f\"[§7] Salvo: {hist_csv}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Salvar figura da curva de treino/validação\n",
    "# ---------------------------------------------------------------------\n",
    "plt.figure(figsize=(8,4.6))\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"Treino\")\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"],   label=\"Validação\")\n",
    "plt.title(\"Curva de treinamento do Autoencoder\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Erro médio (loss)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "curve_png = ART_DIR / \"training_curve.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(curve_png, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[§7] Salvo: {curve_png}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Recarregar o melhor checkpoint e salvar erros de reconstrução (validação)\n",
    "# ---------------------------------------------------------------------\n",
    "# Garante que os pesos salvos são os do melhor val_loss\n",
    "if (RUN_DIR / \"ae.pt\").exists():\n",
    "    model.load_state_dict(torch.load(RUN_DIR / \"ae.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "# Erros por linha em X_val (MSE por amostra)\n",
    "val_tensor = torch.from_numpy(X_val).to(device)\n",
    "with torch.no_grad():\n",
    "    xr_val = model(val_tensor).cpu().numpy()\n",
    "val_err = ((X_val - xr_val)**2).mean(axis=1).astype(np.float32)\n",
    "recon_err_val_path = RUN_DIR / \"reconstruction_errors_val.npy\"\n",
    "np.save(recon_err_val_path, val_err)\n",
    "print(f\"[§7] Salvo: {recon_err_val_path} (shape={val_err.shape})\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Salvar configuração do modelo/treino\n",
    "# ---------------------------------------------------------------------\n",
    "model_config = {\n",
    "    \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"device\": str(device),\n",
    "    \"input_dim\": int(input_dim),\n",
    "    \"hidden_sizes\": list(map(int, HIDDEN_SIZES)),\n",
    "    \"latent_dim\": int(LATENT_DIM),\n",
    "    \"use_batchnorm\": bool(USE_BN),\n",
    "    \"dropout\": float(DROPOUT),\n",
    "    \"loss_fn\": LOSS_FN.lower(),\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": float(LR),\n",
    "    \"weight_decay\": float(WEIGHT_DECAY),\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "    \"epochs_requested\": int(EPOCHS),\n",
    "    \"early_stopping_patience\": int(PATIENCE),\n",
    "    \"best_epoch\": int(best_epoch),\n",
    "    \"best_val_loss\": float(best_val) if math.isfinite(best_val) else None,\n",
    "}\n",
    "MODEL_CFG_PATH.write_text(json.dumps(model_config, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[§7] Salvo: {MODEL_CFG_PATH}\")\n",
    "\n",
    "print(\"[§7] TREINAMENTO CONCLUÍDO.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg72YRNCg6nt"
   },
   "source": [
    "## **Etapa 8:** Pontuação (geração de scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "y1kz_w1y4Tog"
   },
   "outputs": [],
   "source": [
    "# @title §8 — Pontuação (seleção de CSV em prerun/ e geração de scores)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "1) Carregar artefatos de treino (features.pkl, categorical_maps.json, ae.pt, model_config.json).\n",
    "2) Selecionar CSV pré-processado em PRERUN_DIR para pontuar.\n",
    "3) Reaplicar engenharia de features (Etapa 5) com o mesmo vocabulário.\n",
    "4) Imputar + escalar (artefatos da Etapa 6).\n",
    "5) Inferir com o Autoencoder e calcular erro de reconstrução por linha.\n",
    "6) Salvar scores em runs/<RUN_ID>/scores.csv (inclui 'score' e 'recon_error'), e sumário.\n",
    "\n",
    "Entradas necessárias\n",
    "--------------------\n",
    "- RUN_DIR (Etapa 1), PRERUN_DIR (Etapa 1)\n",
    "- encode_categoricals (Etapa 4)\n",
    "- Mesmas derivadas da Etapa 5 (log1p, sinal, len, data, stats por user)\n",
    "\n",
    "Saídas\n",
    "------\n",
    "- runs/<RUN_ID>/scores.csv                (colunas de contexto + score=recon_error)\n",
    "- runs/<RUN_ID>/score_stats.json          (sumário)\n",
    "- runs/<RUN_ID>/reconstruction_errors_score.npy  (vetor com score)\n",
    "- runs/<RUN_ID>/selected_source.csv       (snapshot do CSV pontuado)\n",
    "\"\"\"\n",
    "\n",
    "import json, re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- pré-checagens ----------\n",
    "assert 'RUN_DIR' in globals() and 'PRERUN_DIR' in globals(), \"Execute Etapas 1–7 antes.\"\n",
    "assert 'encode_categoricals' in globals(), \"Função 'encode_categoricals' ausente (rode Etapa 4).\"\n",
    "\n",
    "RUN_DIR   = Path(RUN_DIR)\n",
    "PRERUN_DIR = Path(PRERUN_DIR)\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Artefatos obrigatórios (Etapas 6–7)\n",
    "feat_path     = RUN_DIR / \"features.pkl\"\n",
    "maps_run_path = RUN_DIR / \"categorical_maps.json\"\n",
    "model_path    = RUN_DIR / \"ae.pt\"\n",
    "model_cfg     = RUN_DIR / \"model_config.json\"\n",
    "\n",
    "missing = [p.name for p in [feat_path, model_path, model_cfg] if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Artefatos ausentes: {missing}. Garanta que as Etapas 6 e 7 foram executadas.\")\n",
    "\n",
    "# (opcional) mapas em ARTIF_DIR, apenas se existir\n",
    "maps_art_path = None\n",
    "if 'ARTIF_DIR' in globals():\n",
    "    maps_art_path = Path(ARTIF_DIR) / \"categorical_maps_latest.json\"\n",
    "\n",
    "# ---------- carregar artefatos ----------\n",
    "import joblib\n",
    "feat_bundle = joblib.load(feat_path)  # dict: feature_cols, features_hash, imputer, scaler, dtype\n",
    "FEATURE_COLS_SAVED = feat_bundle[\"feature_cols\"]\n",
    "IMPUTER = feat_bundle[\"imputer\"]\n",
    "SCALER = feat_bundle[\"scaler\"]\n",
    "\n",
    "def _parse_dtype(raw):\n",
    "    import numpy as np\n",
    "    if isinstance(raw, np.dtype): return raw\n",
    "    if isinstance(raw, type):\n",
    "        try: return np.dtype(raw)\n",
    "        except Exception: pass\n",
    "    if isinstance(raw, str):\n",
    "        try: return np.dtype(raw)\n",
    "        except Exception:\n",
    "            low = raw.lower()\n",
    "            for key in (\"float32\",\"float64\",\"float16\",\"int64\",\"int32\",\"int16\",\"int8\",\"uint8\",\"bool\"):\n",
    "                if key in low: return np.dtype(key)\n",
    "    return np.dtype(\"float32\")\n",
    "\n",
    "DTYPE_OUT = _parse_dtype(feat_bundle.get(\"dtype\", \"float32\"))\n",
    "\n",
    "# Vocabulário categórico\n",
    "if maps_run_path.exists():\n",
    "    categorical_maps_scoring = json.loads(maps_run_path.read_text(encoding=\"utf-8\"))\n",
    "elif maps_art_path is not None and maps_art_path.exists():\n",
    "    categorical_maps_scoring = json.loads(maps_art_path.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    raise FileNotFoundError(\"Mapas categóricos não encontrados (rode Etapa 4).\")\n",
    "\n",
    "# ---------- Modelo (alinha com §7: Linear -> [BN] -> ReLU -> [Dropout]) ----------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cfg = json.loads(model_cfg.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def _cfg_get(keys, default=None):\n",
    "    for k in keys:\n",
    "        if k in cfg: return cfg[k]\n",
    "    return default\n",
    "\n",
    "# Dimensão deve vir do treino; se houver divergência, preferimos as features salvas no §6\n",
    "INPUT_DIM_CFG  = int(_cfg_get([\"input_dim\"], len(FEATURE_COLS_SAVED)))\n",
    "INPUT_DIM_DATA = int(len(FEATURE_COLS_SAVED))\n",
    "if INPUT_DIM_CFG != INPUT_DIM_DATA:\n",
    "    print(f\"[§8] Aviso: input_dim do modelo ({INPUT_DIM_CFG}) difere do #features carregadas ({INPUT_DIM_DATA}). Usando {INPUT_DIM_DATA}.\")\n",
    "INPUT_DIM = INPUT_DIM_DATA\n",
    "\n",
    "HIDDEN_LIST = list(_cfg_get([\"hidden_dims\",\"hidden_sizes\"], []))\n",
    "BOTTLENECK  = int(_cfg_get([\"bottleneck\",\"latent_dim\"], 16))\n",
    "DROPOUT_P   = float(_cfg_get([\"dropout_p\",\"dropout\"], 0.0))\n",
    "USE_BN      = bool(_cfg_get([\"use_batchnorm\",\"use_bn\"], False))\n",
    "\n",
    "class AE_Aligned(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: list[int], latent: int,\n",
    "                 use_bn: bool = True, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        enc = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            enc += [nn.Linear(last, h)]\n",
    "            if use_bn: enc += [nn.BatchNorm1d(h)]\n",
    "            enc += [nn.ReLU(inplace=True)]\n",
    "            if dropout > 0: enc += [nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        enc += [nn.Linear(last, latent)]\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "\n",
    "        dec = []\n",
    "        last = latent\n",
    "        for h in reversed(hidden):\n",
    "            dec += [nn.Linear(last, h)]\n",
    "            if use_bn: dec += [nn.BatchNorm1d(h)]\n",
    "            dec += [nn.ReLU(inplace=True)]\n",
    "            if dropout > 0: dec += [nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        dec += [nn.Linear(last, in_dim)]\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        xr = self.decoder(z)\n",
    "        return xr, z\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AE_Aligned(INPUT_DIM, HIDDEN_LIST, BOTTLENECK, USE_BN, DROPOUT_P).to(DEVICE)\n",
    "\n",
    "# Tentativa de carregar com strict=True (esperado); se falhar, tenta strict=False e avisa.\n",
    "state = torch.load(model_path, map_location=DEVICE)\n",
    "try:\n",
    "    missing, unexpected = model.load_state_dict(state, strict=True)\n",
    "    # para PyTorch >= 2.0, load_state_dict não retorna tupla; então não faremos nada aqui\n",
    "except RuntimeError as e:\n",
    "    print(\"[§8] Aviso: strict=True falhou ao carregar pesos. Tentando strict=False…\")\n",
    "    model.load_state_dict(state, strict=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ---------- helpers: mesmas derivadas da Etapa 5 ----------\n",
    "CAT_SUFFIX = \"_int\"\n",
    "\n",
    "def _engineer_features_for_scoring(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replica a engenharia da Etapa 5 para garantir o mesmo contrato de features.\"\"\"\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # codificação categórica com o vocabulário CONGELADO\n",
    "    df = encode_categoricals(\n",
    "        df,\n",
    "        cat_cols=list(categorical_maps_scoring.keys()),\n",
    "        maps=categorical_maps_scoring,\n",
    "        suffix=CAT_SUFFIX\n",
    "    )\n",
    "\n",
    "    # derivadas numéricas (idem Etapa 5)\n",
    "    df[\"feat_log_valormi\"] = np.log1p(df[\"valormi\"].abs())\n",
    "\n",
    "    if \"dc\" in df.columns:\n",
    "        df[\"feat_sign_dc\"] = df[\"dc\"].map({\"d\": 1, \"c\": -1}).fillna(0).astype(int)\n",
    "\n",
    "    if \"contacontabil\" in df.columns:\n",
    "        df[\"feat_len_conta\"] = df[\"contacontabil\"].astype(str).str.len().astype(int)\n",
    "\n",
    "    if \"data_lcto\" in df.columns:\n",
    "        _dates = pd.to_datetime(df[\"data_lcto\"], errors=\"coerce\")\n",
    "        df[\"feat_weekday\"] = _dates.dt.weekday.fillna(-1).astype(int)\n",
    "        df[\"feat_month\"]   = _dates.dt.month.fillna(0).astype(int)\n",
    "\n",
    "    if \"username\" in df.columns:\n",
    "        # estatísticas por usuário (usando o lote corrente; mesmo comportamento da Etapa 5)\n",
    "        user_group = df.groupby(\"username\")[\"valormi\"]\n",
    "        df[\"feat_user_mean\"] = df[\"username\"].map(user_group.mean())\n",
    "        df[\"feat_user_std\"]  = df[\"username\"].map(user_group.std().fillna(0))\n",
    "        df[\"feat_user_cnt\"]  = df[\"username\"].map(user_group.count())\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- seleção de arquivo em prerun/ (explícita, com filtro e confirmação) ----------\n",
    "def _human_size(b: int) -> str:\n",
    "    for unit in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if b < 1024: return f\"{b:.1f}{unit}\"\n",
    "        b /= 1024\n",
    "    return f\"{b:.1f}PB\"\n",
    "\n",
    "pr_list_all = sorted(PRERUN_DIR.glob(\"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not pr_list_all:\n",
    "    raise FileNotFoundError(\"Nenhum CSV encontrado em prerun/. Rode a Etapa 2.\")\n",
    "\n",
    "def _mostrar(lista):\n",
    "    print(\"\\n[§8] CSVs em prerun/:\")\n",
    "    for i, p in enumerate(lista):\n",
    "        ts = datetime.fromtimestamp(p.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{i:03d}] {p.name:60s}  { _human_size(p.stat().st_size):>8s}  mtime={ts}\")\n",
    "\n",
    "while True:\n",
    "    _mostrar(pr_list_all)\n",
    "    termo = input(\"\\n(Filtro opcional) termo no nome do arquivo, ou Enter para todos: \").strip()\n",
    "    if termo:\n",
    "        pr_list = [p for p in pr_list_all if termo.lower() in p.name.lower()]\n",
    "        if not pr_list:\n",
    "            print(\"Nenhum arquivo corresponde ao filtro. Tente novamente.\")\n",
    "            continue\n",
    "    else:\n",
    "        pr_list = pr_list_all\n",
    "\n",
    "    _mostrar(pr_list)\n",
    "    raw = input(\"\\nDigite o ÍNDICE do CSV para PONTUAÇÃO (ou 'x' para refiltrar): \").strip().lower()\n",
    "    if raw == \"x\":\n",
    "        continue\n",
    "    try:\n",
    "        idx = int(raw)\n",
    "        assert 0 <= idx < len(pr_list)\n",
    "    except Exception:\n",
    "        print(\"Índice inválido. Tente novamente.\")\n",
    "        continue\n",
    "\n",
    "    selecionado = pr_list[idx]\n",
    "    print(f\"\\nSelecionado: {selecionado.name}\")\n",
    "    ok = input(\"Confirma este arquivo para pontuação? (s/n): \").strip().lower()\n",
    "    if ok == \"s\":\n",
    "        SCORE_CSV = selecionado\n",
    "        break\n",
    "    else:\n",
    "        print(\"Seleção cancelada. Reiniciando…\")\n",
    "\n",
    "print(f\"[§8] Arquivo confirmado: {SCORE_CSV.name}\")\n",
    "\n",
    "# ---------- carregamento e normalizações mínimas ----------\n",
    "CSV_SEP, CSV_ENCODING = \";\", \"utf-8-sig\"\n",
    "\n",
    "def _to_float(v):\n",
    "    if v is None: return np.nan\n",
    "    s = str(v).strip()\n",
    "    if s == \"\": return np.nan\n",
    "    if \",\" in s: s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    s = s.replace(\" \", \"\")\n",
    "    try: return float(s)\n",
    "    except Exception: return np.nan\n",
    "\n",
    "df_raw = pd.read_csv(SCORE_CSV, sep=CSV_SEP, encoding=CSV_ENCODING, dtype=str)\n",
    "df_raw.columns = [c.strip().lower() for c in df_raw.columns]\n",
    "df_raw = df_raw.apply(lambda col: col.str.strip() if col.dtype == object else col)\n",
    "\n",
    "# Contrato mínimo (mesmo da ingestão)\n",
    "req_cols = [\"username\",\"lotacao\",\"data_lcto\",\"valormi\",\"dc\",\"contacontabil\"]\n",
    "falt = [c for c in req_cols if c not in df_raw.columns]\n",
    "if falt:\n",
    "    raise ValueError(f\"Colunas obrigatórias ausentes em {SCORE_CSV.name}: {falt}\")\n",
    "\n",
    "df_raw[\"valormi\"] = df_raw[\"valormi\"].map(_to_float).astype(float)\n",
    "df_raw[\"dc\"] = df_raw[\"dc\"].astype(str).str.lower().str.strip()\n",
    "df_raw[\"contacontabil\"] = df_raw[\"contacontabil\"].astype(str).str.replace(r\"\\D+\",\"\", regex=True)\n",
    "\n",
    "# ---------- engenharia de features (idêntica à Etapa 5) ----------\n",
    "df_feats = _engineer_features_for_scoring(df_raw)\n",
    "\n",
    "# Garante ordem/contrato das features exatamente como salvas no treino\n",
    "missing_feats = [c for c in FEATURE_COLS_SAVED if c not in df_feats.columns]\n",
    "if missing_feats:\n",
    "    raise ValueError(f\"As seguintes features esperadas não existem no arquivo: {missing_feats}\")\n",
    "\n",
    "X_score = df_feats[FEATURE_COLS_SAVED].copy()\n",
    "\n",
    "# Coerção numérica\n",
    "for c in X_score.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(X_score[c]):\n",
    "        X_score[c] = pd.to_numeric(X_score[c], errors=\"coerce\")\n",
    "\n",
    "X_score_np = X_score.to_numpy(dtype=np.float64, copy=True)\n",
    "\n",
    "# ---------- imputar + escalar com artefatos do treino ----------\n",
    "X_imp = IMPUTER.transform(X_score_np)\n",
    "X_std = SCALER.transform(X_imp)\n",
    "X_std = X_std.astype(DTYPE_OUT, copy=False)\n",
    "\n",
    "# ---------- inferência ----------\n",
    "with torch.no_grad():\n",
    "    xb = torch.tensor(X_std, dtype=torch.float32, device=DEVICE)\n",
    "    batch = 16384\n",
    "    errs = []\n",
    "    for i in range(0, xb.shape[0], batch):\n",
    "        x_chunk = xb[i:i+batch]\n",
    "        xh, _ = model(x_chunk)\n",
    "        err = torch.mean((xh - x_chunk) ** 2, dim=1).detach().cpu().numpy()\n",
    "        errs.append(err)\n",
    "    recon_error = np.concatenate(errs, axis=0)\n",
    "\n",
    "# ---------- salvar scores e artefatos leves ----------\n",
    "scores_df = df_raw.copy()\n",
    "scores_df.insert(0, \"score\", recon_error.astype(np.float64))\n",
    "scores_df.insert(1, \"recon_error\", recon_error.astype(np.float64))  # duplicado para compatibilidade\n",
    "\n",
    "scores_path = RUN_DIR / \"scores.csv\"\n",
    "scores_df.to_csv(scores_path, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "print(f\"[§8] Salvo: {scores_path.name}\")\n",
    "\n",
    "# vetor com erros (para gráficos/relatórios)\n",
    "np.save(RUN_DIR / \"reconstruction_errors_score.npy\", recon_error.astype(np.float32))\n",
    "print(\"[§8] Salvo: reconstruction_errors_score.npy\")\n",
    "\n",
    "# snapshot do CSV pontuado (para §12 exibir caminho/arquivo)\n",
    "try:\n",
    "    (RUN_DIR / \"selected_source.csv\").write_bytes(SCORE_CSV.read_bytes())\n",
    "    print(\"[§8] Salvo: selected_source.csv (snapshot do arquivo pontuado)\")\n",
    "except Exception as e:\n",
    "    print(f\"[§8] Aviso: não foi possível salvar selected_source.csv: {e}\")\n",
    "\n",
    "# Sumário dos scores\n",
    "score_stats = {\n",
    "    \"count\": int(recon_error.shape[0]),\n",
    "    \"mean\": float(np.mean(recon_error)),\n",
    "    \"std\": float(np.std(recon_error)),\n",
    "    \"min\": float(np.min(recon_error)),\n",
    "    \"p50\": float(np.percentile(recon_error, 50)),\n",
    "    \"p95\": float(np.percentile(recon_error, 95)),\n",
    "    \"p99\": float(np.percentile(recon_error, 99)),\n",
    "    \"max\": float(np.max(recon_error)),\n",
    "}\n",
    "(RUN_DIR / \"score_stats.json\").write_text(json.dumps(score_stats, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"[§8] Salvo: score_stats.json\")\n",
    "\n",
    "print(\"\\n[§8] Pontuação concluída.\")\n",
    "print(\"Próximo passo: §9 — Calibração (threshold por budget/meta/custo) e metas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUa_QRtbiuNg"
   },
   "source": [
    "## **Etapa 9:** Calibração de threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "H339UXDG408z"
   },
   "outputs": [],
   "source": [
    "# @title §9 — Calibração de Threshold e Metas (budget | meta | costmin)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "Definir o threshold do score (erro de reconstrução) que será usado para marcar alertas:\n",
    "- Modo 'budget' : taxa de alerta alvo (quantil sobre a distribuição de validação)\n",
    "- Modo 'meta'   : número absoluto de alertas no LOTE ATUAL (usa distribuição atual)\n",
    "- Modo 'costmin': minimiza custo proxy com (c_fp, c_fn, prevalência esperada p)\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- RUN_DIR/reconstruction_errors_val.npy  (Etapa 7)\n",
    "- RUN_DIR/scores.csv                     (Etapa 8)  -> coluna 'score'\n",
    "\n",
    "Saídas\n",
    "------\n",
    "- RUN_DIR/threshold.json                 (modo, parâmetros, threshold, KS/PSI, taxas)\n",
    "- RUN_DIR/figures/drift_hist.png         (histograma val vs atual)   [NOVO]\n",
    "- RUN_DIR/scores_summary.json            (n_alerts, alert_rate, threshold, modo)   [NOVO]\n",
    "- Impressão de resumo para conferência\n",
    "\n",
    "Notas\n",
    "-----\n",
    "- KS e PSI são calculados entre a distribuição de VALIDAÇÃO (baseline) e o LOTE ATUAL.\n",
    "- Para 'budget', o threshold é o quantil de validação: q = 1 - ALERT_RATE.\n",
    "- Para 'meta', o threshold é o quantil do lote atual tal que N_alertas = alvo.\n",
    "- Para 'costmin' (proxy, sem rótulos): busca thresholds por quantis do lote atual\n",
    "  minimizando: C = c_fp * rate_alerts + c_fn * max(0, p - rate_alerts).\n",
    "\"\"\"\n",
    "\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert 'RUN_DIR' in globals(), \"Execute a Etapa 1 para definir RUN_DIR.\"\n",
    "val_err_path = Path(RUN_DIR) / \"reconstruction_errors_val.npy\"\n",
    "scores_path  = Path(RUN_DIR) / \"scores.csv\"\n",
    "assert val_err_path.exists(), \"Arquivo de validação ausente (reconstruction_errors_val.npy). Rode as Etapas 6–7.\"\n",
    "assert scores_path.exists(),  \"scores.csv ausente. Rode a Etapa 8.\"\n",
    "\n",
    "# ---------- carregar dados ----------\n",
    "val_err = np.load(val_err_path)          # distribuição baseline (validação)\n",
    "df_sc   = pd.read_csv(scores_path, sep=\";\", encoding=\"utf-8-sig\")\n",
    "assert \"score\" in df_sc.columns, \"scores.csv não possui coluna 'score'.\"\n",
    "scores  = df_sc[\"score\"].to_numpy(dtype=float)\n",
    "\n",
    "# ---------- métricas de drift (KS e PSI) ----------\n",
    "def ks_stat(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a[~np.isnan(a)]\n",
    "    b = b[~np.isnan(b)]\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return float(\"nan\")\n",
    "    xa = np.sort(a)\n",
    "    xb = np.sort(b)\n",
    "    grid = np.unique(np.concatenate([xa, xb], axis=0))\n",
    "    def _ecdf(x, g):\n",
    "        return np.searchsorted(x, g, side=\"right\") / x.size\n",
    "    Fa = np.array([_ecdf(xa, g) for g in grid], dtype=float)\n",
    "    Fb = np.array([_ecdf(xb, g) for g in grid], dtype=float)\n",
    "    return float(np.max(np.abs(Fa - Fb)))\n",
    "\n",
    "def psi_stat(expected: np.ndarray, actual: np.ndarray, bins: int = 20) -> float:\n",
    "    e = expected[~np.isnan(expected)]\n",
    "    a = actual[~np.isnan(actual)]\n",
    "    if e.size == 0 or a.size == 0:\n",
    "        return float(\"nan\")\n",
    "    qs = np.linspace(0, 1, bins + 1)\n",
    "    cuts = np.quantile(e, qs)\n",
    "    cuts = np.unique(cuts)\n",
    "    if cuts.size < 3:\n",
    "        return 0.0\n",
    "    e_hist, _ = np.histogram(e, bins=cuts)\n",
    "    a_hist, _ = np.histogram(a, bins=cuts)\n",
    "    e_prop = np.clip(e_hist / max(e_hist.sum(), 1), 1e-8, 1.0)\n",
    "    a_prop = np.clip(a_hist / max(a_hist.sum(), 1), 1e-8, 1.0)\n",
    "    psi = np.sum((a_prop - e_prop) * np.log(a_prop / e_prop))\n",
    "    return float(psi)\n",
    "\n",
    "KS  = ks_stat(val_err, scores)\n",
    "PSI = psi_stat(val_err, scores, bins=20)\n",
    "\n",
    "print(f\"[§9] KS(val vs atual) = {KS:.4f} | PSI = {PSI:.4f}\")\n",
    "\n",
    "# ---------- modos de calibração ----------\n",
    "def calib_budget(val_err: np.ndarray, alert_rate: float) -> dict:\n",
    "    alert_rate = float(alert_rate)\n",
    "    alert_rate = min(max(alert_rate, 1e-6), 0.99)\n",
    "    thr = float(np.quantile(val_err, 1.0 - alert_rate))\n",
    "    rate_val = float((val_err >= thr).mean())\n",
    "    return {\"threshold\": thr, \"rate_val_expected\": rate_val, \"params\": {\"mode\":\"budget\",\"alert_rate\": alert_rate}}\n",
    "\n",
    "def calib_meta_current(scores: np.ndarray, n_alerts: int) -> dict:\n",
    "    n_alerts = int(max(0, n_alerts))\n",
    "    n = scores.size\n",
    "    if n_alerts <= 0:\n",
    "        thr = float(np.inf)\n",
    "    elif n_alerts >= n:\n",
    "        thr = float(-np.inf)\n",
    "    else:\n",
    "        s = np.sort(scores)[::-1]\n",
    "        thr = float(s[n_alerts - 1])\n",
    "    rate_curr = float((scores >= thr).mean())\n",
    "    return {\"threshold\": thr, \"rate_current\": rate_curr, \"params\": {\"mode\":\"meta\",\"n_alerts\": n_alerts, \"n_rows\": n}}\n",
    "\n",
    "def calib_costmin_proxy(scores: np.ndarray, c_fp: float, c_fn: float, prevalence: float) -> dict:\n",
    "    c_fp = float(max(c_fp, 0.0))\n",
    "    c_fn = float(max(c_fn, 0.0))\n",
    "    p = float(min(max(prevalence, 0.0), 1.0))\n",
    "    if scores.size == 0:\n",
    "        return {\"threshold\": float(\"nan\"), \"rate_current\": float(\"nan\"),\n",
    "                \"params\": {\"mode\":\"costmin\",\"c_fp\": c_fp, \"c_fn\": c_fn, \"prevalence\": p}}\n",
    "    qs = np.linspace(0.0, 1.0, 1001)\n",
    "    thrs = np.quantile(scores, 1.0 - qs)\n",
    "    s_sorted = np.sort(scores)\n",
    "    n = s_sorted.size\n",
    "    best = None; best_thr = None; best_rate = None\n",
    "    for thr in thrs:\n",
    "        idx = np.searchsorted(s_sorted, thr, side=\"left\")\n",
    "        rate = (n - idx) / n\n",
    "        cost = c_fp * rate + c_fn * max(0.0, p - rate)\n",
    "        if (best is None) or (cost < best):\n",
    "            best, best_thr, best_rate = cost, float(thr), float(rate)\n",
    "    return {\"threshold\": best_thr, \"rate_current\": best_rate,\n",
    "            \"params\": {\"mode\":\"costmin\",\"c_fp\": c_fp, \"c_fn\": c_fn, \"prevalence\": p, \"grid\": \"q=0..1 step 0.001\"}}\n",
    "\n",
    "# ---------- interação com o usuário ----------\n",
    "print(\"\\nSelecione o MODO de threshold:\")\n",
    "print(\"  [1] budget  — Threshold por taxa de alerta alvo (ALERT_RATE, ex.: 0.03)\")\n",
    "print(\"  [2] meta    — Threshold por número de alertas desejado no lote atual (N_ALERTS)\")\n",
    "print(\"  [3] costmin — Threshold por custo proxy mínimo (c_fp, c_fn, prevalência p)\")\n",
    "mode_raw = input(\"Digite o índice do modo [1-3]: \").strip()\n",
    "\n",
    "if mode_raw == \"1\":\n",
    "    a_raw = input(\"ALERT_RATE (fração, ex.: 0.03) [default=0.03]: \").strip()\n",
    "    ALERT_RATE = float(a_raw) if a_raw else 0.03\n",
    "    result = calib_budget(val_err, ALERT_RATE)\n",
    "    MODE = \"budget\"\n",
    "\n",
    "elif mode_raw == \"2\":\n",
    "    default_n = max(1, int(0.02 * scores.size))\n",
    "    n_raw = input(f\"N_ALERTS (inteiro, 0..{scores.size}) [default={default_n}]: \").strip()\n",
    "    N_ALERTS = int(n_raw) if n_raw else default_n\n",
    "    result = calib_meta_current(scores, N_ALERTS)\n",
    "    MODE = \"meta\"\n",
    "\n",
    "elif mode_raw == \"3\":\n",
    "    cfp_raw = input(\"c_fp (custo do falso positivo) [default=1.0]: \").strip()\n",
    "    cfn_raw = input(\"c_fn (custo do falso negativo) [default=5.0]: \").strip()\n",
    "    p_raw   = input(\"prevalência esperada p (0..1) [default=0.01]: \").strip()\n",
    "    c_fp = float(cfp_raw) if cfp_raw else 1.0\n",
    "    c_fn = float(cfn_raw) if cfn_raw else 5.0\n",
    "    p    = float(p_raw)   if p_raw   else 0.01\n",
    "    result = calib_costmin_proxy(scores, c_fp, c_fn, p)\n",
    "    MODE = \"costmin\"\n",
    "\n",
    "else:\n",
    "    print(\"Entrada inválida. Usando modo [1] budget com ALERT_RATE=0.03 por padrão.\")\n",
    "    ALERT_RATE = 0.03\n",
    "    result = calib_budget(val_err, ALERT_RATE)\n",
    "    MODE = \"budget\"\n",
    "\n",
    "THRESHOLD = float(result[\"threshold\"])\n",
    "\n",
    "# taxas estimadas (validação e lote atual)\n",
    "rate_val     = float((val_err >= THRESHOLD).mean())\n",
    "rate_current = float((scores  >= THRESHOLD).mean())\n",
    "\n",
    "# ---------- salvar threshold.json (mantendo compatibilidade) ----------\n",
    "summary = {\n",
    "    \"mode\": MODE,\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"ks_val_vs_current\": KS,\n",
    "    \"psi_val_vs_current\": PSI,\n",
    "    \"rate_val_expected\": rate_val,\n",
    "    \"rate_current_estimated\": rate_current,\n",
    "    \"mode_config\": result.get(\"params\", {}),   # <— ajuda o §12 a descrever o método\n",
    "    \"drift\": {\"ks_stat\": KS, \"psi\": PSI},      # <— bloco amigável para leitura posterior\n",
    "    \"inputs\": {\n",
    "        \"val_err_path\": str(val_err_path),\n",
    "        \"scores_path\": str(scores_path),\n",
    "        \"n_val\": int(val_err.shape[0]),\n",
    "        \"n_current\": int(scores.shape[0]),\n",
    "    },\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "out_path = Path(RUN_DIR) / \"threshold.json\"\n",
    "out_path.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[§9] threshold.json salvo em: {out_path.name}\")\n",
    "\n",
    "# ---------- salvar figura de comparação das distribuições (drift_hist.png) ----------\n",
    "fig_dir = Path(RUN_DIR) / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.figure(figsize=(8,4.6))\n",
    "plt.hist(val_err, bins=50, alpha=0.5, label=\"Validação\", density=True)\n",
    "plt.hist(scores,  bins=50, alpha=0.5, label=\"Lote atual\", density=True)\n",
    "plt.axvline(THRESHOLD, color=\"k\", linestyle=\"--\", linewidth=1.2, label=f\"Threshold = {THRESHOLD:.5f}\")\n",
    "plt.title(\"Comparação de distribuições de erro (validação vs lote atual)\")\n",
    "plt.xlabel(\"Erro de reconstrução\")\n",
    "plt.ylabel(\"Densidade\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "drift_png = fig_dir / \"drift_hist.png\"\n",
    "plt.savefig(drift_png, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[§9] Figura salva: {drift_png.name}\")\n",
    "\n",
    "# ---------- salvar scores_summary.json (não altera scores.csv; §10 materializa 'alert') ----------\n",
    "n_alerts = int((scores >= THRESHOLD).sum())\n",
    "scores_summary = {\n",
    "    \"n_linhas\": int(scores.shape[0]),\n",
    "    \"n_alerts\": n_alerts,\n",
    "    \"alert_rate\": float(n_alerts / max(scores.shape[0], 1)),\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"mode\": MODE,\n",
    "    \"mode_config\": result.get(\"params\", {}),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "(Path(RUN_DIR) / \"scores_summary.json\").write_text(json.dumps(scores_summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"[§9] scores_summary.json salvo.\")\n",
    "\n",
    "print(\"\\n[§9] Calibração concluída.\")\n",
    "print(f\"[§9] threshold = {THRESHOLD:.6f}\")\n",
    "print(f\"[§9] taxas: val={rate_val:.4%}  |  atual={rate_current:.4%}\")\n",
    "print(f\"[§9] KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "print(\"Próximo passo: §10 — materializar alerts no scores.csv usando este threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc3Pu1kRjWyt"
   },
   "source": [
    "## **Etapa 10:** Marcação dos alertas de anomalia no arquivo de output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1zdhONwc5La7"
   },
   "outputs": [],
   "source": [
    "# @title §10 — Aplicar threshold e gerar scores com ALERT (auditoria pronta)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "1) Ler RUN_DIR/scores.csv (Etapa 8) e RUN_DIR/threshold.json (Etapa 9).\n",
    "2) Aplicar threshold → coluna 'alert' (0/1).\n",
    "3) Gerar ranking estável por 'score' (desc) e metadados de auditoria.\n",
    "4) Salvar:\n",
    "   - runs/<RUN_ID>/scores_alerts.csv         (dataset completo com alert=0/1)\n",
    "   - runs/<RUN_ID>/scores_alerts_top1000.csv (amostra priorizada)\n",
    "   - runs/<RUN_ID>/alerts_summary.json       (sumário de métricas)\n",
    "   - runs/<RUN_ID>/alerts_by_username.csv    (agregação útil para triagem, se houver 'username')\n",
    "   - runs/<RUN_ID>/alerts_top100.csv         (TOP 100 alertas, p/ relatório §12)   [NOVO]\n",
    "   - runs/<RUN_ID>/scores_summary.json       (n_alerts/alert_rate/threshold/mode)  [NOVO]\n",
    "\n",
    "Pontos FIXOS:\n",
    "- Separador ';' e encoding 'utf-8-sig'\n",
    "- Ordenação estável (mergesort) por score desc para ranking\n",
    "- Colunas de metadados adicionadas: threshold_used, mode, ks, psi, rank_desc\n",
    "\n",
    "Pontos CALIBRÁVEIS:\n",
    "- TOP_K exportado (por padrão 1000)\n",
    "- Colunas de contexto extra no início do CSV final (ORDER_FRONT)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- Pré-checagens --------\n",
    "assert 'RUN_DIR' in globals(), \"Execute a Etapa 1 antes (RUN_DIR).\"\n",
    "run_dir = Path(RUN_DIR)\n",
    "scores_path = run_dir / \"scores.csv\"\n",
    "th_path     = run_dir / \"threshold.json\"\n",
    "assert scores_path.exists(),  \"scores.csv ausente (rode a Etapa 8).\"\n",
    "assert th_path.exists(),      \"threshold.json ausente (rode a Etapa 9).\"\n",
    "\n",
    "# -------- Carregar dados --------\n",
    "CSV_SEP, CSV_ENC = \";\", \"utf-8-sig\"\n",
    "df = pd.read_csv(scores_path, sep=CSV_SEP, encoding=CSV_ENC, dtype=str)\n",
    "\n",
    "# garante colunas base\n",
    "assert \"score\" in df.columns, \"scores.csv precisa ter coluna 'score'.\"\n",
    "# preserva uma cópia do índice original (útil p/ rastreio)\n",
    "df.insert(0, \"_rowid\", np.arange(len(df), dtype=np.int64))\n",
    "\n",
    "# coerção numérica do score\n",
    "df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "# -------- Ler threshold e metadados --------\n",
    "cfg = json.loads(th_path.read_text(encoding=\"utf-8\"))\n",
    "THRESHOLD = float(cfg[\"threshold\"])\n",
    "MODE      = cfg.get(\"mode\", \"?\")\n",
    "KS        = float(cfg.get(\"ks_val_vs_current\", np.nan))\n",
    "PSI       = float(cfg.get(\"psi_val_vs_current\", np.nan))\n",
    "\n",
    "# -------- Aplicar threshold → alert --------\n",
    "df[\"alert\"] = (df[\"score\"] >= THRESHOLD).astype(\"int8\")\n",
    "rate_current = float(df[\"alert\"].mean())\n",
    "\n",
    "# -------- Ordenação estável por score desc (ranking) --------\n",
    "# mergesort é estável → se empatar score, mantém a ordem original (_rowid)\n",
    "df_sorted = df.sort_values(by=[\"score\", \"_rowid\"], ascending=[False, True], kind=\"mergesort\").copy()\n",
    "df_sorted.insert(1, \"rank_desc\", np.arange(1, len(df_sorted) + 1, dtype=np.int64))\n",
    "\n",
    "# -------- Metadados úteis --------\n",
    "df_sorted.insert(2, \"threshold_used\", THRESHOLD)\n",
    "df_sorted.insert(3, \"mode\", MODE)\n",
    "df_sorted.insert(4, \"ks_val_vs_current\", KS)\n",
    "df_sorted.insert(5, \"psi_val_vs_current\", PSI)\n",
    "\n",
    "# -------- Reorganizar colunas (frente com contexto) --------\n",
    "ORDER_FRONT = [\n",
    "    \"alert\", \"rank_desc\", \"score\", \"threshold_used\", \"mode\",\n",
    "    \"ks_val_vs_current\", \"psi_val_vs_current\",\n",
    "    \"_rowid\",\n",
    "]\n",
    "cols_final = ORDER_FRONT + [c for c in df_sorted.columns if c not in ORDER_FRONT]\n",
    "df_final = df_sorted[cols_final].copy()\n",
    "\n",
    "# -------- Salvar saídas principais --------\n",
    "out_full = run_dir / \"scores_alerts.csv\"\n",
    "out_topk = run_dir / \"scores_alerts_top1000.csv\"\n",
    "TOP_K = 1000  # CALIBRÁVEL\n",
    "\n",
    "df_final.to_csv(out_full, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "df_final.head(TOP_K).to_csv(out_topk, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "\n",
    "# -------- Agregação por usuário (se existir coluna 'username') --------\n",
    "if \"username\" in df_final.columns:\n",
    "    by_user = (\n",
    "        df_final.groupby(\"username\", dropna=False)\n",
    "                .agg(alerts=(\"alert\", \"sum\"),\n",
    "                     total=(\"alert\", \"count\"),\n",
    "                     pct_alerts=(\"alert\", \"mean\"),\n",
    "                     max_score=(\"score\", \"max\"))\n",
    "                .reset_index()\n",
    "                .sort_values([\"alerts\",\"max_score\"], ascending=[False, False])\n",
    "    )\n",
    "    by_user[\"pct_alerts\"] = (by_user[\"pct_alerts\"] * 100).round(2)\n",
    "    out_by_user = run_dir / \"alerts_by_username.csv\"\n",
    "    by_user.to_csv(out_by_user, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "    print(f\"[§10] salvo: {out_by_user.name}\")\n",
    "else:\n",
    "    print(\"[§10] coluna 'username' ausente — pulando alerts_by_username.csv\")\n",
    "\n",
    "# -------- Top 100 alertas (p/ relatório §12) — NOVO --------\n",
    "top100 = df_final[df_final[\"alert\"] == 1].copy()\n",
    "top100 = top100.sort_values(by=[\"score\", \"_rowid\"], ascending=[False, True], kind=\"mergesort\").head(100)\n",
    "out_top100 = run_dir / \"alerts_top100.csv\"\n",
    "top100.to_csv(out_top100, index=False, sep=CSV_SEP, encoding=CSV_ENC)\n",
    "print(f\"[§10] salvo: {out_top100.name}\")\n",
    "\n",
    "# -------- Sumários --------\n",
    "summary = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"mode\": MODE,\n",
    "    \"ks_val_vs_current\": KS,\n",
    "    \"psi_val_vs_current\": PSI,\n",
    "    \"n_rows\": int(len(df_final)),\n",
    "    \"n_alerts\": int(df_final[\"alert\"].sum()),\n",
    "    \"alert_rate\": float(rate_current),\n",
    "    \"top_k\": TOP_K,\n",
    "    \"outputs\": {\n",
    "        \"scores_alerts_csv\": str(out_full),\n",
    "        \"scores_alerts_topk_csv\": str(out_topk),\n",
    "        \"alerts_top100_csv\": str(out_top100),\n",
    "        \"alerts_by_username_csv\": str(run_dir / \"alerts_by_username.csv\"),\n",
    "    }\n",
    "}\n",
    "(run_dir / \"alerts_summary.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[§10] salvo: alerts_summary.json\")\n",
    "\n",
    "# -------- Atualizar/criar scores_summary.json (p/ §12) — NOVO --------\n",
    "scores_summary_path = run_dir / \"scores_summary.json\"\n",
    "scores_summary = {\n",
    "    \"n_linhas\": int(len(df_final)),\n",
    "    \"n_alerts\": int(df_final[\"alert\"].sum()),\n",
    "    \"alert_rate\": float(rate_current),\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"mode\": MODE,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "try:\n",
    "    # mantém 'mode_config' se já existir (do §9)\n",
    "    if scores_summary_path.exists():\n",
    "        existing = json.loads(scores_summary_path.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(existing, dict):\n",
    "            for k in (\"mode_config\",):\n",
    "                if k in existing and k not in scores_summary:\n",
    "                    scores_summary[k] = existing[k]\n",
    "    scores_summary_path.write_text(json.dumps(scores_summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"[§10] salvo: {scores_summary_path.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"[§10] aviso: falha ao salvar scores_summary.json: {e}\")\n",
    "\n",
    "print(\"\\n[§10] ALERTS materializados com sucesso.\")\n",
    "print(f\"[§10] threshold={THRESHOLD:.6f}  modo={MODE}  KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "print(f\"[§10] taxa de alertas no lote atual: {rate_current:.2%}\")\n",
    "print(f\"[§10] salvo: {out_full.name}, {out_topk.name}\")\n",
    "print(\"Próximo passo: Etapa 11 — monitoramento de drift (distribuição do score e do erro).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFQskoCtjs_K"
   },
   "source": [
    "## **Etapa 11:** Monitoramento de drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OhFtd1Ak5e6L"
   },
   "outputs": [],
   "source": [
    "# @title §11 — Monitoramento de Drift (score atual vs erro de validação) + séries diárias (exibe e salva figuras)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Objetivo\n",
    "--------\n",
    "1) Carregar:\n",
    "   - RUN_DIR/reconstruction_errors_val.npy  (baseline - Etapa 7)\n",
    "   - RUN_DIR/scores.csv                     (lote atual - Etapas 8/10)\n",
    "2) Calcular métricas de drift: KS e PSI\n",
    "3) Gerar, SALVAR e EXIBIR figuras:\n",
    "   - Histograma comparativo (baseline vs atual)\n",
    "   - CDF comparativa (ECDF baseline vs ECDF atual)\n",
    "   - Boxplot diário (se existir 'data_lcto')\n",
    "4) Salvar artefatos:\n",
    "   - runs/<RUN_ID>/drift_metrics.json\n",
    "   - runs/<RUN_ID>/figures/drift_hist.png\n",
    "   - runs/<RUN_ID>/figures/drift_cdf.png\n",
    "   - runs/<RUN_ID>/figures/drift_daily_box.png (se aplicável)\n",
    "   - runs/<RUN_ID>/drift_bins_psi.csv\n",
    "   - runs/<RUN_ID>/images_base64.json\n",
    "   - runs/<RUN_ID>/drift_monitoring.json   [NOVO — compatível com §12]\n",
    "\"\"\"\n",
    "\n",
    "import json, io, base64\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display  # para exibir PNGs gerados\n",
    "\n",
    "assert 'RUN_DIR' in globals(), \"Execute a Etapa 1 para definir RUN_DIR.\"\n",
    "run_dir = Path(RUN_DIR)\n",
    "fig_dir = run_dir / \"figures\"   # <— §12 espera figuras aqui\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "val_err_path = run_dir / \"reconstruction_errors_val.npy\"\n",
    "scores_path  = run_dir / \"scores.csv\"\n",
    "assert val_err_path.exists(), \"reconstruction_errors_val.npy ausente (rode Etapa 7).\"\n",
    "assert scores_path.exists(),  \"scores.csv ausente (rode Etapas 8–10).\"\n",
    "\n",
    "# ----------------- carregar dados -----------------\n",
    "val_err = np.load(val_err_path)                       # baseline (val)\n",
    "df_sc   = pd.read_csv(scores_path, sep=\";\", encoding=\"utf-8-sig\")\n",
    "assert \"score\" in df_sc.columns, \"scores.csv precisa ter coluna 'score'.\"\n",
    "scores  = pd.to_numeric(df_sc[\"score\"], errors=\"coerce\").to_numpy()\n",
    "\n",
    "# ----------------- helpers métricas -----------------\n",
    "def ks_stat(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a[~np.isnan(a)]\n",
    "    b = b[~np.isnan(b)]\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return float(\"nan\")\n",
    "    xa, xb = np.sort(a), np.sort(b)\n",
    "    grid = np.unique(np.concatenate([xa, xb]))\n",
    "    def _ecdf(x, g): return np.searchsorted(x, g, side=\"right\") / x.size\n",
    "    Fa = np.array([_ecdf(xa, g) for g in grid], dtype=float)\n",
    "    Fb = np.array([_ecdf(xb, g) for g in grid], dtype=float)\n",
    "    return float(np.max(np.abs(Fa - Fb)))\n",
    "\n",
    "def psi_stat(expected: np.ndarray, actual: np.ndarray, bins: int = 20):\n",
    "    \"\"\"Population Stability Index com bins por quantis do baseline.\"\"\"\n",
    "    e = expected[~np.isnan(expected)]\n",
    "    a = actual[~np.isnan(actual)]\n",
    "    if e.size == 0 or a.size == 0:\n",
    "        return float(\"nan\"), None\n",
    "    qs = np.linspace(0, 1, bins + 1)\n",
    "    cuts = np.quantile(e, qs)\n",
    "    cuts = np.unique(cuts)\n",
    "    if cuts.size < 3:\n",
    "        return 0.0, pd.DataFrame()\n",
    "    e_hist, edges = np.histogram(e, bins=cuts)\n",
    "    a_hist, _     = np.histogram(a, bins=cuts)\n",
    "    e_prop = np.clip(e_hist / max(1, e_hist.sum()), 1e-8, 1.0)\n",
    "    a_prop = np.clip(a_hist / max(1, a_hist.sum()), 1e-8, 1.0)\n",
    "    contrib = (a_prop - e_prop) * np.log(a_prop / e_prop)\n",
    "    psi = float(np.sum(contrib))\n",
    "    bins_df = pd.DataFrame({\n",
    "        \"bin_left\": edges[:-1],\n",
    "        \"bin_right\": edges[1:],\n",
    "        \"expected_count\": e_hist,\n",
    "        \"actual_count\": a_hist,\n",
    "        \"expected_prop\": e_prop,\n",
    "        \"actual_prop\": a_prop,\n",
    "        \"psi_contrib\": contrib,\n",
    "    })\n",
    "    return psi, bins_df\n",
    "\n",
    "# ----------------- parâmetros gráficos -----------------\n",
    "NUM_BINS = 50\n",
    "FIG_DPI  = 140\n",
    "\n",
    "# ----------------- métricas KS/PSI -----------------\n",
    "KS  = ks_stat(val_err, scores)\n",
    "PSI, psi_bins = psi_stat(val_err, scores, bins=20)\n",
    "\n",
    "# ----------------- figuras: histograma comparativo -----------------\n",
    "fig1, ax1 = plt.subplots(figsize=(8,4), dpi=FIG_DPI)\n",
    "ax1.hist(val_err, bins=NUM_BINS, density=True, alpha=0.5, label=\"Validação (baseline)\")\n",
    "ax1.hist(scores,  bins=NUM_BINS, density=True, alpha=0.5, label=\"Lote atual (scores)\")\n",
    "ax1.set_title(f\"Distribuições — baseline vs atual  |  KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "ax1.set_xlabel(\"Erro / Score\")\n",
    "ax1.set_ylabel(\"Densidade\")\n",
    "ax1.legend()\n",
    "hist_path = fig_dir / \"drift_hist.png\"   # <— salva em figures/\n",
    "fig1.tight_layout()\n",
    "fig1.savefig(hist_path, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "plt.close(fig1)\n",
    "\n",
    "# EXIBIR histograma\n",
    "display(Image(filename=str(hist_path)))\n",
    "\n",
    "# ----------------- figuras: CDF comparativa -----------------\n",
    "def _ecdf_values(x: np.ndarray):\n",
    "    x = x[~np.isnan(x)]\n",
    "    x = np.sort(x)\n",
    "    y = np.arange(1, x.size + 1) / x.size if x.size else np.array([])\n",
    "    return x, y\n",
    "\n",
    "xv, yv = _ecdf_values(val_err)\n",
    "xs, ys = _ecdf_values(scores)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8,4), dpi=FIG_DPI)\n",
    "if xv.size: ax2.step(xv, yv, where=\"post\", label=\"ECDF validação\")\n",
    "if xs.size: ax2.step(xs, ys, where=\"post\", label=\"ECDF atual\")\n",
    "ax2.set_title(\"CDF acumulada — baseline vs atual\")\n",
    "ax2.set_xlabel(\"Erro / Score\")\n",
    "ax2.set_ylabel(\"Proporção ≤ x\")\n",
    "ax2.legend()\n",
    "cdf_path = fig_dir / \"drift_cdf.png\"     # <— salva em figures/\n",
    "fig2.tight_layout()\n",
    "fig2.savefig(cdf_path, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "plt.close(fig2)\n",
    "\n",
    "# EXIBIR CDF\n",
    "display(Image(filename=str(cdf_path)))\n",
    "\n",
    "# ----------------- figura: boxplot diário (se houver data_lcto) -----------------\n",
    "daily_path = None\n",
    "if \"data_lcto\" in df_sc.columns:\n",
    "    dt = pd.to_datetime(df_sc[\"data_lcto\"], errors=\"coerce\")\n",
    "    df_daily = pd.DataFrame({\"data\": dt.dt.date.astype(\"string\"), \"score\": pd.to_numeric(df_sc[\"score\"], errors=\"coerce\")})\n",
    "    df_daily = df_daily.dropna()\n",
    "    if not df_daily.empty:\n",
    "        groups = df_daily.groupby(\"data\")[\"score\"].apply(list)\n",
    "        labels = list(groups.index)\n",
    "        data   = list(groups.values)\n",
    "\n",
    "        fig3, ax3 = plt.subplots(figsize=(max(8, min(16, 0.25*len(labels))), 4), dpi=FIG_DPI)\n",
    "        ax3.boxplot(data, showfliers=False)\n",
    "        ax3.set_title(\"Distribuição diária do score (boxplot sem outliers)\")\n",
    "        ax3.set_xlabel(\"Data do lançamento\")\n",
    "        ax3.set_ylabel(\"Score\")\n",
    "        step = max(1, len(labels)//20)\n",
    "        ax3.set_xticks(range(1, len(labels)+1)[::step], labels[::step], rotation=45, ha=\"right\")\n",
    "        fig3.tight_layout()\n",
    "        daily_path = fig_dir / \"drift_daily_box.png\"  # <— salva em figures/\n",
    "        fig3.savefig(daily_path, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "        plt.close(fig3)\n",
    "\n",
    "        # EXIBIR boxplot diário\n",
    "        display(Image(filename=str(daily_path)))\n",
    "\n",
    "# ----------------- salvar tabelas auxiliares (PSI bins) -----------------\n",
    "psi_bins_path = run_dir / \"drift_bins_psi.csv\"\n",
    "if isinstance(psi_bins, pd.DataFrame) and not psi_bins.empty:\n",
    "    psi_bins.to_csv(psi_bins_path, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "else:\n",
    "    psi_bins_path = None\n",
    "\n",
    "# ----------------- export base64 para Etapa 12 (HTML) -----------------\n",
    "def _png_to_b64(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"ascii\")\n",
    "\n",
    "images_b64 = {}\n",
    "images_b64[\"drift_hist.png\"] = _png_to_b64(hist_path)\n",
    "images_b64[\"drift_cdf.png\"]  = _png_to_b64(cdf_path)\n",
    "if daily_path:\n",
    "    images_b64[\"drift_daily_box.png\"] = _png_to_b64(daily_path)\n",
    "\n",
    "(run_dir / \"images_base64.json\").write_text(json.dumps(images_b64), encoding=\"utf-8\")\n",
    "\n",
    "# ----------------- salvar métricas (compat) -----------------\n",
    "metrics = {\n",
    "    \"ks_val_vs_current\": float(KS),\n",
    "    \"psi_val_vs_current\": float(PSI),\n",
    "    \"n_val\": int(np.sum(~np.isnan(val_err))),\n",
    "    \"n_current\": int(np.sum(~np.isnan(scores))),\n",
    "    \"hist_png\": str(hist_path),\n",
    "    \"cdf_png\": str(cdf_path),\n",
    "    \"daily_box_png\": (str(daily_path) if daily_path else None),\n",
    "    \"psi_bins_csv\": (str(psi_bins_path) if psi_bins_path else None),\n",
    "}\n",
    "(run_dir / \"drift_metrics.json\").write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ----------------- salvar drift_monitoring.json (formato simples p/ §12) — NOVO -----------------\n",
    "drift_monitoring = {\n",
    "    \"kpis\": {\"KS\": float(KS), \"PSI\": float(PSI)},\n",
    "    \"figures\": {\n",
    "        \"hist\": str(hist_path),\n",
    "        \"cdf\": str(cdf_path),\n",
    "        \"daily_box\": (str(daily_path) if daily_path else None)\n",
    "    },\n",
    "    \"bins_psi_csv\": (str(psi_bins_path) if psi_bins_path else None)\n",
    "}\n",
    "(run_dir / \"drift_monitoring.json\").write_text(json.dumps(drift_monitoring, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n[§11] Monitoramento de drift concluído.\")\n",
    "print(f\"[§11] KS={KS:.4f}  PSI={PSI:.4f}\")\n",
    "print(f\"[§11] Figuras salvas e exibidas: {hist_path.name}, {cdf_path.name}\" + (f\", {Path(daily_path).name}\" if daily_path else \"\"))\n",
    "if psi_bins_path:\n",
    "    print(f\"[§11] Tabela de bins do PSI: {Path(psi_bins_path).name}\")\n",
    "print(\"[§11] images_base64.json e drift_monitoring.json prontos para o relatório HTML (Etapa 12).\")\n",
    "print(\"\\nPróximo passo: Etapa 12 — relatório HTML com imagens embutidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jXlzTkKkusq"
   },
   "source": [
    "## **Etapa 12:** Relatório HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ACV1EP2e57g4"
   },
   "outputs": [],
   "source": [
    "# @title §12 — Relatório consolidado (HTML, somente leitura de artefatos; sem reprocessamento)\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Este parágrafo gera um ÚNICO relatório HTML, incorporando:\n",
    "1) Informações da execução (data/hora, paths, arquivos gerados, utilidade)\n",
    "2) Contexto sobre AE Tabular e redes neurais (linguagem acessível)\n",
    "3) Descrição dos features\n",
    "4) Estatística descritiva do treino/validação (quantitativos; usa features_desc.json se existir)\n",
    "5) Estatística da base de execução (Etapa 8) e comparação de distribuição com treino (se existir)\n",
    "6) Monitoramento de drift (se existir)\n",
    "7) Top 100 lançamentos com alerta (Etapa 10), se existir\n",
    "\n",
    "Regras:\n",
    "- NÃO executa nenhum cálculo pesado nem reprocessa dados: tudo é lido do RUN_DIR/artifacts.\n",
    "- Gráficos já existentes são incorporados (base64) e redimensionados para largura máx. 500px.\n",
    "- Monetário com 2 casas; demais com no máx. 5 casas.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, io, json, base64, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# --------------------------\n",
    "# Pré-checagens MÍNIMAS\n",
    "# --------------------------\n",
    "assert 'RUN_DIR' in globals(), \"Execute §1 antes (RUN_DIR).\"\n",
    "assert 'PROJ_ROOT' in globals(), \"Execute §1 antes (PROJ_ROOT).\"\n",
    "\n",
    "RUN_DIR = Path(RUN_DIR)\n",
    "REPORTS_DIR = Path(REPORTS_DIR) if 'REPORTS_DIR' in globals() else (Path(PROJ_ROOT) / \"reports\")\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Utilidades\n",
    "# --------------------------\n",
    "def _b64_img(path: Path, max_width_px: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Retorna uma <img> com base64 inline. O redimensionamento por largura é feito via atributo HTML/CSS.\n",
    "    (Não reamostra o arquivo — usa width para layout.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = path.read_bytes()\n",
    "        mime = \"image/png\" if path.suffix.lower()==\".png\" else \"image/jpeg\"\n",
    "        b64 = base64.b64encode(data).decode(\"ascii\")\n",
    "        return f'<img src=\"data:{mime};base64,{b64}\" alt=\"{path.name}\" style=\"max-width:{max_width_px}px;width:100%;height:auto;border:1px solid #ddd;border-radius:6px;\"/>'\n",
    "    except Exception as e:\n",
    "        return f'<div style=\"color:#b00;\">(Falha ao embutir imagem {path.name}: {e})</div>'\n",
    "\n",
    "def _fmt_money(v) -> str:\n",
    "    try:\n",
    "        f = float(v)\n",
    "        return f\"{f:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "def _fmt_stat(v) -> str:\n",
    "    try:\n",
    "        f = float(v)\n",
    "        # até 5 casas decimais (sem notação científica)\n",
    "        s = f\"{f:.5f}\"\n",
    "        # remove zeros supérfluos à direita\n",
    "        s = re.sub(r\"(\\.[0-9]*?)0+$\", r\"\\1\", s)\n",
    "        s = re.sub(r\"\\.$\", \"\", s)\n",
    "        return s\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "def _safe_json(path: Path) -> Any | None:\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _safe_csv(path: Path, **kw) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_csv(path, **kw)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _list_files_human(folder: Path) -> List[tuple[str, str]]:\n",
    "    out = []\n",
    "    for p in sorted(folder.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            size = p.stat().st_size\n",
    "            n = size\n",
    "            for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "                if n < 1024:\n",
    "                    out.append((str(p.relative_to(PROJ_ROOT)), f\"{n:.1f}{u}\"))\n",
    "                    break\n",
    "                n /= 1024.0\n",
    "    return out\n",
    "\n",
    "def _section(title: str, body_html: str) -> str:\n",
    "    return f\"\"\"\n",
    "    <section style=\"margin:24px 0;\">\n",
    "      <h2 style=\"margin:0 0 8px 0;font-family:Inter,Arial;font-weight:700;\">{title}</h2>\n",
    "      <div style=\"font-family:Inter,Arial;line-height:1.5;font-size:14px;color:#222;\">\n",
    "        {body_html}\n",
    "      </div>\n",
    "    </section>\n",
    "    \"\"\"\n",
    "\n",
    "def _table_dicts(rows: List[Dict[str, Any]], col_order: List[str]|None=None, monetary_cols: List[str]|None=None, max_rows:int=1000) -> str:\n",
    "    if not rows:\n",
    "        return \"<div style='color:#555;'>Sem dados.</div>\"\n",
    "    if col_order is None:\n",
    "        col_order = list(rows[0].keys())\n",
    "    monetary_cols = set(monetary_cols or [])\n",
    "    head = \"\".join(f\"<th style='text-align:left;padding:6px 8px;background:#f5f5f5;border-bottom:1px solid #ddd;'>{c}</th>\" for c in col_order)\n",
    "    body = []\n",
    "    for r in rows[:max_rows]:\n",
    "        tds = []\n",
    "        for c in col_order:\n",
    "            v = r.get(c, \"\")\n",
    "            if c in monetary_cols:\n",
    "                v = _fmt_money(v)\n",
    "            elif isinstance(v, (int, float, np.floating)) and c not in monetary_cols:\n",
    "                v = _fmt_stat(v)\n",
    "            tds.append(f\"<td style='padding:6px 8px;border-bottom:1px solid #eee;'>{v}</td>\")\n",
    "        body.append(\"<tr>\" + \"\".join(tds) + \"</tr>\")\n",
    "    return f\"<div style='overflow:auto;'><table style='border-collapse:collapse;width:100%;min-width:480px;'>{'<thead><tr>'+head+'</tr></thead><tbody>' + ''.join(body) + '</tbody></table></div>'}\"\n",
    "\n",
    "# --------------------------\n",
    "# Localiza artefatos\n",
    "# --------------------------\n",
    "paths = {\n",
    "    \"run_json\"            : RUN_DIR / \"run.json\",\n",
    "    \"selected_source_csv\" : RUN_DIR / \"selected_source.csv\",\n",
    "    \"features_config\"     : RUN_DIR / \"features_config.json\",\n",
    "    \"features_desc\"       : RUN_DIR / \"features_desc.json\",\n",
    "    \"categorical_maps\"    : RUN_DIR / \"categorical_maps.json\",\n",
    "    \"training_history\"    : RUN_DIR / \"training_history.csv\",            # §7\n",
    "    \"model_config\"        : RUN_DIR / \"model_config.json\",               # §7\n",
    "    \"ae_weights\"          : RUN_DIR / \"ae.pt\",                           # §7\n",
    "    \"recon_err_val\"       : RUN_DIR / \"reconstruction_errors_val.npy\",   # §7\n",
    "    \"scores_summary\"      : RUN_DIR / \"scores_summary.json\",             # §9/§10 (quando existir)\n",
    "    \"alerts_top_csv\"      : RUN_DIR / \"alerts_top100.csv\",               # §10 (quando existir)\n",
    "    \"exec_stats_json\"     : RUN_DIR / \"exec_stats.json\",                 # §8 (quando existir)\n",
    "    \"dist_compare_png\"    : RUN_DIR / \"figures/dist_exec_vs_train.png\",  # §8 (quando existir)\n",
    "    \"drift_json\"          : RUN_DIR / \"drift_monitoring.json\",           # §11 (quando existir)\n",
    "    \"drift_fig_dir\"       : RUN_DIR / \"figures\",                         # pasta com gráficos diversos\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 1) informações da execução\n",
    "# --------------------------\n",
    "run_meta = _safe_json(paths[\"run_json\"]) or {}\n",
    "created_at = run_meta.get(\"created_at\")\n",
    "paths_meta = run_meta.get(\"paths\", {})\n",
    "lista_arquivos = _list_files_human(RUN_DIR)\n",
    "\n",
    "html_exec = []\n",
    "html_exec.append(f\"<p><b>Data/hora da execução:</b> {created_at or '(desconhecido)'} (timezone: {run_meta.get('timezone','?')})</p>\")\n",
    "html_exec.append(\"<p><b>Pastas relevantes</b></p>\")\n",
    "html_exec.append(_table_dicts(\n",
    "    [{\"chave\": k, \"caminho\": v} for k,v in paths_meta.items()],\n",
    "    col_order=[\"chave\",\"caminho\"]\n",
    "))\n",
    "\n",
    "if lista_arquivos:\n",
    "    util_map = {\n",
    "        \"run.json\": \"Metadados da execução\",\n",
    "        \"selected_source.csv\": \"Snapshot da base usada para treino/val\",\n",
    "        \"journal_entries.parquet\": \"Snapshot parquet da base\",\n",
    "        \"features_config.json\": \"Configuração de colunas de features\",\n",
    "        \"features_desc.json\": \"Estatísticas descritivas de X_train/X_val\",\n",
    "        \"dataset_npz.npz\": \"Conjuntos prontos (train/val) e índices\",\n",
    "        \"categorical_maps.json\": \"Vocabulário categórico congelado\",\n",
    "        \"training_history.csv\": \"Histórico de loss (treino/val)\",\n",
    "        \"model_config.json\": \"Arquitetura/hiperparâmetros do AE\",\n",
    "        \"ae.pt\": \"Pesos do modelo treinado\",\n",
    "        \"reconstruction_errors_val.npy\": \"Erros de reconstrução no conjunto de validação\",\n",
    "        \"exec_stats.json\": \"Estatísticas descritivas da base executada\",\n",
    "        \"scores_summary.json\": \"Sumário de pontuações/limiares\",\n",
    "        \"alerts_top100.csv\": \"Top 100 alertas (pós-calibração)\",\n",
    "        \"drift_monitoring.json\": \"KPIs e caminhos das figuras de drift (Etapa 11)\",\n",
    "    }\n",
    "    rows = []\n",
    "    for rel, sz in lista_arquivos:\n",
    "        base = os.path.basename(rel)\n",
    "        rows.append({\"arquivo\": rel, \"tamanho\": sz, \"utilidade\": util_map.get(base, \"\")})\n",
    "    html_exec.append(\"<p><b>Arquivos gerados nesta execução</b></p>\")\n",
    "    html_exec.append(_table_dicts(rows, col_order=[\"arquivo\",\"tamanho\",\"utilidade\"]))\n",
    "else:\n",
    "    html_exec.append(\"<p style='color:#b00;'>Aviso: não foi possível listar arquivos em RUN_DIR.</p>\")\n",
    "\n",
    "sec1 = _section(\"1) Informações da execução\", \"\".join(html_exec))\n",
    "\n",
    "# --------------------------\n",
    "# 2) contextualização AE Tabular\n",
    "# --------------------------\n",
    "ctx = \"\"\"\n",
    "<p><b>O que é um Autoencoder (AE) tabular?</b><br/>\n",
    "É um tipo de rede neural que aprende a <i>reconstruir</i> seus próprios dados de entrada.\n",
    "Para dados tabulares, montamos uma matriz numérica (features) e treinamos a rede para que a saída seja o mais\n",
    "parecida possível com a entrada. Se um registro for muito “diferente” do padrão aprendido, o erro de reconstrução tende a ficar mais alto — e isso é um bom sinal de possível anomalia.</p>\n",
    "\n",
    "<p><b>Como a rede neural funciona, em linhas gerais?</b><br/>\n",
    "Uma rede neural é composta por camadas de neurônios artificiais. No AE, há um <i>codificador</i> (que comprime as informações em um “gargalo”) e um <i>decodificador</i> (que tenta reconstruir os dados originais).\n",
    "Durante o treino, comparamos a reconstrução com a entrada e ajustamos os pesos para reduzir a diferença (o “erro”).</p>\n",
    "\n",
    "<p><b>Visão geral do pipeline</b><br/>\n",
    "(1) Pré-processar CSVs para um formato padronizado.\n",
    "(2) Ingerir os dados tratados e congelar um vocabulário para colunas categóricas.\n",
    "(3) Engenhar features (ex.: codificação de categorias, transformações numéricas).\n",
    "(4) Preparar matriz, fazer split treino/validação, imputar e normalizar.\n",
    "(5) Treinar o AE com <i>early stopping</i> e registrar histórico.\n",
    "(6) Calibrar um limiar no erro de reconstrução para sinalizar alertas.\n",
    "(7) Executar no conjunto-alvo e gerar relatórios e monitoramento de drift.</p>\n",
    "\"\"\"\n",
    "sec2 = _section(\"2) Contextualização do AE Tabular e da rede neural\", ctx)\n",
    "\n",
    "# --------------------------\n",
    "# 3) descrição dos features\n",
    "# --------------------------\n",
    "feat_cfg = _safe_json(paths[\"features_config\"]) or {}\n",
    "feature_cols = feat_cfg.get(\"feature_cols\", [])\n",
    "n_features = feat_cfg.get(\"n_features\", len(feature_cols))\n",
    "\n",
    "desc_features_html = []\n",
    "if feature_cols:\n",
    "    desc_features_html.append(\"<p>O modelo usou as seguintes colunas de features. Colunas com sufixo <code>_int</code> são categorias codificadas; prefixo <code>feat_</code> indica derivadas numéricas.</p>\")\n",
    "    rows = [{\"#\": i+1, \"feature\": c, \"tipo\": (\"categórica codificada\" if c.endswith(\"_int\") else \"derivada/numérica\")}\n",
    "            for i,c in enumerate(feature_cols)]\n",
    "    desc_features_html.append(_table_dicts(rows, col_order=[\"#\",\"feature\",\"tipo\"]))\n",
    "else:\n",
    "    desc_features_html.append(\"<p style='color:#b00;'>Aviso: não encontrei features_config.json. Rode §§4–6 antes para registrar features.</p>\")\n",
    "\n",
    "sec3 = _section(\"3) Features do modelo\", \"\".join(desc_features_html))\n",
    "\n",
    "# --------------------------\n",
    "# 4) treino/validação: estatística e épocas\n",
    "# --------------------------\n",
    "feat_desc = _safe_json(paths[\"features_desc\"]) or {}\n",
    "hist_df = _safe_csv(paths[\"training_history\"])\n",
    "model_cfg = _safe_json(paths[\"model_config\"]) or {}\n",
    "\n",
    "html_tv = []\n",
    "if feat_desc:\n",
    "    shape_tr = feat_desc.get(\"train\", {}).get(\"shape\")\n",
    "    shape_va = feat_desc.get(\"val\", {}).get(\"shape\")\n",
    "    html_tv.append(f\"<p><b>Formas:</b> train={shape_tr}, val={shape_va}</p>\")\n",
    "    means = feat_desc.get(\"train\", {}).get(\"mean\", [])\n",
    "    stds  = feat_desc.get(\"train\", {}).get(\"std\",  [])\n",
    "    rows = [{\"feature\": feature_cols[i] if i < len(feature_cols) else f\"col_{i}\",\n",
    "             \"média (train)\": _fmt_stat(means[i]) if i < len(means) else \"\",\n",
    "             \"desvio-padrão (train)\": _fmt_stat(stds[i])  if i < len(stds)  else \"\"}\n",
    "            for i in range(min(len(means), len(stds)))]\n",
    "    if rows:\n",
    "        html_tv.append(\"<p><b>Resumo estatístico (treino)</b></p>\")\n",
    "        html_tv.append(_table_dicts(rows, col_order=[\"feature\",\"média (train)\",\"desvio-padrão (train)\"]))\n",
    "else:\n",
    "    html_tv.append(\"<p style='color:#b00;'>Aviso: ausente features_desc.json (gerado na §6).</p>\")\n",
    "\n",
    "if hist_df is not None and not hist_df.empty:\n",
    "    n_epochs = int(hist_df[\"epoch\"].max()) + 1 if \"epoch\" in hist_df.columns else len(hist_df)\n",
    "    html_tv.append(f\"<p><b>Épocas de treino:</b> {n_epochs}</p>\")\n",
    "    curve_candidates = [\n",
    "        RUN_DIR / \"figures\" / \"training_curve.png\",\n",
    "        RUN_DIR / \"figures\" / \"loss_history.png\"\n",
    "    ]\n",
    "    imgs = [p for p in curve_candidates if p.exists()]\n",
    "    if imgs:\n",
    "        html_tv.append(\"<p><b>Curva de treino/validação</b><br/><i>Perdas por época; observe a estabilização e a parada antecipada.</i></p>\")\n",
    "        for p in imgs:\n",
    "            html_tv.append(_b64_img(p, 500))\n",
    "else:\n",
    "    html_tv.append(\"<p style='color:#b00;'>Aviso: ausente training_history.csv (gerado na §7).</p>\")\n",
    "\n",
    "sec4 = _section(\"4) Base de treino e validação\", \"\".join(html_tv))\n",
    "\n",
    "# --------------------------\n",
    "# 5) estatística da base de EXECUÇÃO (Etapa 8) e comparação de distribuição\n",
    "# --------------------------\n",
    "html_execset = []\n",
    "exec_stats = _safe_json(paths[\"exec_stats_json\"]) or {}\n",
    "\n",
    "# (1) resumo estatístico (se existir)\n",
    "if exec_stats:\n",
    "    basic = exec_stats.get(\"basic\", {})\n",
    "    if basic:\n",
    "        rows = [{\"métrica\": k, \"valor\": _fmt_stat(v)} for k,v in basic.items()]\n",
    "        html_execset.append(\"<p><b>Resumo estatístico da base de execução</b></p>\")\n",
    "        html_execset.append(_table_dicts(rows, col_order=[\"métrica\",\"valor\"]))\n",
    "else:\n",
    "    html_execset.append(\"<p style='color:#555;'>Sem exec_stats.json (gerado normalmente na §8).</p>\")\n",
    "\n",
    "# (2) alertas/threshold/método — SEMPRE que existir scores_summary.json\n",
    "scores_sum = _safe_json(paths[\"scores_summary\"]) or {}\n",
    "parts = []\n",
    "if \"n_alerts\" in scores_sum and \"n_linhas\" in scores_sum:\n",
    "    try:\n",
    "        rate = float(scores_sum.get(\"alert_rate\", 0.0)) * 100\n",
    "        parts.append(f\"<b>Alertas:</b> {int(scores_sum['n_alerts']):,} de {int(scores_sum['n_linhas']):,} ({rate:.2f}%)\")\n",
    "    except Exception:\n",
    "        parts.append(f\"<b>Alertas:</b> {int(scores_sum['n_alerts']):,} de {int(scores_sum['n_linhas']):,}\")\n",
    "if \"threshold\" in scores_sum:\n",
    "    parts.append(f\"<b>Limiar</b>: {_fmt_stat(scores_sum['threshold'])}\")\n",
    "mode = scores_sum.get(\"mode\")\n",
    "if mode:\n",
    "    parts.append(f\"<b>Método</b>: {mode}\")\n",
    "if parts:\n",
    "    html_execset.append(\"<p>\" + \" &nbsp;•&nbsp; \".join(parts) + \"</p>\")\n",
    "\n",
    "# (3) figura de comparação, se existir\n",
    "if paths[\"dist_compare_png\"].exists():\n",
    "    html_execset.append(\"<p><b>Comparação de distribuição</b><br/><i>Distribuições da base de execução vs. treino (por feature chave).</i></p>\")\n",
    "    html_execset.append(_b64_img(paths[\"dist_compare_png\"], 500))\n",
    "else:\n",
    "    html_execset.append(\"<p style='color:#555;'>Gráfico de comparação de distribuição não encontrado. Ao rodar a §8, salve em <code>figures/dist_exec_vs_train.png</code>.</p>\")\n",
    "\n",
    "sec5 = _section(\"5) Base de execução: estatística e comparação de distribuição\", \"\".join(html_execset))\n",
    "\n",
    "# --------------------------\n",
    "# 6) Monitoramento de drift\n",
    "# --------------------------\n",
    "html_drift = []\n",
    "drift_obj = _safe_json(paths[\"drift_json\"]) or {}\n",
    "if drift_obj:\n",
    "    html_drift.append(\"<p><b>O que é drift?</b> É a mudança do comportamento dos dados ao longo do tempo. Se o modelo foi treinado com um padrão e a produção passa a ter outro, as previsões podem piorar. Monitoramos métricas de distância entre distribuições e o erro de reconstrução.</p>\")\n",
    "    kpis = drift_obj.get(\"kpis\", {})\n",
    "    if kpis:\n",
    "        rows = [{\"métrica\": k, \"valor\": _fmt_stat(v)} for k,v in kpis.items()]\n",
    "        html_drift.append(_table_dicts(rows, col_order=[\"métrica\",\"valor\"]))\n",
    "    if paths[\"drift_fig_dir\"].exists():\n",
    "        figs = sorted([p for p in paths[\"drift_fig_dir\"].glob(\"drift_*.png\")])\n",
    "        if figs:\n",
    "            html_drift.append(\"<p><b>Gráficos de drift</b> (interprete como proximidade/alteração entre distribuições ao longo do tempo):</p>\")\n",
    "            for p in figs:\n",
    "                html_drift.append(_b64_img(p, 500))\n",
    "else:\n",
    "    html_drift.append(\"<p style='color:#555;'>Sem arquivo de drift (<code>drift_monitoring.json</code>). Rode o monitoramento para preencher esta seção.</p>\")\n",
    "\n",
    "sec6 = _section(\"6) Monitoramento de drift\", \"\".join(html_drift))\n",
    "\n",
    "# --------------------------\n",
    "# 7) Top 100 alertas (Etapa 10)\n",
    "# --------------------------\n",
    "html_alerts = []\n",
    "alerts_df = _safe_csv(paths[\"alerts_top_csv\"], sep=\";\", encoding=\"utf-8-sig\")\n",
    "if alerts_df is not None and not alerts_df.empty:\n",
    "    money_like = [c for c in alerts_df.columns if re.search(r\"valor|valormi\", c, re.I)]\n",
    "    rows = []\n",
    "    for _, r in alerts_df.head(100).iterrows():\n",
    "        d = {k: r[k] for k in alerts_df.columns}\n",
    "        for c in money_like:\n",
    "            if c in d:\n",
    "                d[c] = _fmt_money(d[c])\n",
    "        rows.append(d)\n",
    "    # preferências de colunas; usa apenas as que existirem\n",
    "    prefer = [\"documento_num\",\"username\",\"lotacao\",\"contacontabil\",\"valormi\",\"score\",\"rank_desc\",\"motivo\",\"alert\"]\n",
    "    keep = [c for c in prefer if c in alerts_df.columns]\n",
    "    if not keep:\n",
    "        keep = list(alerts_df.columns)[:10]\n",
    "    html_alerts.append(_table_dicts(rows, col_order=keep, monetary_cols=money_like, max_rows=100))\n",
    "else:\n",
    "    html_alerts.append(\"<p style='color:#555;'>Arquivo <code>alerts_top100.csv</code> não encontrado.</p>\")\n",
    "\n",
    "sec7 = _section(\"7) Top 100 lançamentos com alertas\", \"\".join(html_alerts))\n",
    "\n",
    "# --------------------------\n",
    "# Montagem final do HTML\n",
    "# --------------------------\n",
    "now_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "title = f\"Relatório AE Tabular — Run {RUN_DIR.name}\"\n",
    "\n",
    "STYLE = \"\"\"\n",
    "<style>\n",
    "  @media (prefers-color-scheme: dark){\n",
    "    body{ background:#0f1115; color:#e6e6e6; }\n",
    "    table{ color:#e6e6e6; }\n",
    "  }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "html_full = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"pt-br\">\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<title>{title}</title>\n",
    "{STYLE}\n",
    "</head>\n",
    "<body style=\"margin:24px; font-family:Inter,Arial;\">\n",
    "  <header style=\"margin-bottom:16px;\">\n",
    "    <h1 style=\"margin:0 0 4px 0;\">{title}</h1>\n",
    "    <div style=\"color:#666;font-size:12px;\">Gerado em {now_str}</div>\n",
    "    <hr style=\"margin-top:12px;border:none;border-top:1px solid #ddd;\"/>\n",
    "  </header>\n",
    "  {sec1}\n",
    "  {sec2}\n",
    "  {sec3}\n",
    "  {sec4}\n",
    "  {sec5}\n",
    "  {sec6}\n",
    "  {sec7}\n",
    "  <footer style=\"margin-top:24px;color:#888;font-size:12px;\">\n",
    "    <hr style=\"border:none;border-top:1px solid #ddd;\"/>\n",
    "    <div>Este relatório apenas agrega artefatos já existentes no <code>RUN_DIR</code>; nenhuma etapa de processamento foi reexecutada.</div>\n",
    "  </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "out_html = REPORTS_DIR / f\"relatorio_run_{RUN_DIR.name}.html\"\n",
    "out_html.write_text(html_full, encoding=\"utf-8\")\n",
    "print(f\"[§12] Relatório HTML gerado: {out_html}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMREb0XnPfU/TcYMrmffXD5",
   "collapsed_sections": [
    "clC8wQo4V2_j"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
