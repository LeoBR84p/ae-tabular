{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r5OMfQfNaAm"
   },
   "source": [
    "#**Licen√ßa de Uso**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d22_lVKTNenJ"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "‚Üí You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution‚ÄìNonCommercial 4.0 International License.\n",
    "\n",
    "‚Üí You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**¬© 2025 Leandro Bernardo Rodrigues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRNqsqxZWPzs"
   },
   "source": [
    "# **Gest√£o do Ambiente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clC8wQo4V2_j"
   },
   "source": [
    "##**Criar reposit√≥rio .git no Colab**\n",
    "---\n",
    "Google Drive √© considerado o ponto de verdade\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1760321535671,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "wQi7bUIQUmAr",
    "outputId": "10878a69-6039-4686-e83c-beead081416d"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# par√°grafo git: inicializa√ß√£o do reposit√≥rio no drive e push inicial para o github\n",
    "\n",
    "# imports\n",
    "from pathlib import Path\n",
    "import subprocess, os, sys, getpass, textwrap\n",
    "\n",
    "# util de shell\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout, r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "# garantir que o diret√≥rio do projeto exista\n",
    "repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# montar drive no colab se necess√°rio\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive  # type: ignore\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        print(\"montando google drive‚Ä¶\")\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# configurar safe.directory para evitar avisos do git com caminhos de rede\n",
    "try:\n",
    "    sh([\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", str(repo_dir)])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# inicializar reposit√≥rio se ainda n√£o existir\n",
    "if not (repo_dir / \".git\").exists():\n",
    "    print(\"inicializando reposit√≥rio git‚Ä¶\")\n",
    "    sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "    # garantir branch principal como main (compat√≠vel com vers√µes antigas)\n",
    "    try:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"branch\", \"-M\", default_branch], cwd=repo_dir)\n",
    "else:\n",
    "    print(\".git j√° existe; seguindo\")\n",
    "\n",
    "# configurar identidade local\n",
    "sh([\"git\", \"config\", \"user.name\", author_name], cwd=repo_dir)\n",
    "sh([\"git\", \"config\", \"user.email\", author_email], cwd=repo_dir)\n",
    "\n",
    "# criar .gitignore b√°sico e readme se estiverem ausentes\n",
    "gitignore_path = repo_dir / \".gitignore\"\n",
    "if not gitignore_path.exists():\n",
    "    gitignore_path.write_text(textwrap.dedent(\"\"\"\n",
    "      # python\n",
    "      __pycache__/\n",
    "      *.py[cod]\n",
    "      *.egg-info/\n",
    "      .venv*/\n",
    "      venv/\n",
    "\n",
    "      # segredos\n",
    "      .env\n",
    "      *.key\n",
    "      *.pem\n",
    "      *.tok\n",
    "\n",
    "      # jupyter/colab\n",
    "      .ipynb_checkpoints/\n",
    "\n",
    "      # artefatos e dados locais (n√£o versionar)\n",
    "      data/\n",
    "      input/                 # inclui input.csv sens√≠vel\n",
    "      output/\n",
    "      runs/\n",
    "      logs/\n",
    "      figures/\n",
    "      *.log\n",
    "      *.tmp\n",
    "      *.bak\n",
    "      *.png\n",
    "      *.jpg\n",
    "      *.pdf\n",
    "      *.html\n",
    "\n",
    "      # allowlist para a pasta de refer√™ncias\n",
    "      !references/\n",
    "      !references/**\n",
    "    \"\"\").strip() + \"\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado .gitignore\")\n",
    "\n",
    "readme_path = repo_dir / \"README.md\"\n",
    "if not readme_path.exists():\n",
    "    readme_path.write_text(f\"# {repo_name}\\n\\nprojeto de autoencoder tabular para journal entries.\\n\", encoding=\"utf-8\")\n",
    "    print(\"criado README.md\")\n",
    "\n",
    "# configurar remoto origin\n",
    "remote_base = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "existing_remotes = sh([\"git\", \"remote\"], cwd=repo_dir)\n",
    "if \"origin\" not in existing_remotes.split():\n",
    "    sh([\"git\", \"remote\", \"add\", \"origin\", remote_base], cwd=repo_dir)\n",
    "    print(f\"remoto origin adicionado: {remote_base}\")\n",
    "else:\n",
    "    # se j√° existe, garantir que aponta para o repo correto\n",
    "    current_url = sh([\"git\", \"remote\", \"get-url\", \"origin\"], cwd=repo_dir)\n",
    "    if current_url != remote_base:\n",
    "        sh([\"git\", \"remote\", \"set-url\", \"origin\", remote_base], cwd=repo_dir)\n",
    "        print(f\"remoto origin atualizado para: {remote_base}\")\n",
    "    else:\n",
    "        print(\"remoto origin j√° configurado corretamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-aIMDUNNhfi"
   },
   "source": [
    "##**Utilit√°rio:** verifica√ß√£o da formata√ß√£o de c√≥digo\n",
    "\n",
    "Black [88] + Isort, desconsiderando c√©lulas m√°gicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Jfa9X-d-Kfdn"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0001\n",
    "#pr√©-visualizar/aplicar (pula magics) ‚Äî isort(profile=black)+black(88) { display-mode: \"form\" }\n",
    "import sys, subprocess, os, re, difflib, textwrap, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===== CONFIG =====\n",
    "NOTEBOOK = \"/content/drive/MyDrive/Notebooks/data-analysis/notebooks/AETabular_main.ipynb\"  # <- ajuste\n",
    "LINE_LENGTH = 88\n",
    "# ==================\n",
    "\n",
    "# 1) Instalar libs no MESMO Python do kernel\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"black\", \"isort\", \"nbformat\"])\n",
    "\n",
    "import nbformat\n",
    "import black\n",
    "import isort\n",
    "\n",
    "BLACK_MODE = black.Mode(line_length=LINE_LENGTH)\n",
    "ISORT_CFG  = isort.Config(profile=\"black\", line_length=LINE_LENGTH)\n",
    "\n",
    "# 2) Regras para pular c√©lulas com magics/shell\n",
    "#   - linhas come√ßando com %, %%, !\n",
    "#   - chamadas a get_ipython(\n",
    "MAGIC_LINE = re.compile(r\"^\\s*(%{1,2}|!)\", re.M)\n",
    "GET_IPY    = re.compile(r\"get_ipython\\s*\\(\")\n",
    "\n",
    "def has_magics(code: str) -> bool:\n",
    "    return bool(MAGIC_LINE.search(code) or GET_IPY.search(code))\n",
    "\n",
    "def format_code(code: str) -> str:\n",
    "    # isort primeiro, depois black\n",
    "    sorted_code = isort.api.sort_code_string(code, config=ISORT_CFG)\n",
    "    return black.format_str(sorted_code, mode=BLACK_MODE)\n",
    "\n",
    "def summarize_diff(diff_lines: List[str]) -> Tuple[int, int]:\n",
    "    added = removed = 0\n",
    "    for ln in diff_lines:\n",
    "        # ignorar cabe√ßalhos do diff\n",
    "        if ln.startswith((\"---\", \"+++\", \"@@\")):\n",
    "            continue\n",
    "        if ln.startswith(\"+\"):\n",
    "            added += 1\n",
    "        elif ln.startswith(\"-\"):\n",
    "            removed += 1\n",
    "    return added, removed\n",
    "\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(title)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "if not os.path.exists(NOTEBOOK):\n",
    "    raise FileNotFoundError(f\"Notebook n√£o encontrado:\\n{NOTEBOOK}\")\n",
    "\n",
    "# 3) Leitura do .ipynb\n",
    "with open(NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "changed_cells = []  # (idx, added, removed, diff_text, preview_snippet, new_code)\n",
    "\n",
    "# 4) Pr√©-visualiza√ß√£o c√©lula a c√©lula\n",
    "header(\"Pr√©-visualiza√ß√£o (N√ÉO grava) ‚Äî somente c√©lulas com mudan√ßas\")\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "\n",
    "    original = cell.get(\"source\", \"\")\n",
    "    if not original.strip():\n",
    "        continue\n",
    "\n",
    "    # Pular c√©lulas com magics/shell\n",
    "    if has_magics(original):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formatted = format_code(original)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] c√©lula {i}: erro no formatador ‚Äî pulando ({e})\")\n",
    "        continue\n",
    "\n",
    "    if original.strip() != formatted.strip():\n",
    "        # Gerar diff unificado leg√≠vel\n",
    "        diff = list(difflib.unified_diff(\n",
    "            original.splitlines(), formatted.splitlines(),\n",
    "            fromfile=f\"cell_{i}:before\", tofile=f\"cell_{i}:after\", lineterm=\"\"\n",
    "        ))\n",
    "        add, rem = summarize_diff(diff)\n",
    "        snippet = original.strip().splitlines()[0][:120] if original.strip().splitlines() else \"<c√©lula vazia>\"\n",
    "        changed_cells.append((i, add, rem, \"\\n\".join(diff), snippet, formatted))\n",
    "\n",
    "# 5) Exibi√ß√£o dos diffs por c√©lula (se houver)\n",
    "if not changed_cells:\n",
    "    print(\"‚úî Nada a alterar: todas as c√©lulas (n√£o m√°gicas) j√° est√£o conforme isort/black.\")\n",
    "else:\n",
    "    total_add = total_rem = 0\n",
    "    for (idx, add, rem, diff_text, snippet, _new) in changed_cells:\n",
    "        total_add += add\n",
    "        total_rem += rem\n",
    "        header(f\"Diff ‚Äî C√©lula #{idx}  (+{add}/-{rem})\")\n",
    "        print(f\"Primeira linha da c√©lula: {snippet!r}\\n\")\n",
    "        print(diff_text)\n",
    "\n",
    "    header(\"Resumo\")\n",
    "    print(f\"C√©lulas com mudan√ßas: {len(changed_cells)}\")\n",
    "    print(f\"Linhas adicionadas:   {total_add}\")\n",
    "    print(f\"Linhas removidas:     {total_rem}\")\n",
    "\n",
    "# 6) Perguntar se aplica\n",
    "if changed_cells:\n",
    "    print(\"\\nDigite 'p' para **Proceder** e gravar as mudan√ßas nessas c√©lulas, ou 'c' para **Cancelar**.\")\n",
    "    try:\n",
    "        choice = input(\"Proceder (p) / Cancelar (c): \").strip().lower()\n",
    "    except Exception:\n",
    "        choice = \"c\"\n",
    "\n",
    "    if choice == \"p\":\n",
    "        # Backup antes de escrever\n",
    "        backup = NOTEBOOK + \".bak\"\n",
    "        if not os.path.exists(backup):\n",
    "            with open(backup, \"w\", encoding=\"utf-8\") as bf:\n",
    "                nbformat.write(nb, bf)\n",
    "\n",
    "        # Aplicar somente nas c√©lulas com mudan√ßas\n",
    "        idx_to_new = {idx: new for (idx, _a, _r, _d, _s, new) in changed_cells}\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if i in idx_to_new and cell.get(\"cell_type\") == \"code\":\n",
    "                cell[\"source\"] = idx_to_new[i]\n",
    "\n",
    "        # Escrever no .ipynb\n",
    "        with open(NOTEBOOK, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "\n",
    "        # Sync delay (Drive)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        header(\"Conclu√≠do\")\n",
    "        print(f\"‚úî Mudan√ßas aplicadas em {len(changed_cells)} c√©lula(s).\")\n",
    "        print(f\"Backup criado em: {backup}\")\n",
    "        print(\"Dica: recarregue o notebook no Colab para ver a formata√ß√£o atualizada.\")\n",
    "    else:\n",
    "        print(\"\\nOpera√ß√£o cancelada. Nada foi gravado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwzVcwk9Ol0K"
   },
   "source": [
    "##**Sincronizar altera√ß√µes no c√≥digo do projeto**\n",
    "Comandos para sincronizar c√≥digo (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive √© considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16371,
     "status": "ok",
     "timestamp": 1760321482679,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "2hJZaAa2OqEp",
    "outputId": "dced9304-82d5-41e0-baf7-8fde04ec8676"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive √© a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de vers√£o (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; s√≥ 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_AETABULAR (fallback: env; √∫ltimo caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#utilit√°rios silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heur√≠stica curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permiss√µes/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local inv√°lido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#configura√ß√µes do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do reposit√≥rio no GitHub\n",
    "repo_name      = \"ae-tabular\"    # nome do reposit√≥rio\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive n√£o montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se √© um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta opera√ß√µes pendentes (n√£o apaga hist√≥rico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #for√ßa branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diret√≥rio como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necess√°rio)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normaliza√ß√£o\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos j√° rastreados que hoje s√£o ignorados pelo .gitignore,\n",
    "    limpa o √≠ndice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contr√°rio.\n",
    "    \"\"\"\n",
    "    #remove lock de √≠ndice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o √≠ndice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que est√£o rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detec√ß√£o? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de vers√£o\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato leg√≠vel + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autentica√ß√£o (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_AETABULAR')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - vari√°vel de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_AETABULAR\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#listas de for√ßa\n",
    "FORCE_UNTRACK = [\"input/\", \"output/\", \"data/\", \"runs/\", \"logs/\", \"figures/\"]\n",
    "FORCE_TRACK   = [\"references/\"]  #versionar tudo dentro (PDFs inclusive)\n",
    "\n",
    "def force_index_rules():\n",
    "    #garante que pastas sens√≠veis NUNCA fiquem rastreadas\n",
    "    for p in FORCE_UNTRACK:\n",
    "        try:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "    #garante que references/ SEMPRE entre (√∫til se ainda h√° *.pdf globais)\n",
    "    for p in FORCE_TRACK:\n",
    "        try:\n",
    "            git(\"add\", \"-f\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de vers√£o (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudan√ßa: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #vers√£o\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma √∫nica vez, se necess√°rio)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #aplica regras de for√ßa\n",
    "        force_index_rules()\n",
    "\n",
    "        #stage de tudo (Drive √© a verdade; remo√ß√µes entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se s√≥ a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se j√° existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} ‚Äî {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} ‚Äî {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive √© a verdade): valida√ß√£o + push for√ßado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # voc√™ √© o owner; n√£o perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se n√£o existir (repo vazio), faz um probe gen√©rico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #reposit√≥rio pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push for√ßado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. Vers√£o atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem √∫nica, curta, sem detalhes sens√≠veis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt9CwUvIWJ4-"
   },
   "source": [
    "#**Projeto**\n",
    "Comandos para sincronizar c√≥digo (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive √© considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOA1JgyIPBYj"
   },
   "source": [
    "## **Etapa 1:** Ativa√ß√£o do ambiente virtual\n",
    "---\n",
    "Monta o Google Drive, define a BASE e REPO do projeto Git, cria/ativa o ambiente virtual.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "6rreQXmSPFlE"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0003\n",
    "#inicializa√ß√£o robusta: Drive + venv fora do Drive + Git checks (com patch de venv/ensurepip) { display-mode: \"form\" }\n",
    "#for√ßa clear do kernel/vari√°veis desta sess√£o\n",
    "%reset -f\n",
    "\n",
    "#imports b√°sicos -----\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML\n",
    "import json, os, sys, time, shutil, pathlib, subprocess\n",
    "\n",
    "#helper de subprocess -----\n",
    "def run(cmd, check=True, cwd=None):\n",
    "    r = subprocess.run(cmd, text=True, capture_output=True, cwd=cwd)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout + r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "#fun√ß√µes utilit√°rias de Drive/FS -----\n",
    "def _is_mount_active(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"verifica em /proc/mounts se o mountpoint est√° realmente montado\"\"\"\n",
    "    try:\n",
    "        with open(\"/proc/mounts\", \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if len(parts) > 1 and parts[1] == mountpoint:\n",
    "                    return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def _cleanup_local_mountpoint(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"limpa conte√∫do local do mountpoint quando N√ÉO est√° montado\"\"\"\n",
    "    if os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
    "        print(f\"[info] mountpoint '{mountpoint}' cont√©m arquivos locais. limpando...\")\n",
    "        for name in os.listdir(mountpoint):\n",
    "            p = os.path.join(mountpoint, name)\n",
    "            try:\n",
    "                if os.path.isfile(p) or os.path.islink(p):\n",
    "                    os.remove(p)\n",
    "                else:\n",
    "                    shutil.rmtree(p)\n",
    "            except Exception as e:\n",
    "                print(f\"[aviso] n√£o foi poss√≠vel remover {p}: {e}\")\n",
    "        print(\"[ok] limpeza conclu√≠da.\")\n",
    "\n",
    "def safe_mount_google_drive(preferred_mountpoint: str = \"/content/drive\", readonly: bool = False, timeout_ms: int = 120000):\n",
    "    \"\"\"desmonta se preciso, limpa o mountpoint local e monta o drive\"\"\"\n",
    "    try:\n",
    "        if _is_mount_active(preferred_mountpoint):\n",
    "          # print(\"[info] drive montado. tentando desmontar...\")\n",
    "          drive.flush_and_unmount()\n",
    "          for _ in range(50):\n",
    "              if not _is_mount_active(preferred_mountpoint):\n",
    "                  break\n",
    "              time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not _is_mount_active(preferred_mountpoint):\n",
    "        _cleanup_local_mountpoint(preferred_mountpoint)\n",
    "\n",
    "    os.makedirs(preferred_mountpoint, exist_ok=True)\n",
    "    if os.listdir(preferred_mountpoint):\n",
    "        alt = \"/mnt/drive\"\n",
    "        print(f\"[aviso] '{preferred_mountpoint}' ainda n√£o est√° vazio. usando alternativo '{alt}'.\")\n",
    "        os.makedirs(alt, exist_ok=True)\n",
    "        mountpoint = alt\n",
    "    else:\n",
    "        mountpoint = preferred_mountpoint\n",
    "\n",
    "    print(f\"[info] montando o google drive em '{mountpoint}'...\")\n",
    "    drive.mount(mountpoint, force_remount=True, timeout_ms=timeout_ms, readonly=readonly)\n",
    "    print(\"[ok]   drive montado com sucesso.\")\n",
    "    return mountpoint\n",
    "\n",
    "def safe_chdir(path):\n",
    "    \"\"\"usa os.chdir com valida√ß√µes, evitando %cd\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"caminho n√£o existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"[ok]   diret√≥rio atual:\", os.getcwd())\n",
    "\n",
    "#par√¢metros do projeto -----\n",
    "GITHUB_OWNER = \"LeoBR84p\"\n",
    "GITHUB_REPO  = \"ae-tabular\"\n",
    "CLEAN_URL    = f\"https://github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "#montar/remontar o google drive (robusto)\n",
    "MOUNTPOINT = safe_mount_google_drive(\"/content/drive\")\n",
    "BASE = f\"{MOUNTPOINT}/MyDrive/Notebooks\"  #ajuste se quiser\n",
    "REPO = \"ae-tabular\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "#venv fora do drive (mais r√°pido e evita sync)\n",
    "VENV_PATH = \"/content/.venv_data\"\n",
    "VENV_BIN  = f\"{VENV_PATH}/bin\"\n",
    "VENV_PY   = f\"{VENV_BIN}/python\"\n",
    "VENV_PIP  = f\"{VENV_BIN}/pip\"   #pode n√£o existir ainda se o venv foi criado sem pip\n",
    "\n",
    "#cria√ß√£o do venv com fallback para 'virtualenv'\n",
    "def create_or_repair_venv(venv_path: str, venv_python: str):\n",
    "    if not os.path.exists(VENV_BIN):\n",
    "        #print(f\"[info] criando venv (stdlib) em {venv_path} --without-pip ...\")\n",
    "        try:\n",
    "            run([sys.executable, \"-m\", \"venv\", \"--without-pip\", venv_path], check=True)\n",
    "            print(\"[ok]   venv criado (sem pip).\")\n",
    "        except Exception as e:\n",
    "            print(f\"[aviso] venv(stdlib) falhou: {e}\")\n",
    "            #print(\"[info] instalando 'virtualenv' e criando venv alternativo com pip embutido...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", sys.executable, venv_path], check=True)\n",
    "            print(\"[ok]   venv criado via virtualenv.\")\n",
    "    else:\n",
    "        print(f\"[ok]   venv j√° existe em {venv_path}\")\n",
    "\n",
    "create_or_repair_venv(VENV_PATH, VENV_PY)\n",
    "\n",
    "#ajusta PATH antes de qualquer instala√ß√£o\n",
    "os.environ[\"PATH\"] = f\"{VENV_BIN}{os.pathsep}{os.environ['PATH']}\"\n",
    "os.environ[\"VIRTUAL_ENV\"] = VENV_PATH\n",
    "print(\"[ok]   venv adicionado ao PATH\")\n",
    "\n",
    "#garante pip dentro do venv (ensurepip -> fallback virtualenv)\n",
    "def _ensure_pip_in_venv(vpy: str):\n",
    "    try:\n",
    "        run([vpy, \"-m\", \"pip\", \"--version\"], check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        #print(\"[info] pip ausente no venv. tentando ensurepip dentro do venv...\")\n",
    "        try:\n",
    "            run([vpy, \"-m\", \"ensurepip\", \"--upgrade\", \"--default-pip\"], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            #print(f\"[aviso] ensurepip no venv falhou: {e}\")\n",
    "            #print(\"[info] fallback: usando virtualenv para semear o pip dentro do venv existente...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", vpy, VENV_PATH], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "\n",
    "if not _ensure_pip_in_venv(VENV_PY):\n",
    "    raise RuntimeError(\"n√£o foi poss√≠vel provisionar o pip dentro do venv\")\n",
    "\n",
    "# garante que os pacotes instalados no venv sejam vis√≠veis para este kernel\n",
    "_ver = subprocess.check_output([VENV_PY, \"-c\", \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\"], text=True).strip()\n",
    "_site_dir = f\"{VENV_PATH}/lib/python{_ver}/site-packages\"\n",
    "if _site_dir not in sys.path:\n",
    "    sys.path.insert(0, _site_dir)\n",
    "print(\"[ok]   site-packages do venv adicionado ao sys.path:\", _site_dir)\n",
    "\n",
    "#instala depend√™ncias de sess√£o DENTRO do venv\n",
    "print(\"[info] instalando pacotes no venv...\")\n",
    "run([VENV_PY, \"-m\", \"pip\", \"install\", \"-q\", \"jupytext\", \"nbdime\", \"nbstripout\"])\n",
    "\n",
    "#habilita integra√ß√£o do nbdime com git (global)\n",
    "print(\"[info] habilitando nbdime em git config --global ...\")\n",
    "run([VENV_PY, \"-m\", \"nbdime\", \"config-git\", \"--enable\", \"--global\"])\n",
    "\n",
    "#checks do reposit√≥rio git + navega√ß√£o at√© a pasta do projeto\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"[aviso] pasta do projeto n√£o encontrada em {PROJ}.\")\n",
    "else:\n",
    "    print(\"[ok]   pasta do projeto encontrada.\")\n",
    "    safe_chdir(PROJ)\n",
    "    if not os.path.isdir(\".git\"):\n",
    "        print(\"[aviso] esta pasta n√£o parece ser um reposit√≥rio Git (.git ausente).\")\n",
    "    else:\n",
    "        print(\"[ok]   reposit√≥rio Git detectado.\")\n",
    "\n",
    "# resumo do ambiente (confirma√ß√£o objetiva e detalhada)\n",
    "kernel_py = sys.executable\n",
    "venv_py = VENV_PY\n",
    "site_dir = _site_dir\n",
    "\n",
    "# verifica se o site-packages do venv est√° no sys.path\n",
    "site_ok = site_dir in sys.path\n",
    "\n",
    "# obt√©m vers√µes e caminhos\n",
    "try:\n",
    "    py_ver = subprocess.check_output([venv_py, \"-V\"], text=True).strip()\n",
    "    pip_ver = subprocess.check_output([venv_py, \"-m\", \"pip\", \"--version\"], text=True).strip()\n",
    "    pip_path = subprocess.check_output(\n",
    "        [venv_py, \"-m\", \"pip\", \"show\", \"pip\"], text=True, stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    pip_path_line = next((l for l in pip_path.splitlines() if l.startswith(\"Location:\")), \"\")\n",
    "except subprocess.CalledProcessError:\n",
    "    py_ver, pip_ver, pip_path_line = \"erro\", \"erro\", \"\"\n",
    "\n",
    "# imprime status linha a linha\n",
    "print(f\"[ok]   venv habilitado\" if venv_py else \"[erro] venv n√£o encontrado\")\n",
    "print(f\"[info] python em uso: {kernel_py}\")\n",
    "print(f\"[info] vers√£o do python: {py_ver}\")\n",
    "print(f\"[ok]   pip do venv ativo\" if \"pip\" in pip_ver.lower() else \"[erro] pip do venv n√£o detectado\")\n",
    "print(f\"[info] caminho do pip: {venv_py.replace('python','pip')}\")\n",
    "print(f\"[ok]   site-packages no sys.path: {site_dir}\" if site_ok else f\"[erro] site-packages ausente no sys.path: {site_dir}\")\n",
    "print(f\"[info] vers√£o do pip: {pip_ver}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: T-800 ativado. Diagn√≥stico do ambiente conclu√≠do. '\n",
    "             'üéØ Alvo principal: organiza√ß√£o do notebook e venv fora do drive.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXFHi64oRHjs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1760320462073,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "a0dv1bqORH_w",
    "outputId": "3077366d-5003-4032-d579-a755b99957e3"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID003\n",
    "# imports principais\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import json, os, getpass, platform, subprocess, sys\n",
    "\n",
    "# configura√ß√µes do projeto (j√° fornecidas)\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do reposit√≥rio no GitHub\n",
    "repo_name      = \"ae-tabular\"       # nome do reposit√≥rio\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/ae-tabular\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "# zona de tempo brasilia\n",
    "TZ_BR = ZoneInfo(\"America/Sao_Paulo\")\n",
    "\n",
    "def skynet(msg: str):\n",
    "    \"\"\"log simples padronizado\"\"\"\n",
    "    ts = datetime.now(TZ_BR).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[skynet {ts}] {msg}\")\n",
    "\n",
    "# 1) montar o google drive se necess√°rio\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive  # type: ignore\n",
    "    if not os.path.ismount(\"/content/drive\"):\n",
    "        skynet(\"montando google drive‚Ä¶\")\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "    else:\n",
    "        skynet(\"google drive j√° montado\")\n",
    "except Exception:\n",
    "    skynet(\"ambiente n√£o √© colab ou google drive indispon√≠vel, prosseguindo assim mesmo\")\n",
    "\n",
    "# 2) criar estrutura de pastas do projeto\n",
    "INPUT_DIR  = repo_dir / \"input\"\n",
    "OUTPUT_DIR = repo_dir / \"output\"\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 3) criar subpasta de execu√ß√£o em output com carimbo de data e hora de bras√≠lia\n",
    "run_stamp = datetime.now(TZ_BR).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "RUN_DIR = OUTPUT_DIR / run_stamp\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "# 4) opcional: escrever metadados m√≠nimos da execu√ß√£o\n",
    "run_meta = {\n",
    "    \"author_name\": author_name,\n",
    "    \"author_email\": author_email,\n",
    "    \"project\": repo_name,\n",
    "    \"run_stamp_br\": run_stamp,\n",
    "    \"timezone\": \"America/Sao_Paulo\",\n",
    "    \"python_version\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"uid\": getpass.getuser(),\n",
    "}\n",
    "with open(RUN_DIR / \"run.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 5) git opcional: configurar identidade local se este for um reposit√≥rio git\n",
    "def _git(cmd, cwd=None):\n",
    "    return subprocess.run([\"git\", *cmd], cwd=cwd, text=True, capture_output=True)\n",
    "\n",
    "if (repo_dir / \".git\").exists():\n",
    "    _git([\"config\", \"user.name\", author_name], cwd=repo_dir)\n",
    "    _git([\"config\", \"user.email\", author_email], cwd=repo_dir)\n",
    "    skynet(\"git detectado e identidade configurada no reposit√≥rio\")\n",
    "else:\n",
    "    skynet(\"reposit√≥rio git n√£o detectado em repo_dir, etapa git ser√° ignorada por enquanto\")\n",
    "\n",
    "# 6) impress√£o de caminhos √∫teis\n",
    "skynet(\"ambiente preparado com sucesso\")\n",
    "print(\"repo_dir:   \", repo_dir)\n",
    "print(\"input_dir:  \", INPUT_DIR)\n",
    "print(\"output_dir: \", OUTPUT_DIR)\n",
    "print(\"run_dir:    \", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1yqyFuEYPbL"
   },
   "source": [
    "## **Etapa 2:** Ingest√£o de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32304,
     "status": "ok",
     "timestamp": 1760322681145,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "lLmTZYkJZtEu",
    "outputId": "87a358d7-6613-4014-cfa3-7a4dcf93f838"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# par√°grafo 2: ingest√£o de journal entries (csv utf-8 bom ;), sele√ß√£o por √≠ndice e simula√ß√£o 50k\n",
    "\n",
    "# imports\n",
    "import os, sys, json, math, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# depend√™ncias do par√°grafo 1\n",
    "assert 'RUN_DIR' in globals() and 'repo_dir' in globals(), \"execute o par√°grafo 1 antes.\"\n",
    "assert 'INPUT_DIR' in globals() and 'OUTPUT_DIR' in globals(), \"execute o par√°grafo 1 antes.\"\n",
    "assert 'TZ_BR' in globals() and callable(skynet), \"execute o par√°grafo 1 antes.\"\n",
    "\n",
    "# par√¢metros de leitura obrigat√≥rios\n",
    "CSV_ENCODING = \"utf-8-sig\"   # utf-8 com bom\n",
    "CSV_SEP      = \";\"           # separador ponto e v√≠rgula\n",
    "\n",
    "# colunas esperadas\n",
    "REQUIRED_COLS = [\n",
    "    \"username\",        # nome do usu√°rio\n",
    "    \"lotacao\",         # unidade/lota√ß√£o\n",
    "    \"tipoconta\",       # Ativo, Passivo, Resultado, Receita, Despesa, Outras\n",
    "    \"valormi\",         # valor em reais com 2 casas decimais\n",
    "    \"dc\",              # 'd' para d√©bito, 'c' para cr√©dito\n",
    "    \"contacontabil\",   # conta cont√°bil COSIF (n√∫mero)\n",
    "]\n",
    "\n",
    "# categorias v√°lidas\n",
    "TIPOCONTA_VALIDAS = {\"Ativo\",\"Passivo\",\"Resultado\",\"Receita\",\"Despesa\",\"Outras\"}\n",
    "DC_VALIDOS        = {\"d\",\"c\"}\n",
    "\n",
    "# utilit√°rio: listar csvs no google drive do usu√°rio (mydrive), ordenados por data desc\n",
    "def listar_csvs_mydrive(max_files=2000):\n",
    "    raiz = Path(\"/content/drive/MyDrive\")\n",
    "    arquivos = []\n",
    "    for p in raiz.rglob(\"*.csv\"):\n",
    "        try:\n",
    "            stat = p.stat()\n",
    "            arquivos.append((p, stat.st_mtime, stat.st_size))\n",
    "            if len(arquivos) >= max_files:\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    # ordena por mtime desc\n",
    "    arquivos.sort(key=lambda x: x[1], reverse=True)\n",
    "    return arquivos\n",
    "\n",
    "# utilit√°rio: impress√£o amig√°vel de bytes\n",
    "def _fmt_size(n):\n",
    "    for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
    "        if n < 1024:\n",
    "            return f\"{n:.1f}{u}\"\n",
    "        n /= 1024\n",
    "    return f\"{n:.1f}PB\"\n",
    "\n",
    "# gera√ß√£o de dataset simulado (50k linhas) balanceado por partidas dobradas\n",
    "def gerar_csv_simulado_50k(dest_dir: Path) -> Path:\n",
    "    # mapeia tipoconta -> prefixo cosif plaus√≠vel\n",
    "    cosif_prefix = {\n",
    "        \"Ativo\": \"1\",\n",
    "        \"Passivo\": \"2\",\n",
    "        \"Resultado\": \"3\",\n",
    "        \"Receita\": \"7\",\n",
    "        \"Despesa\": \"8\",\n",
    "        \"Outras\": \"9\",\n",
    "    }\n",
    "    # listas de apoio\n",
    "    lotacoes = [f\"UNID-{i:03d}\" for i in range(1, 121)]\n",
    "    usuarios = [f\"user{i:04d}\" for i in range(1, 3001)]\n",
    "    tipos    = list(cosif_prefix.keys())\n",
    "    # n√∫mero de pares (d,c)\n",
    "    n_pairs = 25_000  # 50k linhas\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    rows = []\n",
    "    for _ in range(n_pairs):\n",
    "        # valor do par, duas casas\n",
    "        base_val = float(np.round(rng.uniform(10.0, 50_000.0), 2))\n",
    "        # pequena varia√ß√£o entre d e c para simular centavos e depois ajustar\n",
    "        d_val = float(np.round(base_val * rng.uniform(0.5, 1.5), 2))\n",
    "        c_val = d_val  # assegura partida dobrada perfeita\n",
    "\n",
    "        # escolhe atributos d√©bito\n",
    "        t_d  = random.choice(tipos)\n",
    "        lot_d = random.choice(lotacoes)\n",
    "        usr_d = random.choice(usuarios)\n",
    "        cc_d  = f\"{cosif_prefix[t_d]}{rng.integers(0, 10_000_000):07d}\"\n",
    "\n",
    "        # escolhe atributos cr√©dito\n",
    "        t_c  = random.choice(tipos)\n",
    "        lot_c = random.choice(lotacoes)\n",
    "        usr_c = random.choice(usuarios)\n",
    "        cc_c  = f\"{cosif_prefix[t_c]}{rng.integers(0, 10_000_000):07d}\"\n",
    "\n",
    "        rows.append((usr_d, lot_d, t_d, d_val, \"d\", cc_d))\n",
    "        rows.append((usr_c, lot_c, t_c, c_val, \"c\", cc_c))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=REQUIRED_COLS)\n",
    "\n",
    "    # garante duas casas decimais no export; manteremos float em mem√≥ria\n",
    "    stamp = datetime.now(TZ_BR).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = dest_dir / f\"journal_simulado_{stamp}.csv\"\n",
    "\n",
    "    # exporta com utf-8-sig e separador ';'\n",
    "    # formata valormi com duas casas decimais\n",
    "    df_fmt = df.copy()\n",
    "    df_fmt[\"valormi\"] = df_fmt[\"valormi\"].map(lambda x: f\"{x:.2f}\")\n",
    "    df_fmt.to_csv(out_path, index=False, sep=CSV_SEP, encoding=CSV_ENCODING)\n",
    "\n",
    "    return out_path\n",
    "\n",
    "# etapa a) gerar um csv simulado em INPUT_DIR para prot√≥tipo\n",
    "sim_path = gerar_csv_simulado_50k(INPUT_DIR)\n",
    "skynet(f\"csv simulado gerado: {sim_path}\")\n",
    "\n",
    "# etapa b) listar csvs do mydrive para sele√ß√£o por √≠ndice\n",
    "skynet(\"varrendo csvs em /content/drive/MyDrive (isso pode levar algum tempo em drives grandes)\")\n",
    "csvs = listar_csvs_mydrive(max_files=2000)\n",
    "\n",
    "# inclui o simulado rec√©m-gerado no topo da lista para facilitar\n",
    "csvs = [(sim_path, sim_path.stat().st_mtime, sim_path.stat().st_size)] + csvs\n",
    "\n",
    "# imprime lista\n",
    "print(\"\\narquivos csv encontrados (√≠ndice, tamanho, caminho):\")\n",
    "for i, (p, mtime, sz) in enumerate(csvs):\n",
    "    print(f\"[{i:03d}] {_fmt_size(sz):>8}  {str(p)}\")\n",
    "\n",
    "# sele√ß√£o por √≠ndice\n",
    "while True:\n",
    "    try:\n",
    "        sel = input(\"\\ndigite o √≠ndice do csv desejado e pressione enter: \").strip()\n",
    "        idx = int(sel)\n",
    "        assert 0 <= idx < len(csvs)\n",
    "        SELECTED_CSV = Path(csvs[idx][0])\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"√≠ndice inv√°lido. tente novamente. erro: {e}\")\n",
    "\n",
    "skynet(f\"arquivo selecionado: {SELECTED_CSV}\")\n",
    "\n",
    "# etapa c) validar formato (encoding/delimitador/colunas) e carregar dataframe\n",
    "def carregar_validar_csv(path: Path) -> pd.DataFrame:\n",
    "    # leitura com requisitos obrigat√≥rios\n",
    "    df = pd.read_csv(path, sep=CSV_SEP, encoding=CSV_ENCODING, dtype=str)\n",
    "    # remove espa√ßos nas colunas e valores\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # checa colunas requeridas\n",
    "    faltantes = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if faltantes:\n",
    "        raise ValueError(f\"colunas ausentes: {faltantes}. esperado: {REQUIRED_COLS}\")\n",
    "\n",
    "    # tipagem: valormi -> float com duas casas (ponto decimal)\n",
    "    # se vier com v√≠rgula decimal por engano, converte\n",
    "    def _parse_val(v):\n",
    "        v = (v or \"\").replace(\".\", \"\").replace(\",\", \".\") if isinstance(v, str) and v.count(\",\")==1 and v.count(\".\")>1 else v\n",
    "        v = (v or \"\").replace(\",\", \".\") if isinstance(v, str) else v\n",
    "        try:\n",
    "            return round(float(v), 2)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"valormi\"] = df[\"valormi\"].apply(_parse_val).astype(float)\n",
    "\n",
    "    # normaliza dc\n",
    "    df[\"dc\"] = df[\"dc\"].str.lower()\n",
    "    # normaliza tipoconta (capitaliza√ß√£o 1¬™ mai√∫scula)\n",
    "    df[\"tipoconta\"] = df[\"tipoconta\"].str.capitalize()\n",
    "\n",
    "    # valida√ß√µes de conte√∫do\n",
    "    problemas = {}\n",
    "    if not set(REQUIRED_COLS).issubset(df.columns):\n",
    "        problemas[\"colunas_invalidas\"] = list(set(REQUIRED_COLS) - set(df.columns))\n",
    "    if (~df[\"dc\"].isin(DC_VALIDOS)).any():\n",
    "        problemas[\"dc_invalido\"] = int((~df[\"dc\"].isin(DC_VALIDOS)).sum())\n",
    "    if (~df[\"tipoconta\"].isin(TIPOCONTA_VALIDAS)).any():\n",
    "        problemas[\"tipoconta_invalida\"] = int((~df[\"tipoconta\"].isin(TIPOCONTA_VALIDAS)).sum())\n",
    "    if df[\"valormi\"].isna().any():\n",
    "        problemas[\"valormi_na\"] = int(df[\"valormi\"].isna().sum())\n",
    "    if (df[\"valormi\"] < 0).any():\n",
    "        problemas[\"valormi_negativo\"] = int((df[\"valormi\"] < 0).sum())\n",
    "\n",
    "    # contacontabil: manter como string num√©rica\n",
    "    df[\"contacontabil\"] = df[\"contacontabil\"].astype(str)\n",
    "    if (~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).any():\n",
    "        problemas[\"contacontabil_nao_numerica\"] = int((~df[\"contacontabil\"].str.fullmatch(r\"\\d+\")).sum())\n",
    "\n",
    "    if problemas:\n",
    "        report_path = RUN_DIR / \"validacao_ingestao.json\"\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(problemas, f, ensure_ascii=False, indent=2)\n",
    "        raise ValueError(f\"falhas de valida√ß√£o encontradas. veja {report_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "DF_RAW = carregar_validar_csv(SELECTED_CSV)\n",
    "skynet(f\"csv carregado com sucesso: {len(DF_RAW):,} linhas\")\n",
    "\n",
    "# etapa d) persistir c√≥pia da fonte e snapshot parquet no run_dir\n",
    "# c√≥pia padronizada do csv selecionado para rastreabilidade\n",
    "src_copy = RUN_DIR / f\"selected_source.csv\"\n",
    "if SELECTED_CSV.resolve() != src_copy.resolve():\n",
    "    try:\n",
    "        # reexporta com formata√ß√£o padronizada garantida (utf-8-sig ; e valormi com 2 casas)\n",
    "        df_exp = DF_RAW.copy()\n",
    "        df_exp[\"valormi\"] = df_exp[\"valormi\"].map(lambda x: f\"{x:.2f}\")\n",
    "        df_exp.to_csv(src_copy, index=False, sep=CSV_SEP, encoding=CSV_ENCODING)\n",
    "        skynet(f\"fonte padronizada salva em {src_copy}\")\n",
    "    except Exception as e:\n",
    "        skynet(f\"aviso: n√£o foi poss√≠vel salvar c√≥pia padronizada do csv ({e})\")\n",
    "\n",
    "# salva snapshot parquet para processamento r√°pido nas pr√≥ximas fases\n",
    "snap_path = RUN_DIR / \"journal_entries.parquet\"\n",
    "DF_RAW.to_parquet(snap_path, index=False)\n",
    "skynet(f\"snapshot parquet salvo: {snap_path}\")\n",
    "\n",
    "# imprime um resumo inicial\n",
    "print(\"\\nvis√£o geral:\")\n",
    "print(DF_RAW.head(5))\n",
    "print(\"\\ncontagem por dc:\\n\", DF_RAW[\"dc\"].value_counts(dropna=False))\n",
    "print(\"\\ncontagem por tipoconta:\\n\", DF_RAW[\"tipoconta\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHnnmL-ebMGV"
   },
   "source": [
    "## **Etapa 3:** Limpeza, configura√ß√µes e split (train/val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3474,
     "status": "ok",
     "timestamp": 1760323381443,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "olv2piVPbMRF",
    "outputId": "e1b18211-4caf-4feb-a1b5-25b1493e7ec1"
   },
   "outputs": [],
   "source": [
    "# par√°grafo 3: limpeza, engenharia de features e split train/val\n",
    "\n",
    "import os, json, math, re, pickle, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# depend√™ncias do par√°grafo 1 e 2\n",
    "assert 'RUN_DIR' in globals() and 'snap_path' in globals() and 'skynet' in globals(), \"execute os par√°grafos 1 e 2 antes.\"\n",
    "assert Path(snap_path).exists(), \"snapshot parquet ausente. finalize o par√°grafo 2.\"\n",
    "\n",
    "# importa dados brutos do snapshot\n",
    "DF = pd.read_parquet(snap_path)\n",
    "\n",
    "# 1) limpeza b√°sica\n",
    "# remove espa√ßos extras novamente e normaliza casos (seguran√ßa)\n",
    "for c in DF.columns:\n",
    "    if DF[c].dtype == object:\n",
    "        DF[c] = DF[c].astype(str).str.strip()\n",
    "\n",
    "DF[\"dc\"] = DF[\"dc\"].str.lower().map({\"d\": \"d\", \"c\": \"c\"})  # garante dom√≠nio\n",
    "DF[\"tipoconta\"] = DF[\"tipoconta\"].str.capitalize()\n",
    "\n",
    "# garante num√©rico valormi (j√° veio como float do par√°grafo 2)\n",
    "DF[\"valormi\"] = DF[\"valormi\"].astype(float)\n",
    "\n",
    "# remove linhas com falhas cr√≠ticas\n",
    "mask_ok = (\n",
    "    DF[\"username\"].notna() &\n",
    "    DF[\"lotacao\"].notna() &\n",
    "    DF[\"tipoconta\"].notna() &\n",
    "    DF[\"dc\"].isin({\"d\",\"c\"}) &\n",
    "    DF[\"contacontabil\"].str.fullmatch(r\"\\d+\") &\n",
    "    DF[\"valormi\"].notna() & (DF[\"valormi\"] >= 0)\n",
    ")\n",
    "removed = (~mask_ok).sum()\n",
    "DF = DF.loc[mask_ok].reset_index(drop=True)\n",
    "skynet(f\"linhas removidas por valida√ß√£o adicional: {removed}\")\n",
    "\n",
    "# 2) features auxiliares\n",
    "# 2.1) decomposi√ß√µes COSIF (prefixos para hierarquia)\n",
    "DF[\"cosif_len\"] = DF[\"contacontabil\"].str.len().clip(upper=12).astype(int)\n",
    "DF[\"cosif_p1\"]  = DF[\"contacontabil\"].str[0]          # 1 d√≠gito\n",
    "DF[\"cosif_p2\"]  = DF[\"contacontabil\"].str[:2]         # 2 d√≠gitos\n",
    "DF[\"cosif_p3\"]  = DF[\"contacontabil\"].str[:3]         # 3 d√≠gitos\n",
    "\n",
    "# 2.2) transforma√ß√µes em valor\n",
    "DF[\"valormi_log1p\"] = np.log1p(DF[\"valormi\"])\n",
    "# (opcional) normaliza√ß√µes por grupo vir√£o como z-scores abaixo\n",
    "\n",
    "# 2.3) frequency encoding para categorias grandes\n",
    "def freq_encode(series: pd.Series) -> pd.Series:\n",
    "    freq = series.value_counts(dropna=False)\n",
    "    return series.map(freq).astype(float)\n",
    "\n",
    "DF[\"freq_username\"]      = freq_encode(DF[\"username\"])\n",
    "DF[\"freq_lotacao\"]       = freq_encode(DF[\"lotacao\"])\n",
    "DF[\"freq_contacontabil\"] = freq_encode(DF[\"contacontabil\"])\n",
    "DF[\"freq_cosif_p2\"]      = freq_encode(DF[\"cosif_p2\"])\n",
    "DF[\"freq_cosif_p3\"]      = freq_encode(DF[\"cosif_p3\"])\n",
    "\n",
    "# 2.4) estat√≠sticas por grupo (m√©dia, desvio e z-score de valormi)\n",
    "def add_group_stats(df: pd.DataFrame, key: str, val_col: str = \"valormi\"):\n",
    "    g = df.groupby(key)[val_col].agg([\"mean\",\"std\",\"median\"]).rename(\n",
    "        columns={\"mean\":f\"{key}_mean\", \"std\":f\"{key}_std\", \"median\":f\"{key}_median\"}\n",
    "    )\n",
    "    df = df.join(g, on=key)\n",
    "    # evita divis√£o por zero\n",
    "    df[f\"{key}_std\"] = df[f\"{key}_std\"].replace(0, np.nan)\n",
    "    df[f\"{key}_z\"]   = (df[val_col] - df[f\"{key}_mean\"]) / df[f\"{key}_std\"]\n",
    "    df[f\"{key}_mad\"] = (df[val_col] - df[f\"{key}_median\"]).abs()\n",
    "    return df\n",
    "\n",
    "for k in [\"username\", \"lotacao\", \"contacontabil\", \"cosif_p2\", \"cosif_p3\"]:\n",
    "    DF = add_group_stats(DF, k, \"valormi\")\n",
    "\n",
    "# 2.5) codifica√ß√£o de baixo cardinalidade\n",
    "# dc: bin√°ria (d=1, c=0); tipoconta: one-hot\n",
    "DF[\"dc_bin\"] = DF[\"dc\"].map({\"d\":1, \"c\":0}).astype(int)\n",
    "tipos = sorted(DF[\"tipoconta\"].dropna().unique().tolist())\n",
    "tipodummies = pd.get_dummies(DF[\"tipoconta\"], prefix=\"tipo\", dtype=int)\n",
    "DF = pd.concat([DF, tipodummies], axis=1)\n",
    "\n",
    "# 3) matriz de features final\n",
    "num_cols = [\n",
    "    \"valormi\", \"valormi_log1p\",\n",
    "    \"freq_username\", \"freq_lotacao\", \"freq_contacontabil\", \"freq_cosif_p2\", \"freq_cosif_p3\",\n",
    "    \"cosif_len\",\n",
    "    \"username_mean\",\"username_std\",\"username_z\",\"username_median\",\"username_mad\",\n",
    "    \"lotacao_mean\",\"lotacao_std\",\"lotacao_z\",\"lotacao_median\",\"lotacao_mad\",\n",
    "    \"contacontabil_mean\",\"contacontabil_std\",\"contacontabil_z\",\"contacontabil_median\",\"contacontabil_mad\",\n",
    "    \"cosif_p2_mean\",\"cosif_p2_std\",\"cosif_p2_z\",\"cosif_p2_median\",\"cosif_p2_mad\",\n",
    "    \"cosif_p3_mean\",\"cosif_p3_std\",\"cosif_p3_z\",\"cosif_p3_median\",\"cosif_p3_mad\",\n",
    "    \"dc_bin\",\n",
    "]\n",
    "oh_cols = tipodummies.columns.tolist()\n",
    "\n",
    "# garante exist√™ncia de todas as colunas numericas (caso algum grupo n√£o crie std etc.)\n",
    "for c in num_cols:\n",
    "    if c not in DF.columns:\n",
    "        DF[c] = np.nan\n",
    "\n",
    "FEATURE_COLS = num_cols + oh_cols\n",
    "DF_FE = DF[FEATURE_COLS].copy()\n",
    "\n",
    "# 4) imputa√ß√£o simples e padroniza√ß√£o\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler  = StandardScaler()\n",
    "\n",
    "X_imp = imputer.fit_transform(DF_FE.values)\n",
    "X_std = scaler.fit_transform(X_imp)\n",
    "\n",
    "# 5) split n√£o supervisionado (aleat√≥rio reprodut√≠vel)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val = train_test_split(X_std, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "skynet(f\"features geradas: {DF_FE.shape[1]} colunas; amostras: {DF_FE.shape[0]:,}\")\n",
    "skynet(f\"split realizado: train={X_train.shape[0]:,} | val={X_val.shape[0]:,}\")\n",
    "\n",
    "# 6) persist√™ncia de artefatos para as pr√≥ximas fases\n",
    "artifacts = {\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"tipos\": tipos,\n",
    "    \"imputer\": imputer,\n",
    "    \"scaler\": scaler,\n",
    "}\n",
    "with open(RUN_DIR / \"features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "\n",
    "np.savez(RUN_DIR / \"dataset_npz.npz\", X_train=X_train, X_val=X_val)\n",
    "\n",
    "# 7) diagn√≥stico r√°pido\n",
    "summary = {\n",
    "    \"n_rows\": int(DF_FE.shape[0]),\n",
    "    \"n_features\": int(DF_FE.shape[1]),\n",
    "    \"train_rows\": int(X_train.shape[0]),\n",
    "    \"val_rows\": int(X_val.shape[0]),\n",
    "    \"one_hot_tipoconta_cols\": oh_cols,\n",
    "}\n",
    "with open(RUN_DIR / \"features_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"amostra das primeiras 5 linhas de features padronizadas (numpy):\")\n",
    "print(X_std[:5])\n",
    "\n",
    "skynet(f\"artefatos salvos em {RUN_DIR}: features.pkl e dataset_npz.npz\")\n",
    "\n",
    "# complemento do par√°grafo 3: estat√≠sticas por grupo e total de linhas removidas\n",
    "\n",
    "# imprime quantas linhas foram removidas por falhas cr√≠ticas na valida√ß√£o\n",
    "print(f\"\\nlinhas removidas por falhas cr√≠ticas (par√°grafo 3): {removed}\")\n",
    "\n",
    "# utilit√°rio para exibir dataframes no colab/notebook\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = print  # fallback simples\n",
    "\n",
    "# fun√ß√£o de resumo por grupo e identifica√ß√£o de outliers via |z|\n",
    "def resumo_por_grupo(df: pd.DataFrame, key: str, top_n: int = 10):\n",
    "    # resumo estat√≠stico por grupo (valormi)\n",
    "    agg = (\n",
    "        df.groupby(key)[\"valormi\"]\n",
    "          .agg(n=\"size\", media=\"mean\", desvio=\"std\", mediana=\"median\", minimo=\"min\", maximo=\"max\")\n",
    "          .sort_values(\"n\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== resumo por grupo: {key} (top {top_n} por contagem) ===\")\n",
    "    display(agg.head(top_n))\n",
    "\n",
    "    # salva o resumo completo em csv (utf-8-sig ; )\n",
    "    resumo_path = RUN_DIR / f\"resumo_{key}.csv\"\n",
    "    agg.to_csv(resumo_path, sep=\";\", encoding=\"utf-8-sig\")\n",
    "    skynet(f\"resumo por grupo '{key}' salvo em {resumo_path}\")\n",
    "\n",
    "    # top outliers por |z| no grupo, usando as colunas j√° criadas no par√°grafo 3\n",
    "    zcol = f\"{key}_z\"\n",
    "    mean_col = f\"{key}_mean\"\n",
    "    std_col  = f\"{key}_std\"\n",
    "\n",
    "    if zcol in df.columns and mean_col in df.columns and std_col in df.columns:\n",
    "        out = (\n",
    "            df[[key, \"valormi\", zcol, mean_col, std_col]]\n",
    "            .dropna(subset=[zcol])\n",
    "            .assign(abs_z=lambda x: x[zcol].abs())\n",
    "            .sort_values(\"abs_z\", ascending=False)\n",
    "            .head(top_n)\n",
    "        )\n",
    "        print(f\"\\n--- top {top_n} potenciais outliers por |z| em {key} ---\")\n",
    "        display(out)\n",
    "\n",
    "        out_path = RUN_DIR / f\"outliers_{key}.csv\"\n",
    "        out.to_csv(out_path, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "        skynet(f\"outliers por |z| '{key}' salvos em {out_path}\")\n",
    "    else:\n",
    "        print(f\"(aviso) colunas de z-score n√£o encontradas para '{key}' ‚Äî verifique a etapa de features.\")\n",
    "\n",
    "# executa para os principais agrupamentos cont√°beis\n",
    "for k in [\"username\", \"lotacao\", \"contacontabil\", \"cosif_p2\", \"cosif_p3\"]:\n",
    "    resumo_por_grupo(DF, k, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1noFN5QdMLP"
   },
   "source": [
    "# **Etapa 4:** Autoencoder utilizando treino com early-stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213805,
     "status": "ok",
     "timestamp": 1760323743280,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "A9lcTuoIdVcG",
    "outputId": "c35e22b1-9b3e-47c6-f019-7fadf6905033"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# par√°grafo 4: autoencoder denso (pytorch) com early-stopping e artefatos\n",
    "\n",
    "# imports\n",
    "import os, json, math, time, random, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# depend√™ncias anteriores\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os par√°grafos anteriores.\"\n",
    "ds_path = RUN_DIR / \"dataset_npz.npz\"\n",
    "assert ds_path.exists(), \"dataset_npz.npz ausente ‚Äî finalize o par√°grafo 3.\"\n",
    "with np.load(ds_path) as npz:\n",
    "    X_train = npz[\"X_train\"].astype(np.float32)\n",
    "    X_val   = npz[\"X_val\"].astype(np.float32)\n",
    "\n",
    "# tenta importar torch\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"pytorch n√£o dispon√≠vel. instale no colab com: pip install torch --quiet\") from e\n",
    "\n",
    "# dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "skynet(f\"treino em dispositivo: {device}\")\n",
    "\n",
    "# reprodutibilidade\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# dataset e dataloaders\n",
    "BATCH_SIZE = 1024\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(X_train))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(X_val))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# configura√ß√£o do modelo\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [256, 128, 64]  # voc√™ pode ajustar\n",
    "dropout_p = 0.05\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, h_dims, dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc = []\n",
    "        last = in_dim\n",
    "        for h in h_dims:\n",
    "            enc += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "\n",
    "        dec = []\n",
    "        rev = list(reversed(h_dims))\n",
    "        last = rev[0]\n",
    "        for h in rev[1:]:\n",
    "            dec += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        dec += [nn.Linear(last, in_dim)]  # camada de sa√≠da linear\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z)\n",
    "        return y\n",
    "\n",
    "model = AutoEncoder(input_dim, hidden_dims, dropout=dropout_p).to(device)\n",
    "skynet(f\"modelo criado: input_dim={input_dim}, hidden={hidden_dims}, dropout={dropout_p}\")\n",
    "\n",
    "# otimizador e perda\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# early-stopping\n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE   = 15\n",
    "MIN_DELTA  = 1e-5\n",
    "\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "best_val = float(\"inf\")\n",
    "best_epoch = -1\n",
    "no_improve = 0\n",
    "best_path = RUN_DIR / \"ae.pt\"\n",
    "\n",
    "def epoch_pass(dataloader, train: bool):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            yhat = model(xb)\n",
    "            loss = criterion(yhat, yb)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            count += xb.size(0)\n",
    "    return total_loss / max(count, 1)\n",
    "\n",
    "skynet(\"iniciando treino do autoencoder\")\n",
    "t0 = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    tr_loss = epoch_pass(train_dl, train=True)\n",
    "    vl_loss = epoch_pass(val_dl, train=False)\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"val_loss\"].append(vl_loss)\n",
    "\n",
    "    # logs ocasionais\n",
    "    if epoch == 1 or epoch % 5 == 0:\n",
    "        skynet(f\"epoch {epoch:03d}  train={tr_loss:.6f}  val={vl_loss:.6f}\")\n",
    "\n",
    "    # early-stopping\n",
    "    if vl_loss + MIN_DELTA < best_val:\n",
    "        best_val = vl_loss\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "        torch.save({\"model_state\": model.state_dict(),\n",
    "                    \"input_dim\": input_dim,\n",
    "                    \"hidden_dims\": hidden_dims,\n",
    "                    \"dropout_p\": dropout_p}, best_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            skynet(f\"early-stopping em epoch {epoch} (sem melhoria por {PATIENCE} √©pocas)\")\n",
    "            break\n",
    "\n",
    "t1 = time.time()\n",
    "skynet(f\"treino finalizado em {(t1 - t0):.1f}s; melhor √©poca={best_epoch} val_loss={best_val:.6f}\")\n",
    "assert best_path.exists(), \"modelo n√£o foi salvo ‚Äî verifique o treino.\"\n",
    "\n",
    "# salva hist√≥rico\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_csv = RUN_DIR / \"training_history.csv\"\n",
    "hist_df.to_csv(hist_csv, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "skynet(f\"hist√≥rico de treino salvo em {hist_csv}\")\n",
    "\n",
    "# gr√°fico das perdas\n",
    "plt.figure(figsize=(7,4.2))\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"train\")\n",
    "plt.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"],   label=\"val\")\n",
    "plt.xlabel(\"√©poca\")\n",
    "plt.ylabel(\"mse\")\n",
    "plt.title(\"autoencoder: curva de perdas\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plot_path = RUN_DIR / \"loss_plot.png\"\n",
    "plt.savefig(plot_path, dpi=140, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "skynet(f\"gr√°fico de perdas salvo em {plot_path}\")\n",
    "\n",
    "# carrega melhor estado e calcula erros de reconstru√ß√£o no conjunto de valida√ß√£o\n",
    "chk = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(chk[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_tensor = torch.from_numpy(X_val).to(device)\n",
    "    recon_val  = model(val_tensor).cpu().numpy()\n",
    "val_err = np.mean((recon_val - X_val) ** 2, axis=1).astype(np.float32)\n",
    "\n",
    "# salva distribui√ß√£o de erros e percentis para apoio a threshold\n",
    "err_path = RUN_DIR / \"reconstruction_errors_val.npy\"\n",
    "np.save(err_path, val_err)\n",
    "\n",
    "percentis = [50, 75, 90, 95, 97, 99, 99.5, 99.9]\n",
    "thr = {f\"p{p}\": float(np.percentile(val_err, p)) for p in percentis}\n",
    "thr[\"mean\"] = float(np.mean(val_err))\n",
    "thr[\"std\"]  = float(np.std(val_err))\n",
    "thr[\"suggested_threshold\"] = thr[\"p99.5\"]  # sugest√£o inicial (ajuste conforme sua capacidade operacional)\n",
    "\n",
    "thr_path = RUN_DIR / \"thresholds.json\"\n",
    "with open(thr_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(thr, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "skynet(f\"erros de valida√ß√£o salvos em {err_path}\")\n",
    "skynet(f\"thresholds salvos em {thr_path}\")\n",
    "print(\"\\nresumo thresholds (val):\")\n",
    "print(pd.Series(thr))\n",
    "\n",
    "# salva configura√ß√£o do modelo\n",
    "cfg = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dims\": hidden_dims,\n",
    "    \"dropout_p\": dropout_p,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LR,\n",
    "    \"max_epochs\": MAX_EPOCHS,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"min_delta\": MIN_DELTA,\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_val_loss\": best_val,\n",
    "    \"device\": str(device),\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "with open(RUN_DIR / \"model_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "skynet(\"artefatos do modelo escritos em disco: ae.pt, training_history.csv, loss_plot.png, thresholds.json, model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEdTKvXDf-PP"
   },
   "source": [
    "## **Etapa 6:** Calibra√ß√£o e metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1709,
     "status": "ok",
     "timestamp": 1760324524434,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "BqM0y_P2gE8F",
    "outputId": "9c671c93-41a0-4ae6-a85f-3f453187aba8"
   },
   "outputs": [],
   "source": [
    "# par√°grafo 6: calibra√ß√£o do limiar (threshold) e relat√≥rios de custo / alertas\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, json, math\n",
    "from pathlib import Path\n",
    "\n",
    "# depend√™ncias\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os par√°grafos anteriores.\"\n",
    "thr_path = RUN_DIR / \"thresholds.json\"\n",
    "err_path = RUN_DIR / \"reconstruction_errors_val.npy\"\n",
    "assert thr_path.exists() and err_path.exists(), \"execute o par√°grafo 4 antes.\"\n",
    "\n",
    "# carrega erros de reconstru√ß√£o do conjunto de valida√ß√£o\n",
    "val_err = np.load(err_path)\n",
    "with open(thr_path, encoding=\"utf-8\") as f:\n",
    "    thr_info = json.load(f)\n",
    "\n",
    "# par√¢metros operacionais\n",
    "CUSTO_FP = 1.0    # custo unit√°rio de um falso positivo (alerta desnecess√°rio)\n",
    "CUSTO_FN = 25.0   # custo unit√°rio de um falso negativo (fraude/erro n√£o detectado)\n",
    "ALERTAS_META = 200  # meta operacional de alertas/dia\n",
    "\n",
    "# fun√ß√£o: calcular custo e quantidade de alertas para cada limiar\n",
    "def calcular_metricas(errs: np.ndarray, limiares: np.ndarray):\n",
    "    n = len(errs)\n",
    "    # ordena erros\n",
    "    errs_sorted = np.sort(errs)\n",
    "    metrics = []\n",
    "    for thr in limiares:\n",
    "        alertas = np.count_nonzero(errs > thr)\n",
    "        fp_rate = alertas / n\n",
    "        fn_rate = 1 - fp_rate\n",
    "        custo = fp_rate * CUSTO_FP + fn_rate * CUSTO_FN\n",
    "        metrics.append((thr, alertas, fp_rate, fn_rate, custo))\n",
    "    df = pd.DataFrame(metrics, columns=[\"threshold\", \"alertas\", \"fp_rate\", \"fn_rate\", \"custo\"])\n",
    "    return df\n",
    "\n",
    "# grid de limiares entre p50 e p99.9\n",
    "low, high = np.percentile(val_err, [50, 99.9])\n",
    "limiares = np.linspace(low, high, 200)\n",
    "df_calib = calcular_metricas(val_err, limiares)\n",
    "\n",
    "# acha o limiar que gera ~ALERTAS_META alertas\n",
    "target_idx = (df_calib[\"alertas\"] - ALERTAS_META).abs().idxmin()\n",
    "limiar_meta = float(df_calib.loc[target_idx, \"threshold\"])\n",
    "\n",
    "# curva de custo\n",
    "plt.figure(figsize=(6.8, 4))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"custo\"], label=\"custo total\")\n",
    "plt.axvline(limiar_meta, color=\"r\", ls=\"--\", label=f\"meta {ALERTAS_META} alertas\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"custo relativo\")\n",
    "plt.title(\"Curva de custo vs. threshold\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "cost_path = RUN_DIR / \"cost_curve.png\"\n",
    "plt.savefig(cost_path, dpi=140)\n",
    "plt.close()\n",
    "\n",
    "# curva de alertas\n",
    "plt.figure(figsize=(6.8, 4))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"alertas\"])\n",
    "plt.axhline(ALERTAS_META, color=\"r\", ls=\"--\", label=\"meta operacional\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"quantidade de alertas\")\n",
    "plt.title(\"Alertas vs. threshold\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "alerts_path = RUN_DIR / \"alerts_vs_threshold.png\"\n",
    "plt.savefig(alerts_path, dpi=140)\n",
    "plt.close()\n",
    "\n",
    "# salva relat√≥rio completo\n",
    "calib_csv = RUN_DIR / \"calibration_report.csv\"\n",
    "df_calib.to_csv(calib_csv, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "# resumo das m√©tricas principais\n",
    "summary = {\n",
    "    \"threshold_meta\": limiar_meta,\n",
    "    \"alertas_meta\": int(df_calib.loc[target_idx, \"alertas\"]),\n",
    "    \"custo_meta\": float(df_calib.loc[target_idx, \"custo\"]),\n",
    "    \"custo_minimo\": float(df_calib[\"custo\"].min()),\n",
    "    \"threshold_custo_minimo\": float(df_calib.loc[df_calib[\"custo\"].idxmin(), \"threshold\"]),\n",
    "}\n",
    "with open(RUN_DIR / \"calibration_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "skynet(f\"calibra√ß√£o conclu√≠da. limiar_meta={limiar_meta:.6f} ‚Üí {summary['alertas_meta']} alertas estimados\")\n",
    "skynet(f\"relat√≥rios salvos em: {calib_csv}, {cost_path}, {alerts_path}\")\n",
    "print(pd.Series(summary))\n",
    "\n",
    "# complemento do par√°grafo 6: marcar pontos escolhidos e exibir/salvar figuras\n",
    "\n",
    "# recupera valores nos pontos de interesse\n",
    "idx_meta = target_idx\n",
    "thr_meta = float(df_calib.loc[idx_meta, \"threshold\"])\n",
    "custo_meta = float(df_calib.loc[idx_meta, \"custo\"])\n",
    "alertas_meta_calc = int(df_calib.loc[idx_meta, \"alertas\"])\n",
    "\n",
    "idx_cmin = int(df_calib[\"custo\"].idxmin())\n",
    "thr_cmin = float(df_calib.loc[idx_cmin, \"threshold\"])\n",
    "custo_min = float(df_calib.loc[idx_cmin, \"custo\"])\n",
    "alertas_cmin = int(df_calib.loc[idx_cmin, \"alertas\"])\n",
    "\n",
    "# 1) curva de custo com marca√ß√µes\n",
    "plt.figure(figsize=(7.2, 4.5))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"custo\"], label=\"custo total\")\n",
    "plt.axvline(thr_meta, linestyle=\"--\", label=f\"threshold_meta = {thr_meta:.6f}\")\n",
    "plt.scatter([thr_meta, thr_cmin], [custo_meta, custo_min], s=60, label=\"pontos escolhidos\")\n",
    "plt.annotate(f\"meta\\nc={custo_meta:.2f}\", xy=(thr_meta, custo_meta),\n",
    "             xytext=(10, 10), textcoords=\"offset points\")\n",
    "plt.annotate(f\"m√≠nimo\\nc={custo_min:.2f}\", xy=(thr_cmin, custo_min),\n",
    "             xytext=(10, -15), textcoords=\"offset points\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"custo relativo\")\n",
    "plt.title(\"Curva de custo vs. threshold (com marca√ß√µes)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "cost_marked_path = RUN_DIR / \"cost_curve_marked.png\"\n",
    "plt.savefig(cost_marked_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"curva de custo (marcada) salva em {cost_marked_path}\")\n",
    "\n",
    "# 2) curva de alertas com marca√ß√µes\n",
    "plt.figure(figsize=(7.2, 4.5))\n",
    "plt.plot(df_calib[\"threshold\"], df_calib[\"alertas\"], label=\"alertas\")\n",
    "plt.axhline(ALERTAS_META, linestyle=\"--\", label=f\"meta operacional = {ALERTAS_META}\")\n",
    "plt.scatter([thr_meta, thr_cmin], [alertas_meta_calc, alertas_cmin], s=60, label=\"pontos escolhidos\")\n",
    "plt.annotate(f\"meta\\nA={alertas_meta_calc}\", xy=(thr_meta, alertas_meta_calc),\n",
    "             xytext=(10, 10), textcoords=\"offset points\")\n",
    "plt.annotate(f\"m√≠nimo(custo)\\nA={alertas_cmin}\", xy=(thr_cmin, alertas_cmin),\n",
    "             xytext=(10, -15), textcoords=\"offset points\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"quantidade de alertas\")\n",
    "plt.title(\"Alertas vs. threshold (com marca√ß√µes)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "alerts_marked_path = RUN_DIR / \"alerts_vs_threshold_marked.png\"\n",
    "plt.savefig(alerts_marked_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"curva de alertas (marcada) salva em {alerts_marked_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXb4j3zdhS2V"
   },
   "source": [
    "## **Etapa 7:** Pontua√ß√£o em lote e scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3931,
     "status": "ok",
     "timestamp": 1760324847866,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "dPz-pBI2iWnE",
    "outputId": "95f50720-71f6-497b-e57b-693d55c460d9"
   },
   "outputs": [],
   "source": [
    "# par√°grafo 7: pontua√ß√£o em lote com autoencoder treinado e export de scores (vers√£o corrigida)\n",
    "\n",
    "import os, json, math, pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# depend√™ncias anteriores\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os par√°grafos anteriores.\"\n",
    "assert 'REQUIRED_COLS' in globals() and 'CSV_SEP' in globals() and 'CSV_ENCODING' in globals(), \"execute o par√°grafo 2 antes.\"\n",
    "assert 'carregar_validar_csv' in globals(), \"a fun√ß√£o de carga/valida√ß√£o do par√°grafo 2 √© necess√°ria.\"\n",
    "artifacts_path = RUN_DIR / \"features.pkl\"\n",
    "model_path     = RUN_DIR / \"ae.pt\"\n",
    "assert artifacts_path.exists() and model_path.exists(), \"artefatos ausentes: finalize os par√°grafos 3 e 4.\"\n",
    "\n",
    "# carrega artefatos de features e modelo\n",
    "with open(artifacts_path, \"rb\") as f:\n",
    "    feats = pickle.load(f)\n",
    "FEATURE_COLS = feats[\"feature_cols\"]\n",
    "imputer      = feats[\"imputer\"]\n",
    "scaler       = feats[\"scaler\"]\n",
    "\n",
    "# threshold: usa o da calibra√ß√£o se existir; sen√£o p99.5 do par√°grafo 4\n",
    "thr_use = None\n",
    "calib_summary = RUN_DIR / \"calibration_summary.json\"\n",
    "thr_json      = RUN_DIR / \"thresholds.json\"\n",
    "if calib_summary.exists():\n",
    "    with open(calib_summary, encoding=\"utf-8\") as f:\n",
    "        thr_use = json.load(f).get(\"threshold_meta\", None)\n",
    "if thr_use is None and thr_json.exists():\n",
    "    with open(thr_json, encoding=\"utf-8\") as f:\n",
    "        tj = json.load(f)\n",
    "        thr_use = tj.get(\"suggested_threshold\", None)\n",
    "assert thr_use is not None, \"threshold n√£o encontrado; execute o par√°grafo 6 (ou use thresholds.json do par√°grafo 4).\"\n",
    "thr_use = float(thr_use)\n",
    "\n",
    "# fun√ß√£o: frequency encoding consistente\n",
    "def _freq_encode(series: pd.Series) -> pd.Series:\n",
    "    freq = series.value_counts(dropna=False)\n",
    "    return series.map(freq).astype(float)\n",
    "\n",
    "# fun√ß√£o: estat√≠sticas por grupo (mean/std/median, z, mad)\n",
    "def _add_group_stats(df: pd.DataFrame, key: str, val_col: str = \"valormi\"):\n",
    "    g = df.groupby(key, dropna=False)[val_col].agg([\"mean\",\"std\",\"median\"]).rename(\n",
    "        columns={\"mean\":f\"{key}_mean\", \"std\":f\"{key}_std\", \"median\":f\"{key}_median\"}\n",
    "    )\n",
    "    df = df.join(g, on=key)\n",
    "    df[f\"{key}_std\"] = df[f\"{key}_std\"].replace(0, np.nan)\n",
    "    df[f\"{key}_z\"]   = (df[val_col] - df[f\"{key}_mean\"]) / df[f\"{key}_std\"]\n",
    "    df[f\"{key}_mad\"] = (df[val_col] - df[f\"{key}_median\"]).abs()\n",
    "    return df\n",
    "\n",
    "# fun√ß√£o: construir features exatamente como no par√°grafo 3 (com corre√ß√£o .str.strip())\n",
    "def construir_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # normaliza√ß√£o b√°sica de strings (uso correto de .str.strip())\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    # dom√≠nios padronizados\n",
    "    df[\"dc\"] = df[\"dc\"].astype(str).str.lower().map({\"d\":\"d\",\"c\":\"c\"})\n",
    "    df[\"tipoconta\"] = df[\"tipoconta\"].astype(str).str.capitalize()\n",
    "    # garante tipo num√©rico para valormi\n",
    "    df[\"valormi\"] = pd.to_numeric(df[\"valormi\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "    # decomposi√ß√µes cosif\n",
    "    df[\"contacontabil\"] = df[\"contacontabil\"].astype(str).str.strip()\n",
    "    df[\"cosif_len\"] = df[\"contacontabil\"].str.len().clip(upper=12).astype(int)\n",
    "    df[\"cosif_p1\"]  = df[\"contacontabil\"].str[0]\n",
    "    df[\"cosif_p2\"]  = df[\"contacontabil\"].str[:2]\n",
    "    df[\"cosif_p3\"]  = df[\"contacontabil\"].str[:3]\n",
    "\n",
    "    # transforma√ß√µes de valor\n",
    "    df[\"valormi_log1p\"] = np.log1p(df[\"valormi\"].clip(lower=0))\n",
    "\n",
    "    # frequency encoding\n",
    "    df[\"freq_username\"]      = _freq_encode(df[\"username\"])\n",
    "    df[\"freq_lotacao\"]       = _freq_encode(df[\"lotacao\"])\n",
    "    df[\"freq_contacontabil\"] = _freq_encode(df[\"contacontabil\"])\n",
    "    df[\"freq_cosif_p2\"]      = _freq_encode(df[\"cosif_p2\"])\n",
    "    df[\"freq_cosif_p3\"]      = _freq_encode(df[\"cosif_p3\"])\n",
    "\n",
    "    # estat√≠sticas por grupo\n",
    "    for k in [\"username\",\"lotacao\",\"contacontabil\",\"cosif_p2\",\"cosif_p3\"]:\n",
    "        df = _add_group_stats(df, k, \"valormi\")\n",
    "\n",
    "    # codifica√ß√µes de baixa cardinalidade\n",
    "    df[\"dc_bin\"] = df[\"dc\"].map({\"d\":1,\"c\":0}).astype(\"Int64\").astype(int)\n",
    "    tipodummies = pd.get_dummies(df[\"tipoconta\"], prefix=\"tipo\", dtype=int)\n",
    "\n",
    "    df = pd.concat([df, tipodummies], axis=1)\n",
    "\n",
    "    # garante presen√ßa de todas as colunas esperadas e na mesma ordem do treino\n",
    "    for c in FEATURE_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    df_out = df[FEATURE_COLS].copy()\n",
    "    return df_out\n",
    "\n",
    "# carrega modelo pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "chk = torch.load(model_path, map_location=device)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, h_dims, dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc, last = [], in_dim\n",
    "        for h in h_dims:\n",
    "            enc += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "        dec, rev = [], list(reversed(h_dims))\n",
    "        last = rev[0]\n",
    "        for h in rev[1:]:\n",
    "            dec += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        dec += [nn.Linear(last, in_dim)]\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z)\n",
    "        return y\n",
    "\n",
    "model = AutoEncoder(chk[\"input_dim\"], chk[\"hidden_dims\"], dropout=chk[\"dropout_p\"]).to(device)\n",
    "model.load_state_dict(chk[\"model_state\"])\n",
    "model.eval()\n",
    "skynet(f\"modelo carregado para pontua√ß√£o (device={device})\")\n",
    "\n",
    "# sele√ß√£o do arquivo a pontuar: reusar o selecionado ou escolher outro\n",
    "use_same = input(\"pressione enter para usar o CSV j√° selecionado; ou digite 'novo' para escolher outro: \").strip().lower()\n",
    "if use_same == \"novo\":\n",
    "    assert 'listar_csvs_mydrive' in globals(), \"a fun√ß√£o listar_csvs_mydrive do par√°grafo 2 √© necess√°ria.\"\n",
    "    csvs = listar_csvs_mydrive(max_files=2000)\n",
    "    print(\"\\narquivos csv encontrados (√≠ndice, tamanho, caminho):\")\n",
    "    for i, (p, mtime, sz) in enumerate(csvs):\n",
    "        print(f\"[{i:03d}] {sz/1024/1024:6.2f}MB  {str(p)}\")\n",
    "    sel = int(input(\"\\ndigite o √≠ndice do csv desejado: \").strip())\n",
    "    CSV_TO_SCORE = Path(csvs[sel][0])\n",
    "else:\n",
    "    assert 'SELECTED_CSV' in globals(), \"n√£o h√° CSV selecionado anteriormente; escolha 'novo'.\"\n",
    "    CSV_TO_SCORE = Path(SELECTED_CSV)\n",
    "\n",
    "skynet(f\"pontuando arquivo: {CSV_TO_SCORE}\")\n",
    "\n",
    "# carrega e valida o csv\n",
    "DF_SCORE_SRC = carregar_validar_csv(CSV_TO_SCORE)\n",
    "\n",
    "# remove linhas com falhas cr√≠ticas como no par√°grafo 3\n",
    "mask_ok_score = (\n",
    "    DF_SCORE_SRC[\"username\"].notna() &\n",
    "    DF_SCORE_SRC[\"lotacao\"].notna() &\n",
    "    DF_SCORE_SRC[\"tipoconta\"].notna() &\n",
    "    DF_SCORE_SRC[\"dc\"].isin({\"d\",\"c\"}) &\n",
    "    DF_SCORE_SRC[\"contacontabil\"].astype(str).str.fullmatch(r\"\\d+\") &\n",
    "    DF_SCORE_SRC[\"valormi\"].notna() & (DF_SCORE_SRC[\"valormi\"] >= 0)\n",
    ")\n",
    "removed_score = int((~mask_ok_score).sum())\n",
    "DF_SCORE = DF_SCORE_SRC.loc[mask_ok_score].reset_index(drop=True)\n",
    "skynet(f\"linhas removidas nesta pontua√ß√£o: {removed_score}\")\n",
    "\n",
    "# constroi features e aplica imputer/scaler treinados\n",
    "X_feat = construir_features(DF_SCORE)\n",
    "X_imp  = imputer.transform(X_feat.values)\n",
    "X_std  = scaler.transform(X_imp)\n",
    "\n",
    "# infer√™ncia\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.from_numpy(X_std.astype(np.float32)).to(device)\n",
    "    X_recon  = model(X_tensor).cpu().numpy()\n",
    "\n",
    "# erro de reconstru√ß√£o (mse por amostra) e contribui√ß√µes por feature (erro quadr√°tico)\n",
    "err_vec = np.mean((X_recon - X_std) ** 2, axis=1).astype(np.float32)\n",
    "contrib_mat = (X_recon - X_std) ** 2  # mesma escala das features padronizadas\n",
    "contrib_cols = FEATURE_COLS\n",
    "\n",
    "# top-k contribui√ß√µes por amostra (nomes das vari√°veis)\n",
    "TOPK = 5\n",
    "topk_idx = np.argsort(contrib_mat, axis=1)[:, ::-1][:, :TOPK]\n",
    "topk_names = [[contrib_cols[j] for j in row] for row in topk_idx]\n",
    "\n",
    "# flag por limiar\n",
    "flags = (err_vec > thr_use).astype(int)\n",
    "\n",
    "# comp√µe dataframe de sa√≠da\n",
    "OUT = DF_SCORE[[\"username\",\"lotacao\",\"tipoconta\",\"valormi\",\"dc\",\"contacontabil\"]].copy()\n",
    "OUT[\"recon_error\"] = err_vec\n",
    "OUT[\"flag_threshold\"] = flags\n",
    "OUT[\"threshold_used\"] = thr_use\n",
    "for k in range(TOPK):\n",
    "    OUT[f\"top{k+1}\"] = [names[k] for names in topk_names]\n",
    "\n",
    "# salva csv de scores\n",
    "scores_path = RUN_DIR / \"scores.csv\"\n",
    "OUT.to_csv(scores_path, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "skynet(f\"scores salvos em {scores_path}\")\n",
    "\n",
    "# salva contribui√ß√µes m√©dias por feature (para vis√£o global)\n",
    "mean_contrib = contrib_mat.mean(axis=0)\n",
    "CONTRIB_DF = pd.DataFrame({\"feature\": contrib_cols, \"mean_contrib\": mean_contrib})\n",
    "CONTRIB_DF.sort_values(\"mean_contrib\", ascending=False, inplace=True)\n",
    "contrib_path = RUN_DIR / \"feature_contributions_mean.csv\"\n",
    "CONTRIB_DF.to_csv(contrib_path, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "skynet(f\"contribui√ß√µes m√©dias por feature salvas em {contrib_path}\")\n",
    "\n",
    "# histograma de erros com linha de threshold\n",
    "plt.figure(figsize=(7.0, 4.4))\n",
    "plt.hist(err_vec, bins=60, alpha=0.85)\n",
    "plt.axvline(thr_use, linestyle=\"--\", label=f\"threshold={thr_use:.6f}\")\n",
    "plt.xlabel(\"erro de reconstru√ß√£o\")\n",
    "plt.ylabel(\"frequ√™ncia\")\n",
    "plt.title(\"distribui√ß√£o do erro de reconstru√ß√£o (batch scoring)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "hist_path = RUN_DIR / \"scoring_error_hist.png\"\n",
    "plt.savefig(hist_path, dpi=140, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "skynet(f\"histograma de erros salvo em {hist_path}\")\n",
    "\n",
    "# imprime amostra dos top-n suspeitos\n",
    "TOPN_PRINT = 15\n",
    "rank = OUT.sort_values(\"recon_error\", ascending=False).head(TOPN_PRINT)\n",
    "print(\"\\nTop suspeitos por erro de reconstru√ß√£o:\")\n",
    "print(rank[[\"username\",\"lotacao\",\"tipoconta\",\"valormi\",\"dc\",\"contacontabil\",\"recon_error\",\"flag_threshold\",\"top1\",\"top2\",\"top3\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMskiJYXiikw"
   },
   "source": [
    "## **Etapa 8:** Monitoramento de drift e drift do erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1760324959801,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "PpFJcJPBiuCx",
    "outputId": "1b39448b-cb27-4df0-cc4a-0017e98e30bc"
   },
   "outputs": [],
   "source": [
    "# par√°grafo 8: monitoramento de drift (PSI por feature) e drift do erro (PSI + KS)\n",
    "\n",
    "import os, json, math, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# depend√™ncias dos par√°grafos 3, 4 e 7\n",
    "assert 'RUN_DIR' in globals() and 'skynet' in globals(), \"execute os par√°grafos anteriores.\"\n",
    "artifacts_path = RUN_DIR / \"features.pkl\"\n",
    "npz_path       = RUN_DIR / \"dataset_npz.npz\"            # cont√©m X_train/X_val (padronizados)\n",
    "val_err_path   = RUN_DIR / \"reconstruction_errors_val.npy\"  # erros da valida√ß√£o (par. 4)\n",
    "scores_path    = RUN_DIR / \"scores.csv\"                 # resultado da pontua√ß√£o (par. 7)\n",
    "\n",
    "assert artifacts_path.exists() and npz_path.exists() and val_err_path.exists(), \\\n",
    "    \"artefatos ausentes: finalize os par√°grafos 3 e 4.\"\n",
    "assert scores_path.exists(), \"scores.csv ausente. finalize o par√°grafo 7.\"\n",
    "\n",
    "with open(artifacts_path, \"rb\") as f:\n",
    "    feats = pickle.load(f)\n",
    "FEATURE_COLS = feats[\"feature_cols\"]\n",
    "imputer      = feats[\"imputer\"]\n",
    "scaler       = feats[\"scaler\"]\n",
    "\n",
    "# baseline (treino) para PSI de features\n",
    "with np.load(npz_path) as npz:\n",
    "    X_train_base = npz[\"X_train\"]  # j√° padronizado\n",
    "\n",
    "# erros baseline (valida√ß√£o do treino) para drift de erro\n",
    "err_val_base = np.load(val_err_path)\n",
    "\n",
    "# ----- recuperar features padronizadas do LOTE ATUAL -----\n",
    "# Se X_std e DF_SCORE ainda estiverem em mem√≥ria (par.7), √≥timo; caso contr√°rio, reconstruir\n",
    "def _reconst_features_from_source():\n",
    "    # precisamos do mesmo CSV do par.7; como fallback, usamos o DF reconstru√≠do de scores.csv\n",
    "    # (scores.csv n√£o cont√©m features; ent√£o pedimos para escolher um CSV novamente, garantindo consist√™ncia)\n",
    "    assert 'carregar_validar_csv' in globals() and 'construir_features' in globals(), \\\n",
    "        \"fun√ß√µes do par√°grafo 2 e 7 s√£o necess√°rias.\"\n",
    "    print(\"\\nreconstruindo features: selecione novamente o CSV a monitorar.\")\n",
    "    # reutiliza a listagem do par.2\n",
    "    assert 'listar_csvs_mydrive' in globals(), \"a fun√ß√£o listar_csvs_mydrive do par.2 √© necess√°ria.\"\n",
    "    csvs = listar_csvs_mydrive(max_files=2000)\n",
    "    for i, (p, mtime, sz) in enumerate(csvs):\n",
    "        print(f\"[{i:03d}] {sz/1024/1024:6.2f}MB  {str(p)}\")\n",
    "    sel = int(input(\"\\ndigite o √≠ndice do csv desejado: \").strip())\n",
    "    csv_path = Path(csvs[sel][0])\n",
    "    df_src = carregar_validar_csv(csv_path)\n",
    "    # filtra linhas cr√≠ticas como no par.3/7\n",
    "    mask_ok = (\n",
    "        df_src[\"username\"].notna() &\n",
    "        df_src[\"lotacao\"].notna() &\n",
    "        df_src[\"tipoconta\"].notna() &\n",
    "        df_src[\"dc\"].isin({\"d\",\"c\"}) &\n",
    "        df_src[\"contacontabil\"].astype(str).str.fullmatch(r\"\\d+\") &\n",
    "        df_src[\"valormi\"].notna() & (df_src[\"valormi\"] >= 0)\n",
    "    )\n",
    "    df_use = df_src.loc[mask_ok].reset_index(drop=True)\n",
    "    X_feat = construir_features(df_use)\n",
    "    X_imp  = imputer.transform(X_feat.values)\n",
    "    X_std  = scaler.transform(X_imp)\n",
    "    return X_std\n",
    "\n",
    "if 'X_std' in globals():\n",
    "    X_curr = X_std\n",
    "else:\n",
    "    X_curr = _reconst_features_from_source()\n",
    "\n",
    "# ----- utilit√°rios de PSI/KS -----\n",
    "def _bin_edges_from_base(base_vals: np.ndarray, n_bins: int = 10):\n",
    "    # cria bins por quantis no baseline; garante bordas √∫nicas (jitter se necess√°rio)\n",
    "    qs = np.linspace(0, 1, n_bins + 1)\n",
    "    edges = np.unique(np.quantile(base_vals, qs))\n",
    "    # se todas iguais (vari√¢ncia zero), retorna None\n",
    "    if len(edges) <= 2:\n",
    "        return None\n",
    "    return edges\n",
    "\n",
    "def _hist_proportions(vals: np.ndarray, edges: np.ndarray):\n",
    "    hist, _ = np.histogram(vals, bins=edges)\n",
    "    props = hist.astype(float) / max(1, vals.shape[0])\n",
    "    # suaviza√ß√£o m√≠nima para evitar log(0) no PSI\n",
    "    eps = 1e-6\n",
    "    props = np.clip(props, eps, 1.0)\n",
    "    return props\n",
    "\n",
    "def psi(base: np.ndarray, curr: np.ndarray, n_bins: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    PSI padr√£o por bins definidos a partir do baseline.\n",
    "    \"\"\"\n",
    "    edges = _bin_edges_from_base(base, n_bins)\n",
    "    if edges is None:\n",
    "        return 0.0\n",
    "    p = _hist_proportions(base, edges)\n",
    "    q = _hist_proportions(curr, edges)\n",
    "    return float(np.sum((p - q) * np.log(p / q)))\n",
    "\n",
    "def ks_2sample(a: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    KS 2-amostras sem SciPy (ECDF discreta).\n",
    "    Retorna (D, p_value_approx) com aproxima√ß√£o de p.\n",
    "    \"\"\"\n",
    "    a = np.sort(a)\n",
    "    b = np.sort(b)\n",
    "    n, m = len(a), len(b)\n",
    "    i = j = 0\n",
    "    d = 0.0\n",
    "    while i < n and j < m:\n",
    "        if a[i] <= b[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        fa = i / n\n",
    "        fb = j / m\n",
    "        d = max(d, abs(fa - fb))\n",
    "    # aproxima√ß√£o de p-value (Massey, 1951)\n",
    "    en = np.sqrt(n * m / (n + m))\n",
    "    p = 2.0 * np.exp(-2.0 * (d * en) ** 2)\n",
    "    p = float(np.clip(p, 0.0, 1.0))\n",
    "    return float(d), p\n",
    "\n",
    "# ----- PSI por feature -----\n",
    "skynet(\"calculando PSI por feature (baseline = X_train, atual = lote pontuado)\")\n",
    "psi_list = []\n",
    "# limita a quantidade de colunas para gr√°ficos (mas calcula PSI para todas)\n",
    "for j, col in enumerate(FEATURE_COLS):\n",
    "    base_col = X_train_base[:, j]\n",
    "    curr_col = X_curr[:, j]\n",
    "    try:\n",
    "        score = psi(base_col, curr_col, n_bins=10)\n",
    "    except Exception:\n",
    "        score = np.nan\n",
    "    psi_list.append((col, float(score)))\n",
    "\n",
    "PSI_DF = pd.DataFrame(psi_list, columns=[\"feature\", \"psi\"]).sort_values(\"psi\", ascending=False)\n",
    "psi_csv = RUN_DIR / \"monitor_psi_features.csv\"\n",
    "PSI_DF.to_csv(psi_csv, sep=\";\", encoding=\"utf-8-sig\", index=False)\n",
    "skynet(f\"PSI por feature salvo em {psi_csv}\")\n",
    "\n",
    "# ----- Drift do erro de reconstru√ß√£o -----\n",
    "# Reusa erro do lote atual se dispon√≠vel do par.7; sen√£o, estima novamente a partir do scores.csv (n√£o cont√©m erro por si s√≥).\n",
    "if 'err_vec' in globals():\n",
    "    err_curr = err_vec\n",
    "else:\n",
    "    # tentar recuperar do histograma salvo n√£o √© poss√≠vel; ent√£o avisar\n",
    "    warnings.warn(\"err_vec n√£o encontrado em mem√≥ria; reexecute o par√°grafo 7 para popular err_vec para an√°lise completa.\")\n",
    "    # fallback: abortar se√ß√£o de erro se n√£o houver err_vec\n",
    "    err_curr = None\n",
    "\n",
    "drift_report = {}\n",
    "if err_curr is not None and len(err_curr) > 0:\n",
    "    psi_err = psi(err_val_base, err_curr, n_bins=20)\n",
    "    ks_D, ks_p = ks_2sample(err_val_base, err_curr)\n",
    "    drift_report = {\"psi_error\": float(psi_err), \"ks_D\": float(ks_D), \"ks_p_approx\": float(ks_p)}\n",
    "\n",
    "    # gr√°ficos: hist overlay de erros\n",
    "    plt.figure(figsize=(7.2, 4.6))\n",
    "    plt.hist(err_val_base, bins=60, alpha=0.55, label=\"val (baseline)\")\n",
    "    plt.hist(err_curr,     bins=60, alpha=0.55, label=\"lote atual\")\n",
    "    plt.xlabel(\"erro de reconstru√ß√£o\")\n",
    "    plt.ylabel(\"frequ√™ncia\")\n",
    "    plt.title(\"Distribui√ß√£o do erro de reconstru√ß√£o: baseline vs lote\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    err_hist_path = RUN_DIR / \"error_hist_overlay.png\"\n",
    "    plt.savefig(err_hist_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    skynet(f\"histograma de erro (overlay) salvo em {err_hist_path}\")\n",
    "else:\n",
    "    skynet(\"aviso: err_vec indispon√≠vel; pulei gr√°ficos e m√©tricas de drift do erro.\")\n",
    "\n",
    "# ----- Gr√°fico: Top-k features com maior PSI -----\n",
    "TOPK = 20\n",
    "top_df = PSI_DF.head(TOPK)\n",
    "plt.figure(figsize=(8.8, max(4.0, 0.3 * TOPK)))\n",
    "plt.barh(top_df[\"feature\"][::-1], top_df[\"psi\"][::-1])\n",
    "plt.xlabel(\"PSI\")\n",
    "plt.title(f\"Top {TOPK} features com maior PSI (baseline X_train vs lote)\")\n",
    "plt.grid(axis=\"x\", alpha=0.3)\n",
    "psi_bar_path = RUN_DIR / \"psi_bar_top.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(psi_bar_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"gr√°fico de barras do PSI salvo em {psi_bar_path}\")\n",
    "\n",
    "# ----- Salvamento do relat√≥rio de drift do erro -----\n",
    "if drift_report:\n",
    "    drift_json = RUN_DIR / \"monitor_error_drift.json\"\n",
    "    with open(drift_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(drift_report, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nresumo drift do erro (baseline val vs lote):\")\n",
    "    print(pd.Series(drift_report))\n",
    "    skynet(f\"relat√≥rio de drift do erro salvo em {drift_json}\")\n",
    "\n",
    "# ----- Sinalizadores pr√°ticos -----\n",
    "# heur√≠sticas comuns de PSI:\n",
    "#   < 0.1: est√°vel; 0.1‚Äì0.25: aten√ß√£o; > 0.25: shift relevante\n",
    "flags = {\n",
    "    \"num_features_psi_gt_0_25\": int((PSI_DF[\"psi\"] > 0.25).sum()),\n",
    "    \"num_features_psi_gt_0_10\": int((PSI_DF[\"psi\"] > 0.10).sum()),\n",
    "    \"max_feature_psi\": float(PSI_DF[\"psi\"].max() if len(PSI_DF) else np.nan),\n",
    "    \"max_feature_name\": str(PSI_DF.iloc[0][\"feature\"] if len(PSI_DF) else \"\"),\n",
    "}\n",
    "with open(RUN_DIR / \"monitor_flags.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(flags, f, ensure_ascii=False, indent=2)\n",
    "print(\"\\nresumo de flags de estabilidade (PSI):\")\n",
    "print(pd.Series(flags))\n",
    "skynet(\"monitoramento conclu√≠do: PSI por feature, drift do erro e flags salvos no RUN_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FND4f7mZjuny"
   },
   "source": [
    "### **Complemento** An√°lise gr√°fica do drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4602,
     "status": "ok",
     "timestamp": 1760325245411,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "yfxGTpkGjth_",
    "outputId": "0ceea81a-e272-4918-82c9-509678e53744"
   },
   "outputs": [],
   "source": [
    "# complemento interpretativo do par√°grafo 8: histogramas e ECDF com marca√ß√£o do KS\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# pr√©-requisitos: err_val_base (do par.8) e err_curr (do par.7) em mem√≥ria\n",
    "if 'err_val_base' not in globals():\n",
    "    err_val_base = np.load(RUN_DIR / \"reconstruction_errors_val.npy\")\n",
    "if 'err_curr' not in globals():\n",
    "    raise RuntimeError(\"err_curr indispon√≠vel. execute o par√°grafo 7 para calcular os erros do lote atual (err_vec).\")\n",
    "\n",
    "# fun√ß√£o utilit√°ria para ECDF (degrau)\n",
    "def _ecdf(x: np.ndarray):\n",
    "    x = np.sort(np.asarray(x))\n",
    "    y = np.arange(1, len(x) + 1) / len(x)\n",
    "    return x, y\n",
    "\n",
    "# calcula ECDFs\n",
    "x1, F1 = _ecdf(err_val_base)\n",
    "x2, F2 = _ecdf(err_curr)\n",
    "\n",
    "# constr√≥i grade comum de x (todos os pontos observados)\n",
    "x_all = np.sort(np.unique(np.concatenate([x1, x2])))\n",
    "\n",
    "# valores de F1 e F2 sobre a grade comum (passo √† esquerda)\n",
    "def _step_at(x_grid, x_vals, F_vals):\n",
    "    # para cada x_grid, pega F(x) = propor√ß√£o de pontos <= x_grid\n",
    "    idx = np.searchsorted(x_vals, x_grid, side=\"right\") - 1\n",
    "    idx = np.clip(idx, -1, len(F_vals) - 1)\n",
    "    out = np.where(idx >= 0, F_vals[idx], 0.0)\n",
    "    return out\n",
    "\n",
    "F1g = _step_at(x_all, x1, F1)\n",
    "F2g = _step_at(x_all, x2, F2)\n",
    "\n",
    "# estat√≠stica KS e ponto de maior diverg√™ncia\n",
    "diff = np.abs(F1g - F2g)\n",
    "ks_D = float(diff.max())\n",
    "argmax = int(diff.argmax())\n",
    "x_star = float(x_all[argmax])\n",
    "\n",
    "# 1) histograma comparativo (normalizado) com anota√ß√£o KS\n",
    "plt.figure(figsize=(7.8, 4.8))\n",
    "plt.hist(err_val_base, bins=60, alpha=0.55, density=True, label=\"valida√ß√£o (baseline)\")\n",
    "plt.hist(err_curr,     bins=60, alpha=0.55, density=True, label=\"lote atual\")\n",
    "plt.axvline(x_star, linestyle=\"--\", label=f\"x* (KS) ‚âà {x_star:.6f}\")\n",
    "plt.title(\"erro de reconstru√ß√£o ‚Äî distribui√ß√£o (baseline vs. lote)\")\n",
    "plt.xlabel(\"erro de reconstru√ß√£o\")\n",
    "plt.ylabel(\"densidade\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "hist_interp_path = RUN_DIR / \"error_drift_interpret_hist.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_interp_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"gr√°fico interpretativo (hist): {hist_interp_path}\")\n",
    "\n",
    "# 2) ECDFs com marca√ß√£o visual do KS (dist√¢ncia m√°xima vertical)\n",
    "plt.figure(figsize=(7.8, 4.8))\n",
    "plt.step(x_all, F1g, where=\"post\", label=\"ECDF val (baseline)\")\n",
    "plt.step(x_all, F2g, where=\"post\", label=\"ECDF lote atual\")\n",
    "\n",
    "# marca o ponto de maior diverg√™ncia\n",
    "plt.axvline(x_star, linestyle=\"--\", alpha=0.8)\n",
    "# desenha o segmento vertical da dist√¢ncia KS\n",
    "y1_star = F1g[argmax]\n",
    "y2_star = F2g[argmax]\n",
    "y_low, y_high = sorted([y1_star, y2_star])\n",
    "plt.vlines(x_star, y_low, y_high, linewidth=3, alpha=0.9, label=f\"KS = {ks_D:.3f}\")\n",
    "\n",
    "plt.title(\"erro de reconstru√ß√£o ‚Äî ECDFs e dist√¢ncia KS\")\n",
    "plt.xlabel(\"erro de reconstru√ß√£o\")\n",
    "plt.ylabel(\"F(x)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "ecdf_interp_path = RUN_DIR / \"error_drift_interpret_ecdf.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(ecdf_interp_path, dpi=140)\n",
    "plt.show()\n",
    "skynet(f\"gr√°fico interpretativo (ecdf): {ecdf_interp_path}\")\n",
    "\n",
    "# explica√ß√£o r√°pida impressa\n",
    "print(\n",
    "    f\"\\ninterpreta√ß√£o r√°pida:\\n\"\n",
    "    f\"- KS = {ks_D:.3f} √© a maior dist√¢ncia vertical entre as ECDFs no ponto x* ‚âà {x_star:.6f}.\\n\"\n",
    "    f\"- quanto maior o KS, maior a evid√™ncia de que as distribui√ß√µes de erro mudaram.\\n\"\n",
    "    f\"- se KS estiver pr√≥ximo de 1.0, as curvas quase n√£o se sobrep√µem; se perto de 0, s√£o muito similares.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgOMyD9yS/uWLXnDBP6caW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
